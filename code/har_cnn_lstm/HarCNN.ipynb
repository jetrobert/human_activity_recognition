{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# get_ipython().magic(u'matplotlib inline')\n",
    "# %matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gpu environment config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISUAL_DEVICES\"] = '1'\n",
    "session_conf = tf.ConfigProto()\n",
    "session_conf.gpu_options.allow_growth = True\n",
    "session_conf.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "# session_conf.gpu_options.allocator_type = \"BFC\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# specifical funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    column_names = ['user-id','activity','timestamp', 'x-axis', 'y-axis', 'z-axis']\n",
    "    data = pd.read_csv(file_path,header = None, names = column_names)\n",
    "    return data\n",
    "\n",
    "def feature_normalize(dataset):\n",
    "    mu = np.mean(dataset,axis = 0)\n",
    "    sigma = np.std(dataset,axis = 0)\n",
    "    return (dataset - mu)/sigma\n",
    "\n",
    "def plot_axis(ax, x, y, title):\n",
    "    ax.plot(x, y)\n",
    "    ax.set_title(title)\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.set_ylim([min(y) - np.std(y), max(y) + np.std(y)])\n",
    "    ax.set_xlim([min(x), max(x)])\n",
    "    ax.grid(True)\n",
    "\n",
    "def plot_activity(activity,data):\n",
    "    fig, (ax0, ax1, ax2) = plt.subplots(nrows = 3, figsize = (15, 10), sharex = True)\n",
    "    plot_axis(ax0, data['timestamp'], data['x-axis'], 'x-axis')\n",
    "    plot_axis(ax1, data['timestamp'], data['y-axis'], 'y-axis')\n",
    "    plot_axis(ax2, data['timestamp'], data['z-axis'], 'z-axis')\n",
    "    plt.subplots_adjust(hspace=0.2)\n",
    "    fig.suptitle(activity)\n",
    "    plt.subplots_adjust(top=0.90)\n",
    "    plt.show()\n",
    "\n",
    "def windows(data, size):\n",
    "    start = 0\n",
    "    while start < data.count():\n",
    "        yield start, start + size\n",
    "        start += (size / 2)\n",
    "\n",
    "def segment_signal(data,window_size = 90):\n",
    "    segments = np.empty((0,window_size,3))\n",
    "    labels = np.empty((0))\n",
    "    for (start, end) in windows(data['timestamp'], window_size):\n",
    "        x = data[\"x-axis\"][start:end]\n",
    "        y = data[\"y-axis\"][start:end]\n",
    "        z = data[\"z-axis\"][start:end]\n",
    "        if(len(dataset['timestamp'][start:end]) == window_size):\n",
    "            segments = np.vstack([segments,np.dstack([x,y,z])])\n",
    "            labels = np.append(labels,stats.mode(data[\"activity\"][start:end])[0][0])\n",
    "    return segments, labels\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.0, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def depthwise_conv2d(x, W):\n",
    "    return tf.nn.depthwise_conv2d(x,W, [1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "def apply_depthwise_conv(x,kernel_size,num_channels,depth):\n",
    "    weights = weight_variable([1, kernel_size, num_channels, depth])\n",
    "    biases = bias_variable([depth * num_channels])\n",
    "    return tf.nn.relu(tf.add(depthwise_conv2d(x, weights),biases))\n",
    "\n",
    "def apply_max_pool(x,kernel_size,stride_size):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, kernel_size, 1],\n",
    "                          strides=[1, 1, stride_size, 1], padding='VALID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "dataset = read_data('data/actitracker_raw.txt')\n",
    "dataset['x-axis'] = feature_normalize(dataset['x-axis'])\n",
    "dataset['y-axis'] = feature_normalize(dataset['y-axis'])\n",
    "dataset['z-axis'] = feature_normalize(dataset['z-axis'])\n",
    "for activity in np.unique(dataset[\"activity\"]):\n",
    "    subset = dataset[dataset[\"activity\"] == activity][:180]\n",
    "    plot_activity(activity,subset)\n",
    "\n",
    "segments, labels = segment_signal(dataset)\n",
    "segments, labels = segment_signal(dataset)\n",
    "segmentsData = open('segmentData.pkl', 'wb')\n",
    "pickle.dump(segments, segmentsData)\n",
    "labelsData = open('labelsData.pkl', 'wb')\n",
    "pickle.dump(labels, labelsData)\n",
    "\n",
    "labels = np.asarray(pd.get_dummies(labels), dtype = np.int8)\n",
    "reshaped_segments = segments.reshape(len(segments), 1,90, 3)\n",
    "\n",
    "train_test_split = np.random.rand(len(reshaped_segments)) < 0.70\n",
    "train_x = reshaped_segments[train_test_split]\n",
    "train_y = labels[train_test_split]\n",
    "test_x = reshaped_segments[~train_test_split]\n",
    "test_y = labels[~train_test_split]\n",
    "'''\n",
    "\n",
    "processedData = open('data/processedData.pkl', 'rb')\n",
    "processedData = pickle.load(processedData)\n",
    "\n",
    "train_x, train_y, test_x, test_y = processedData[0],\\\n",
    "        processedData[1], processedData[2], processedData[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# network config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 6\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 10\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 1000\n",
    "\n",
    "learning_rate = 0.0001\n",
    "training_epochs = 100\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build cnn network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1]*shape[2]*depth * num_channels * (depth//10), \n",
    "                                num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net work done\n",
      "WARNING:tensorflow:From <ipython-input-10-7d2504521732>:23: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "('Epoch: ', 0, ' Training Loss: ', 16.427217, ' Training Accuracy: ', 0.62105268)\n",
      "('Epoch: ', 1, ' Training Loss: ', 14.801222, ' Training Accuracy: ', 0.6736843)\n",
      "('Epoch: ', 2, ' Training Loss: ', 13.61653, ' Training Accuracy: ', 0.6842106)\n",
      "('Epoch: ', 3, ' Training Loss: ', 12.47934, ' Training Accuracy: ', 0.71052635)\n",
      "('Epoch: ', 4, ' Training Loss: ', 11.408047, ' Training Accuracy: ', 0.77368426)\n",
      "('Epoch: ', 5, ' Training Loss: ', 10.422956, ' Training Accuracy: ', 0.78947377)\n",
      "('Epoch: ', 6, ' Training Loss: ', 9.5245752, ' Training Accuracy: ', 0.80000007)\n",
      "('Epoch: ', 7, ' Training Loss: ', 8.7159042, ' Training Accuracy: ', 0.82631588)\n",
      "('Epoch: ', 8, ' Training Loss: ', 7.9924841, ' Training Accuracy: ', 0.84736848)\n",
      "('Epoch: ', 9, ' Training Loss: ', 7.3487592, ' Training Accuracy: ', 0.87894738)\n",
      "('Epoch: ', 10, ' Training Loss: ', 6.7775822, ' Training Accuracy: ', 0.9000001)\n",
      "('Epoch: ', 11, ' Training Loss: ', 6.2802978, ' Training Accuracy: ', 0.90000004)\n",
      "('Epoch: ', 12, ' Training Loss: ', 5.8400421, ' Training Accuracy: ', 0.90000004)\n",
      "('Epoch: ', 13, ' Training Loss: ', 5.4479952, ' Training Accuracy: ', 0.90526325)\n",
      "('Epoch: ', 14, ' Training Loss: ', 5.1003914, ' Training Accuracy: ', 0.91052634)\n",
      "('Epoch: ', 15, ' Training Loss: ', 4.7897091, ' Training Accuracy: ', 0.92631584)\n",
      "('Epoch: ', 16, ' Training Loss: ', 4.5100737, ' Training Accuracy: ', 0.93157899)\n",
      "('Epoch: ', 17, ' Training Loss: ', 4.2570992, ' Training Accuracy: ', 0.93157893)\n",
      "('Epoch: ', 18, ' Training Loss: ', 4.0288744, ' Training Accuracy: ', 0.93157899)\n",
      "('Epoch: ', 19, ' Training Loss: ', 3.8250055, ' Training Accuracy: ', 0.93157899)\n",
      "('Epoch: ', 20, ' Training Loss: ', 3.6373796, ' Training Accuracy: ', 0.9368422)\n",
      "('Epoch: ', 21, ' Training Loss: ', 3.4678726, ' Training Accuracy: ', 0.93684226)\n",
      "('Epoch: ', 22, ' Training Loss: ', 3.3117344, ' Training Accuracy: ', 0.93684214)\n",
      "('Epoch: ', 23, ' Training Loss: ', 3.1688845, ' Training Accuracy: ', 0.93684214)\n",
      "('Epoch: ', 24, ' Training Loss: ', 3.0372252, ' Training Accuracy: ', 0.9368422)\n",
      "('Epoch: ', 25, ' Training Loss: ', 2.9153206, ' Training Accuracy: ', 0.9368422)\n",
      "('Epoch: ', 26, ' Training Loss: ', 2.8022509, ' Training Accuracy: ', 0.94210529)\n",
      "('Epoch: ', 27, ' Training Loss: ', 2.696795, ' Training Accuracy: ', 0.94210529)\n",
      "('Epoch: ', 28, ' Training Loss: ', 2.5989294, ' Training Accuracy: ', 0.94210529)\n",
      "('Epoch: ', 29, ' Training Loss: ', 2.507539, ' Training Accuracy: ', 0.94210529)\n",
      "('Epoch: ', 30, ' Training Loss: ', 2.421562, ' Training Accuracy: ', 0.94210535)\n",
      "('Epoch: ', 31, ' Training Loss: ', 2.3407297, ' Training Accuracy: ', 0.94210535)\n",
      "('Epoch: ', 32, ' Training Loss: ', 2.2642159, ' Training Accuracy: ', 0.94210529)\n",
      "('Epoch: ', 33, ' Training Loss: ', 2.191865, ' Training Accuracy: ', 0.94210529)\n",
      "('Epoch: ', 34, ' Training Loss: ', 2.1232226, ' Training Accuracy: ', 0.94736844)\n",
      "('Epoch: ', 35, ' Training Loss: ', 2.0586705, ' Training Accuracy: ', 0.9473685)\n",
      "('Epoch: ', 36, ' Training Loss: ', 1.9976393, ' Training Accuracy: ', 0.94736844)\n",
      "('Epoch: ', 37, ' Training Loss: ', 1.9391203, ' Training Accuracy: ', 0.9473685)\n",
      "('Epoch: ', 38, ' Training Loss: ', 1.8834544, ' Training Accuracy: ', 0.94736844)\n",
      "('Epoch: ', 39, ' Training Loss: ', 1.83049, ' Training Accuracy: ', 0.9473685)\n",
      "('Epoch: ', 40, ' Training Loss: ', 1.7797136, ' Training Accuracy: ', 0.94736844)\n",
      "('Epoch: ', 41, ' Training Loss: ', 1.7309291, ' Training Accuracy: ', 0.9473685)\n",
      "('Epoch: ', 42, ' Training Loss: ', 1.6842217, ' Training Accuracy: ', 0.9473685)\n",
      "('Epoch: ', 43, ' Training Loss: ', 1.6394272, ' Training Accuracy: ', 0.94736844)\n",
      "('Epoch: ', 44, ' Training Loss: ', 1.5965271, ' Training Accuracy: ', 0.94736844)\n",
      "('Epoch: ', 45, ' Training Loss: ', 1.5553746, ' Training Accuracy: ', 0.94736844)\n",
      "('Epoch: ', 46, ' Training Loss: ', 1.5156847, ' Training Accuracy: ', 0.9473685)\n",
      "('Epoch: ', 47, ' Training Loss: ', 1.4777012, ' Training Accuracy: ', 0.94736856)\n",
      "('Epoch: ', 48, ' Training Loss: ', 1.4410895, ' Training Accuracy: ', 0.9473685)\n",
      "('Epoch: ', 49, ' Training Loss: ', 1.4059134, ' Training Accuracy: ', 0.94736844)\n",
      "('Epoch: ', 50, ' Training Loss: ', 1.3719172, ' Training Accuracy: ', 0.9473685)\n",
      "('Epoch: ', 51, ' Training Loss: ', 1.3395391, ' Training Accuracy: ', 0.95263171)\n",
      "('Epoch: ', 52, ' Training Loss: ', 1.3081422, ' Training Accuracy: ', 0.95263165)\n",
      "('Epoch: ', 53, ' Training Loss: ', 1.277983, ' Training Accuracy: ', 0.95263165)\n",
      "('Epoch: ', 54, ' Training Loss: ', 1.2488259, ' Training Accuracy: ', 0.95263159)\n",
      "('Epoch: ', 55, ' Training Loss: ', 1.2206526, ' Training Accuracy: ', 0.95263165)\n",
      "('Epoch: ', 56, ' Training Loss: ', 1.193414, ' Training Accuracy: ', 0.95263165)\n",
      "('Epoch: ', 57, ' Training Loss: ', 1.1672614, ' Training Accuracy: ', 0.95263165)\n",
      "('Epoch: ', 58, ' Training Loss: ', 1.1419196, ' Training Accuracy: ', 0.95263171)\n",
      "('Epoch: ', 59, ' Training Loss: ', 1.1176386, ' Training Accuracy: ', 0.95263165)\n",
      "('Epoch: ', 60, ' Training Loss: ', 1.0941024, ' Training Accuracy: ', 0.95263159)\n",
      "('Epoch: ', 61, ' Training Loss: ', 1.0712903, ' Training Accuracy: ', 0.95263165)\n",
      "('Epoch: ', 62, ' Training Loss: ', 1.0494112, ' Training Accuracy: ', 0.95263165)\n",
      "('Epoch: ', 63, ' Training Loss: ', 1.0280824, ' Training Accuracy: ', 0.95263159)\n",
      "('Epoch: ', 64, ' Training Loss: ', 1.007549, ' Training Accuracy: ', 0.95789474)\n",
      "('Epoch: ', 65, ' Training Loss: ', 0.98766786, ' Training Accuracy: ', 0.96315795)\n",
      "('Epoch: ', 66, ' Training Loss: ', 0.96835017, ' Training Accuracy: ', 0.96315789)\n",
      "('Epoch: ', 67, ' Training Loss: ', 0.94974375, ' Training Accuracy: ', 0.96315789)\n",
      "('Epoch: ', 68, ' Training Loss: ', 0.93159521, ' Training Accuracy: ', 0.96842116)\n",
      "('Epoch: ', 69, ' Training Loss: ', 0.91407609, ' Training Accuracy: ', 0.9684211)\n",
      "('Epoch: ', 70, ' Training Loss: ', 0.89719909, ' Training Accuracy: ', 0.9684211)\n",
      "('Epoch: ', 71, ' Training Loss: ', 0.88064969, ' Training Accuracy: ', 0.9684211)\n",
      "('Epoch: ', 72, ' Training Loss: ', 0.86479211, ' Training Accuracy: ', 0.9684211)\n",
      "('Epoch: ', 73, ' Training Loss: ', 0.84934556, ' Training Accuracy: ', 0.9684211)\n",
      "('Epoch: ', 74, ' Training Loss: ', 0.8343333, ' Training Accuracy: ', 0.96842116)\n",
      "('Epoch: ', 75, ' Training Loss: ', 0.81975764, ' Training Accuracy: ', 0.9684211)\n",
      "('Epoch: ', 76, ' Training Loss: ', 0.80569822, ' Training Accuracy: ', 0.96842104)\n",
      "('Epoch: ', 77, ' Training Loss: ', 0.79187381, ' Training Accuracy: ', 0.9684211)\n",
      "('Epoch: ', 78, ' Training Loss: ', 0.77864265, ' Training Accuracy: ', 0.96842116)\n",
      "('Epoch: ', 79, ' Training Loss: ', 0.76579189, ' Training Accuracy: ', 0.96842116)\n",
      "('Epoch: ', 80, ' Training Loss: ', 0.75319743, ' Training Accuracy: ', 0.96842116)\n",
      "('Epoch: ', 81, ' Training Loss: ', 0.74092531, ' Training Accuracy: ', 0.9684211)\n",
      "('Epoch: ', 82, ' Training Loss: ', 0.72906697, ' Training Accuracy: ', 0.96842116)\n",
      "('Epoch: ', 83, ' Training Loss: ', 0.71758473, ' Training Accuracy: ', 0.9684211)\n",
      "('Epoch: ', 84, ' Training Loss: ', 0.70623857, ' Training Accuracy: ', 0.9684211)\n",
      "('Epoch: ', 85, ' Training Loss: ', 0.69534332, ' Training Accuracy: ', 0.9684211)\n",
      "('Epoch: ', 86, ' Training Loss: ', 0.68477094, ' Training Accuracy: ', 0.9684211)\n",
      "('Epoch: ', 87, ' Training Loss: ', 0.67440146, ' Training Accuracy: ', 0.9684211)\n",
      "('Epoch: ', 88, ' Training Loss: ', 0.66426176, ' Training Accuracy: ', 0.96842116)\n",
      "('Epoch: ', 89, ' Training Loss: ', 0.65442109, ' Training Accuracy: ', 0.9684211)\n",
      "('Epoch: ', 90, ' Training Loss: ', 0.64488357, ' Training Accuracy: ', 0.96842104)\n",
      "('Epoch: ', 91, ' Training Loss: ', 0.63547277, ' Training Accuracy: ', 0.96842104)\n",
      "('Epoch: ', 92, ' Training Loss: ', 0.62637091, ' Training Accuracy: ', 0.97368431)\n",
      "('Epoch: ', 93, ' Training Loss: ', 0.61750042, ' Training Accuracy: ', 0.97368425)\n",
      "('Epoch: ', 94, ' Training Loss: ', 0.60870516, ' Training Accuracy: ', 0.97368431)\n",
      "('Epoch: ', 95, ' Training Loss: ', 0.6002965, ' Training Accuracy: ', 0.97368425)\n",
      "('Epoch: ', 96, ' Training Loss: ', 0.59192514, ' Training Accuracy: ', 0.97368431)\n",
      "('Epoch: ', 97, ' Training Loss: ', 0.58382744, ' Training Accuracy: ', 0.97368425)\n",
      "('Epoch: ', 98, ' Training Loss: ', 0.5759545, ' Training Accuracy: ', 0.97368425)\n",
      "('Epoch: ', 99, ' Training Loss: ', 0.56816852, ' Training Accuracy: ', 0.97368419)\n",
      "('Testing Accuracy:', 0.95454544)\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"cost\"):\n",
    "    loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(\n",
    "                learning_rate = learning_rate).minimize(loss)\n",
    "    # Add scalar summary for cost\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    # Add scalar summary for accuracy\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "print(\"net work done\")\n",
    "\n",
    "cost_history = np.empty(shape=[1],dtype=float)\n",
    "\n",
    "with tf.Session(config=session_conf) as session:\n",
    "    # create a log writer. run 'tensorboard --logdir=./logs/nn_logs'\n",
    "    writer = tf.summary.FileWriter(\"data/cnn_logs\", session.graph)  # for 1.0\n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        for b in range(total_batches):    \n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "            train_indices = np.arange(len(train_x))  # Get A Test Batch\n",
    "            np.random.shuffle(train_indices)\n",
    "            train_indices = train_indices[0: 1000]\n",
    "            summary, acc = session.run([merged, accuracy], \n",
    "                            feed_dict={X: train_x[train_indices], \n",
    "                                       Y: train_y[train_indices]})\n",
    "        print(\"Epoch: \", epoch, \" Training Loss: \", c, \" Training Accuracy: \", acc)\n",
    "        writer.add_summary(summary, epoch)  # Write summary\n",
    "\n",
    "    print(\"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y}))\n",
    "    writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# get_ipython().magic(u'matplotlib inline')\n",
    "# %matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gpu environment config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISUAL_DEVICES\"] = '1'\n",
    "session_conf = tf.ConfigProto()\n",
    "session_conf.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
    "#session_conf.gpu_options.allow_growth = True\n",
    "# session_conf.gpu_options.allocator_type = \"BFC\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# necessary funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    column_names = ['user-id','activity','timestamp', 'x-axis', 'y-axis', 'z-axis']\n",
    "    data = pd.read_csv(file_path,header = None, names = column_names, low_memory=False)\n",
    "    # print(data[0])\n",
    "    return data\n",
    "\n",
    "def feature_normalize(dataset):\n",
    "    mu = np.mean(dataset,axis = 0)\n",
    "    sigma = np.std(dataset,axis = 0)\n",
    "    return (dataset - mu)/sigma\n",
    "    \n",
    "def plot_axis(ax, x, y, title):\n",
    "    ax.plot(x, y)\n",
    "    ax.set_title(title)\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.set_ylim([min(y) - np.std(y), max(y) + np.std(y)])\n",
    "    ax.set_xlim([min(x), max(x)])\n",
    "    ax.grid(True)\n",
    "    \n",
    "def plot_activity(activity,data):\n",
    "    fig, (ax0, ax1, ax2) = plt.subplots(nrows = 3, figsize = (15, 10), sharex = True)\n",
    "    plot_axis(ax0, data['timestamp'], data['x-axis'], 'x-axis')\n",
    "    plot_axis(ax1, data['timestamp'], data['y-axis'], 'y-axis')\n",
    "    plot_axis(ax2, data['timestamp'], data['z-axis'], 'z-axis')\n",
    "    plt.subplots_adjust(hspace=0.2)\n",
    "    fig.suptitle(activity)\n",
    "    plt.subplots_adjust(top=0.90)\n",
    "    plt.show()\n",
    "    \n",
    "def windows(data, size):\n",
    "    start = 0\n",
    "    while start < data.count():\n",
    "        yield start, start + size\n",
    "        start += (size / 2)\n",
    "\n",
    "def segment_signal(data,window_size = 90):\n",
    "    segments = np.empty((0,window_size,3))\n",
    "    labels = np.empty((0))\n",
    "    for (start, end) in windows(data['timestamp'], window_size):\n",
    "        x = data[\"x-axis\"][start:end]\n",
    "        y = data[\"y-axis\"][start:end]\n",
    "        z = data[\"z-axis\"][start:end]\n",
    "        if(len(dataset['timestamp'][start:end]) == window_size):\n",
    "            segments = np.vstack([segments,np.dstack([x,y,z])])\n",
    "            labels = np.append(labels,stats.mode(data[\"activity\"][start:end])[0][0])\n",
    "    return segments, labels\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.0, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def depthwise_conv2d(x, W):\n",
    "    return tf.nn.depthwise_conv2d(x,W, [1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "def apply_depthwise_conv(x,kernel_size,num_channels,depth):\n",
    "    weights = weight_variable([1, kernel_size, num_channels, depth])\n",
    "    biases = bias_variable([depth * num_channels])\n",
    "    return tf.nn.relu(tf.add(depthwise_conv2d(x, weights),biases))\n",
    "    \n",
    "def apply_max_pool(x,kernel_size,stride_size):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, kernel_size, 1], \n",
    "                          strides=[1, 1, stride_size, 1], padding='VALID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (190, 1, 90, 3) (190, 6) (88, 1, 90, 3) (88, 6)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "dataset = read_data('data/actitracker_raw.txt')\n",
    "dataset['x-axis'] = feature_normalize(dataset['x-axis'])\n",
    "dataset['y-axis'] = feature_normalize(dataset['y-axis'])\n",
    "dataset['z-axis'] = feature_normalize(dataset['z-axis'])\n",
    "for activity in np.unique(dataset[\"activity\"]):\n",
    "    subset = dataset[dataset[\"activity\"] == activity][:180]\n",
    "    plot_activity(activity,subset)\n",
    "\n",
    "segments, labels = segment_signal(dataset)\n",
    "segments, labels = segment_signal(dataset)\n",
    "segmentsData = open('segmentData.pkl', 'wb')\n",
    "pickle.dump(segments, segmentsData)\n",
    "labelsData = open('labelsData.pkl', 'wb')\n",
    "pickle.dump(labels, labelsData)\n",
    "\n",
    "labels = np.asarray(pd.get_dummies(labels), dtype = np.int8)\n",
    "reshaped_segments = segments.reshape(len(segments), 1,90, 3)\n",
    "\n",
    "train_test_split = np.random.rand(len(reshaped_segments)) < 0.70\n",
    "train_x = reshaped_segments[train_test_split]\n",
    "train_y = labels[train_test_split]\n",
    "test_x = reshaped_segments[~train_test_split]\n",
    "test_y = labels[~train_test_split]\n",
    "'''\n",
    "\n",
    "processedData = open('data/processedData.pkl', 'rb')\n",
    "processedData = pickle.load(processedData)\n",
    "\n",
    "train_x, train_y, test_x, test_y = processedData[0],\\\n",
    "        processedData[1], processedData[2], processedData[3]\n",
    "print train_x.shape, train_y.shape, test_x.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# network config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_height = 1\n",
    "input_width = 90\n",
    "num_labels = 6\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 10\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 1000\n",
    "\n",
    "learning_rate = 0.0001\n",
    "training_epochs = 100\n",
    "lstm_size = 128\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "\n",
    "n_classes = 6\n",
    "n_hidden = 128\n",
    "n_inputs = 180\n",
    "\n",
    "rnnW = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_inputs, n_hidden])),\n",
    "    'output': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "rnnBiases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_hidden], mean=1.0)),\n",
    "    'output': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build cnn_lstm network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c2 = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "c2Reshape = tf.reshape(c2, [-1, 6, 180])\n",
    "shuff = tf.transpose(c2Reshape, [1, 0, 2])\n",
    "shuff = tf.reshape(shuff, [-1, n_inputs])\n",
    "\n",
    "# Linear activation, reshaping inputs to the LSTM's number of hidden:\n",
    "hidden = tf.nn.relu(tf.matmul(\n",
    "    shuff, rnnW['hidden']\n",
    ") + rnnBiases['hidden'])\n",
    "# X_split = tf.split(shuff, 8, 0) # split them to time_step_size (28 arrays)\n",
    "# X_split = tf.unstack(shuff) # split them to time_step_size (28 arrays)\n",
    "\n",
    "# Split the series because the rnn cell needs time_steps features, each of shape:\n",
    "hidden = tf.split(axis=0, num_or_size_splits=6, value=hidden)\n",
    "\n",
    "lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(lstm_size, forget_bias=1.0, \n",
    "                                          state_is_tuple=True)\n",
    "lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(lstm_size, forget_bias=1.0, \n",
    "                                          state_is_tuple=True)\n",
    "\n",
    "# Stack two LSTM layers, both layers has the same shape\n",
    "lstm_layers = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], \n",
    "                                          state_is_tuple=True)\n",
    "\n",
    "lstmOutputs, _ = tf.contrib.rnn.static_rnn(lstm_layers, hidden, dtype=tf.float32)\n",
    "lstmLastOutput = lstmOutputs[-1]\n",
    "y_ = tf.matmul(lstmLastOutput, rnnW['output']) + rnnBiases['output']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net work done\n",
      "Epoch:  0  Training Loss:  1.18271  Training Accuracy:  0.602273\n",
      "Epoch:  1  Training Loss:  0.90019  Training Accuracy:  0.602273\n",
      "Epoch:  2  Training Loss:  0.805071  Training Accuracy:  0.602273\n",
      "Epoch:  3  Training Loss:  0.762008  Training Accuracy:  0.613636\n",
      "Epoch:  4  Training Loss:  0.735776  Training Accuracy:  0.613636\n",
      "Epoch:  5  Training Loss:  0.715883  Training Accuracy:  0.625\n",
      "Epoch:  6  Training Loss:  0.699208  Training Accuracy:  0.625\n",
      "Epoch:  7  Training Loss:  0.684748  Training Accuracy:  0.625\n",
      "Epoch:  8  Training Loss:  0.672141  Training Accuracy:  0.636364\n",
      "Epoch:  9  Training Loss:  0.661237  Training Accuracy:  0.636364\n",
      "Epoch:  10  Training Loss:  0.651847  Training Accuracy:  0.636364\n",
      "Epoch:  11  Training Loss:  0.64379  Training Accuracy:  0.647727\n",
      "Epoch:  12  Training Loss:  0.636858  Training Accuracy:  0.647727\n",
      "Epoch:  13  Training Loss:  0.630909  Training Accuracy:  0.647727\n",
      "Epoch:  14  Training Loss:  0.625776  Training Accuracy:  0.647727\n",
      "Epoch:  15  Training Loss:  0.621337  Training Accuracy:  0.659091\n",
      "Epoch:  16  Training Loss:  0.617466  Training Accuracy:  0.681818\n",
      "Epoch:  17  Training Loss:  0.61406  Training Accuracy:  0.681818\n",
      "Epoch:  18  Training Loss:  0.611015  Training Accuracy:  0.681818\n",
      "Epoch:  19  Training Loss:  0.608279  Training Accuracy:  0.681818\n",
      "Epoch:  20  Training Loss:  0.605773  Training Accuracy:  0.681818\n",
      "Epoch:  21  Training Loss:  0.603424  Training Accuracy:  0.681818\n",
      "Epoch:  22  Training Loss:  0.601204  Training Accuracy:  0.681818\n",
      "Epoch:  23  Training Loss:  0.599064  Training Accuracy:  0.681818\n",
      "Epoch:  24  Training Loss:  0.596997  Training Accuracy:  0.693182\n",
      "Epoch:  25  Training Loss:  0.594956  Training Accuracy:  0.693182\n",
      "Epoch:  26  Training Loss:  0.592918  Training Accuracy:  0.693182\n",
      "Epoch:  27  Training Loss:  0.59086  Training Accuracy:  0.693182\n",
      "Epoch:  28  Training Loss:  0.588764  Training Accuracy:  0.693182\n",
      "Epoch:  29  Training Loss:  0.586627  Training Accuracy:  0.693182\n",
      "Epoch:  30  Training Loss:  0.584441  Training Accuracy:  0.693182\n",
      "Epoch:  31  Training Loss:  0.582205  Training Accuracy:  0.693182\n",
      "Epoch:  32  Training Loss:  0.579895  Training Accuracy:  0.693182\n",
      "Epoch:  33  Training Loss:  0.577527  Training Accuracy:  0.704545\n",
      "Epoch:  34  Training Loss:  0.57509  Training Accuracy:  0.704545\n",
      "Epoch:  35  Training Loss:  0.572587  Training Accuracy:  0.704545\n",
      "Epoch:  36  Training Loss:  0.570007  Training Accuracy:  0.704545\n",
      "Epoch:  37  Training Loss:  0.567379  Training Accuracy:  0.704545\n",
      "Epoch:  38  Training Loss:  0.56467  Training Accuracy:  0.704545\n",
      "Epoch:  39  Training Loss:  0.561911  Training Accuracy:  0.704545\n",
      "Epoch:  40  Training Loss:  0.559085  Training Accuracy:  0.704545\n",
      "Epoch:  41  Training Loss:  0.556205  Training Accuracy:  0.704545\n",
      "Epoch:  42  Training Loss:  0.553258  Training Accuracy:  0.704545\n",
      "Epoch:  43  Training Loss:  0.550262  Training Accuracy:  0.704545\n",
      "Epoch:  44  Training Loss:  0.5472  Training Accuracy:  0.704545\n",
      "Epoch:  45  Training Loss:  0.544109  Training Accuracy:  0.681818\n",
      "Epoch:  46  Training Loss:  0.540974  Training Accuracy:  0.704545\n",
      "Epoch:  47  Training Loss:  0.537779  Training Accuracy:  0.704545\n",
      "Epoch:  48  Training Loss:  0.53453  Training Accuracy:  0.704545\n",
      "Epoch:  49  Training Loss:  0.531236  Training Accuracy:  0.715909\n",
      "Epoch:  50  Training Loss:  0.527901  Training Accuracy:  0.715909\n",
      "Epoch:  51  Training Loss:  0.524538  Training Accuracy:  0.727273\n",
      "Epoch:  52  Training Loss:  0.521145  Training Accuracy:  0.738636\n",
      "Epoch:  53  Training Loss:  0.517728  Training Accuracy:  0.738636\n",
      "Epoch:  54  Training Loss:  0.514275  Training Accuracy:  0.761364\n",
      "Epoch:  55  Training Loss:  0.510799  Training Accuracy:  0.784091\n",
      "Epoch:  56  Training Loss:  0.507286  Training Accuracy:  0.806818\n",
      "Epoch:  57  Training Loss:  0.503764  Training Accuracy:  0.806818\n",
      "Epoch:  58  Training Loss:  0.500211  Training Accuracy:  0.806818\n",
      "Epoch:  59  Training Loss:  0.496645  Training Accuracy:  0.806818\n",
      "Epoch:  60  Training Loss:  0.493063  Training Accuracy:  0.806818\n",
      "Epoch:  61  Training Loss:  0.48946  Training Accuracy:  0.806818\n",
      "Epoch:  62  Training Loss:  0.485858  Training Accuracy:  0.806818\n",
      "Epoch:  63  Training Loss:  0.482253  Training Accuracy:  0.806818\n",
      "Epoch:  64  Training Loss:  0.478668  Training Accuracy:  0.806818\n",
      "Epoch:  65  Training Loss:  0.475067  Training Accuracy:  0.818182\n",
      "Epoch:  66  Training Loss:  0.471461  Training Accuracy:  0.829545\n",
      "Epoch:  67  Training Loss:  0.467843  Training Accuracy:  0.840909\n",
      "Epoch:  68  Training Loss:  0.464238  Training Accuracy:  0.863636\n",
      "Epoch:  69  Training Loss:  0.46064  Training Accuracy:  0.875\n",
      "Epoch:  70  Training Loss:  0.457039  Training Accuracy:  0.875\n",
      "Epoch:  71  Training Loss:  0.453458  Training Accuracy:  0.886364\n",
      "Epoch:  72  Training Loss:  0.44987  Training Accuracy:  0.886364\n",
      "Epoch:  73  Training Loss:  0.446278  Training Accuracy:  0.897727\n",
      "Epoch:  74  Training Loss:  0.442707  Training Accuracy:  0.897727\n",
      "Epoch:  75  Training Loss:  0.439139  Training Accuracy:  0.897727\n",
      "Epoch:  76  Training Loss:  0.435579  Training Accuracy:  0.897727\n",
      "Epoch:  77  Training Loss:  0.43203  Training Accuracy:  0.897727\n",
      "Epoch:  78  Training Loss:  0.428501  Training Accuracy:  0.897727\n",
      "Epoch:  79  Training Loss:  0.424976  Training Accuracy:  0.897727\n",
      "Epoch:  80  Training Loss:  0.421474  Training Accuracy:  0.897727\n",
      "Epoch:  81  Training Loss:  0.417988  Training Accuracy:  0.897727\n",
      "Epoch:  82  Training Loss:  0.414498  Training Accuracy:  0.897727\n",
      "Epoch:  83  Training Loss:  0.411032  Training Accuracy:  0.920455\n",
      "Epoch:  84  Training Loss:  0.407603  Training Accuracy:  0.920455\n",
      "Epoch:  85  Training Loss:  0.404178  Training Accuracy:  0.931818\n",
      "Epoch:  86  Training Loss:  0.400772  Training Accuracy:  0.931818\n",
      "Epoch:  87  Training Loss:  0.397399  Training Accuracy:  0.931818\n",
      "Epoch:  88  Training Loss:  0.394042  Training Accuracy:  0.931818\n",
      "Epoch:  89  Training Loss:  0.390713  Training Accuracy:  0.931818\n",
      "Epoch:  90  Training Loss:  0.387392  Training Accuracy:  0.931818\n",
      "Epoch:  91  Training Loss:  0.384084  Training Accuracy:  0.931818\n",
      "Epoch:  92  Training Loss:  0.380801  Training Accuracy:  0.931818\n",
      "Epoch:  93  Training Loss:  0.377547  Training Accuracy:  0.931818\n",
      "Epoch:  94  Training Loss:  0.374314  Training Accuracy:  0.931818\n",
      "Epoch:  95  Training Loss:  0.371109  Training Accuracy:  0.931818\n",
      "Epoch:  96  Training Loss:  0.367927  Training Accuracy:  0.931818\n",
      "Epoch:  97  Training Loss:  0.364745  Training Accuracy:  0.931818\n",
      "Epoch:  98  Training Loss:  0.361599  Training Accuracy:  0.931818\n",
      "Epoch:  99  Training Loss:  0.358467  Training Accuracy:  0.931818\n",
      "Testing Accuracy: 0.931818\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"cost\"):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=Y))\n",
    "    # optimizer = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(loss)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=\n",
    "                                                  learning_rate).minimize(loss)\n",
    "\n",
    "    # Add scalar summary for cost\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(y_, 1)) \n",
    "    # Count correct predictions\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) \n",
    "    # Cast boolean to float to average\n",
    "    # Add scalar summary for accuracy\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "\n",
    "print(\"net work done\")\n",
    "\n",
    "cost_history = np.empty(shape=[1],dtype=float)\n",
    "\n",
    "with tf.Session(config=session_conf) as session:\n",
    "    # create a log writer. run 'tensorboard --logdir=./logs/nn_logs'\n",
    "    writer = tf.summary.FileWriter(\"data/cnn_lstm_logs\", session.graph)  # for 1.0\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        for b in range(total_batches):    \n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "\n",
    "            test_indices = np.arange(len(test_x))  # Get A Test Batch\n",
    "            np.random.shuffle(test_indices)\n",
    "            test_indices = test_indices[0: 1000]\n",
    "            summary, acc=session.run([merged, accuracy], \n",
    "                        feed_dict={X: test_x[test_indices], Y: test_y[test_indices]})\n",
    "        print \"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \", acc\n",
    "        writer.add_summary(summary, epoch)  # Write summary\n",
    "\n",
    "    print \"Testing Accuracy:\", session.run(accuracy, feed_dict={X: test_x, Y: test_y})\n",
    "    writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

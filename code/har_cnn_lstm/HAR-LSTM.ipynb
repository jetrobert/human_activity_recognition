{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# HAR LSTM training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import os\n",
    "from utils.utilities import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, labels_train, list_ch_train = read_data(data_path=\"./data/\", split=\"train\") # train\n",
    "X_test, labels_test, list_ch_test = read_data(data_path=\"./data/\", split=\"train\") # test\n",
    "\n",
    "assert list_ch_train == list_ch_test, \"Mistmatch in channels!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Standardize\n",
    "X_train, X_test = standardize(X_train, X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_tr, X_vld, lab_tr, lab_vld = train_test_split(X_train, labels_train, \n",
    "                                                stratify = labels_train, test_size = 0.2,\n",
    "                                                random_state = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "One-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_tr = one_hot(lab_tr)\n",
    "y_vld = one_hot(lab_vld)\n",
    "y_test = one_hot(labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "\n",
    "lstm_size = 27         # 2 times the amount of channels\n",
    "lstm_layers = 2        # Number of layers\n",
    "batch_size = 600       # Batch size\n",
    "seq_len = 128          # Number of steps\n",
    "learning_rate = 0.0005  # Learning rate (default is 0.001)\n",
    "epochs = 500\n",
    "\n",
    "# Fixed\n",
    "n_classes = 6\n",
    "n_channels = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Construct the graph\n",
    "Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "# Construct placeholders\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.float32, [None, seq_len, n_channels], name = 'inputs')\n",
    "    labels_ = tf.placeholder(tf.float32, [None, n_classes], name = 'labels')\n",
    "    keep_prob_ = tf.placeholder(tf.float32, name = 'keep')\n",
    "    learning_rate_ = tf.placeholder(tf.float32, name = 'learning_rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Construct inputs to LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    # Construct the LSTM inputs and LSTM cells\n",
    "    lstm_in = tf.transpose(inputs_, [1,0,2]) # reshape into (seq_len, N, channels)\n",
    "    lstm_in = tf.reshape(lstm_in, [-1, n_channels]) # Now (seq_len*N, n_channels)\n",
    "    \n",
    "    # To cells\n",
    "    lstm_in = tf.layers.dense(lstm_in, lstm_size, activation=None) # or tf.nn.relu, tf.nn.sigmoid, tf.nn.tanh?\n",
    "    \n",
    "    # Open up the tensor into a list of seq_len pieces\n",
    "    lstm_in = tf.split(lstm_in, seq_len, 0)\n",
    "    \n",
    "    # Add LSTM layers\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob_)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Define forward pass, cost function and optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    outputs, final_state = tf.contrib.rnn.static_rnn(cell, lstm_in, dtype=tf.float32,\n",
    "                                                     initial_state = initial_state)\n",
    "    \n",
    "    # We only need the last output tensor to pass into a classifier\n",
    "    logits = tf.layers.dense(outputs[-1], n_classes, name='logits')\n",
    "    \n",
    "    # Cost function and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_))\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate_).minimize(cost) # No grad clipping\n",
    "    \n",
    "    # Grad clipping\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate_)\n",
    "\n",
    "    gradients = train_op.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]\n",
    "    optimizer = train_op.apply_gradients(capped_gradients)\n",
    "    \n",
    "    # Accuracy\n",
    "    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if (os.path.exists('checkpoints') == False):\n",
    "    !mkdir checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/500 Iteration: 5 Train loss: 1.791498 Train acc: 0.215000\n",
      "Epoch: 1/500 Iteration: 10 Train loss: 1.776630 Train acc: 0.236667\n",
      "Epoch: 1/500 Iteration: 15 Train loss: 1.768128 Train acc: 0.221667\n",
      "Epoch: 2/500 Iteration: 20 Train loss: 1.763054 Train acc: 0.241667\n",
      "Epoch: 2/500 Iteration: 25 Train loss: 1.760321 Train acc: 0.240000\n",
      "Epoch: 2/500 Iteration: 25 Validation loss: 1.750999 Validation acc: 0.249167\n",
      "Epoch: 3/500 Iteration: 30 Train loss: 1.760731 Train acc: 0.226667\n",
      "Epoch: 3/500 Iteration: 35 Train loss: 1.743721 Train acc: 0.250000\n",
      "Epoch: 4/500 Iteration: 40 Train loss: 1.737467 Train acc: 0.263333\n",
      "Epoch: 4/500 Iteration: 45 Train loss: 1.737867 Train acc: 0.248333\n",
      "Epoch: 5/500 Iteration: 50 Train loss: 1.695771 Train acc: 0.298333\n",
      "Epoch: 5/500 Iteration: 50 Validation loss: 1.702012 Validation acc: 0.300000\n",
      "Epoch: 6/500 Iteration: 55 Train loss: 1.691932 Train acc: 0.293333\n",
      "Epoch: 6/500 Iteration: 60 Train loss: 1.670525 Train acc: 0.325000\n",
      "Epoch: 7/500 Iteration: 65 Train loss: 1.638698 Train acc: 0.353333\n",
      "Epoch: 7/500 Iteration: 70 Train loss: 1.639620 Train acc: 0.346667\n",
      "Epoch: 8/500 Iteration: 75 Train loss: 1.588918 Train acc: 0.378333\n",
      "Epoch: 8/500 Iteration: 75 Validation loss: 1.548995 Validation acc: 0.420000\n",
      "Epoch: 8/500 Iteration: 80 Train loss: 1.553913 Train acc: 0.390000\n",
      "Epoch: 9/500 Iteration: 85 Train loss: 1.458812 Train acc: 0.465000\n",
      "Epoch: 9/500 Iteration: 90 Train loss: 1.416948 Train acc: 0.461667\n",
      "Epoch: 10/500 Iteration: 95 Train loss: 1.372930 Train acc: 0.471667\n",
      "Epoch: 11/500 Iteration: 100 Train loss: 1.244371 Train acc: 0.543333\n",
      "Epoch: 11/500 Iteration: 100 Validation loss: 1.254650 Validation acc: 0.490000\n",
      "Epoch: 11/500 Iteration: 105 Train loss: 1.234558 Train acc: 0.571667\n",
      "Epoch: 12/500 Iteration: 110 Train loss: 1.253111 Train acc: 0.503333\n",
      "Epoch: 12/500 Iteration: 115 Train loss: 1.200146 Train acc: 0.515000\n",
      "Epoch: 13/500 Iteration: 120 Train loss: 1.175406 Train acc: 0.541667\n",
      "Epoch: 13/500 Iteration: 125 Train loss: 1.159325 Train acc: 0.565000\n",
      "Epoch: 13/500 Iteration: 125 Validation loss: 1.090242 Validation acc: 0.561667\n",
      "Epoch: 14/500 Iteration: 130 Train loss: 1.105328 Train acc: 0.565000\n",
      "Epoch: 14/500 Iteration: 135 Train loss: 1.088497 Train acc: 0.578333\n",
      "Epoch: 15/500 Iteration: 140 Train loss: 1.094746 Train acc: 0.580000\n",
      "Epoch: 16/500 Iteration: 145 Train loss: 0.984737 Train acc: 0.621667\n",
      "Epoch: 16/500 Iteration: 150 Train loss: 0.986110 Train acc: 0.623333\n",
      "Epoch: 16/500 Iteration: 150 Validation loss: 0.955179 Validation acc: 0.640000\n",
      "Epoch: 17/500 Iteration: 155 Train loss: 0.947800 Train acc: 0.660000\n",
      "Epoch: 17/500 Iteration: 160 Train loss: 0.967154 Train acc: 0.616667\n",
      "Epoch: 18/500 Iteration: 165 Train loss: 0.924476 Train acc: 0.646667\n",
      "Epoch: 18/500 Iteration: 170 Train loss: 0.943837 Train acc: 0.643333\n",
      "Epoch: 19/500 Iteration: 175 Train loss: 0.935576 Train acc: 0.645000\n",
      "Epoch: 19/500 Iteration: 175 Validation loss: 0.842389 Validation acc: 0.691667\n",
      "Epoch: 19/500 Iteration: 180 Train loss: 0.864939 Train acc: 0.683333\n",
      "Epoch: 20/500 Iteration: 185 Train loss: 0.896286 Train acc: 0.661667\n",
      "Epoch: 21/500 Iteration: 190 Train loss: 0.803592 Train acc: 0.710000\n",
      "Epoch: 21/500 Iteration: 195 Train loss: 0.789911 Train acc: 0.723333\n",
      "Epoch: 22/500 Iteration: 200 Train loss: 0.808336 Train acc: 0.718333\n",
      "Epoch: 22/500 Iteration: 200 Validation loss: 0.727418 Validation acc: 0.770000\n",
      "Epoch: 22/500 Iteration: 205 Train loss: 0.767047 Train acc: 0.716667\n",
      "Epoch: 23/500 Iteration: 210 Train loss: 0.745196 Train acc: 0.753333\n",
      "Epoch: 23/500 Iteration: 215 Train loss: 0.774541 Train acc: 0.730000\n",
      "Epoch: 24/500 Iteration: 220 Train loss: 0.712339 Train acc: 0.775000\n",
      "Epoch: 24/500 Iteration: 225 Train loss: 0.680473 Train acc: 0.780000\n",
      "Epoch: 24/500 Iteration: 225 Validation loss: 0.638248 Validation acc: 0.794167\n",
      "Epoch: 25/500 Iteration: 230 Train loss: 0.731841 Train acc: 0.770000\n",
      "Epoch: 26/500 Iteration: 235 Train loss: 0.630989 Train acc: 0.806667\n",
      "Epoch: 26/500 Iteration: 240 Train loss: 0.640914 Train acc: 0.806667\n",
      "Epoch: 27/500 Iteration: 245 Train loss: 0.670862 Train acc: 0.780000\n",
      "Epoch: 27/500 Iteration: 250 Train loss: 0.700017 Train acc: 0.783333\n",
      "Epoch: 27/500 Iteration: 250 Validation loss: 0.596326 Validation acc: 0.805000\n",
      "Epoch: 28/500 Iteration: 255 Train loss: 0.644019 Train acc: 0.801667\n",
      "Epoch: 28/500 Iteration: 260 Train loss: 0.679511 Train acc: 0.768333\n",
      "Epoch: 29/500 Iteration: 265 Train loss: 0.634652 Train acc: 0.790000\n",
      "Epoch: 29/500 Iteration: 270 Train loss: 0.621375 Train acc: 0.791667\n",
      "Epoch: 30/500 Iteration: 275 Train loss: 0.640528 Train acc: 0.790000\n",
      "Epoch: 30/500 Iteration: 275 Validation loss: 0.550787 Validation acc: 0.816667\n",
      "Epoch: 31/500 Iteration: 280 Train loss: 0.575745 Train acc: 0.820000\n",
      "Epoch: 31/500 Iteration: 285 Train loss: 0.614946 Train acc: 0.801667\n",
      "Epoch: 32/500 Iteration: 290 Train loss: 0.629672 Train acc: 0.785000\n",
      "Epoch: 32/500 Iteration: 295 Train loss: 0.573385 Train acc: 0.806667\n",
      "Epoch: 33/500 Iteration: 300 Train loss: 0.525487 Train acc: 0.830000\n",
      "Epoch: 33/500 Iteration: 300 Validation loss: 0.512892 Validation acc: 0.822500\n",
      "Epoch: 33/500 Iteration: 305 Train loss: 0.598956 Train acc: 0.801667\n",
      "Epoch: 34/500 Iteration: 310 Train loss: 0.547884 Train acc: 0.805000\n",
      "Epoch: 34/500 Iteration: 315 Train loss: 0.527089 Train acc: 0.838333\n",
      "Epoch: 35/500 Iteration: 320 Train loss: 0.582638 Train acc: 0.811667\n",
      "Epoch: 36/500 Iteration: 325 Train loss: 0.538757 Train acc: 0.816667\n",
      "Epoch: 36/500 Iteration: 325 Validation loss: 0.508749 Validation acc: 0.819167\n",
      "Epoch: 36/500 Iteration: 330 Train loss: 0.511762 Train acc: 0.828333\n",
      "Epoch: 37/500 Iteration: 335 Train loss: 0.563476 Train acc: 0.808333\n",
      "Epoch: 37/500 Iteration: 340 Train loss: 0.553163 Train acc: 0.826667\n",
      "Epoch: 38/500 Iteration: 345 Train loss: 0.514039 Train acc: 0.830000\n",
      "Epoch: 38/500 Iteration: 350 Train loss: 0.575000 Train acc: 0.800000\n",
      "Epoch: 38/500 Iteration: 350 Validation loss: 0.486985 Validation acc: 0.831667\n",
      "Epoch: 39/500 Iteration: 355 Train loss: 0.541549 Train acc: 0.806667\n",
      "Epoch: 39/500 Iteration: 360 Train loss: 0.496805 Train acc: 0.833333\n",
      "Epoch: 40/500 Iteration: 365 Train loss: 0.575828 Train acc: 0.816667\n",
      "Epoch: 41/500 Iteration: 370 Train loss: 0.506326 Train acc: 0.833333\n",
      "Epoch: 41/500 Iteration: 375 Train loss: 0.510523 Train acc: 0.821667\n",
      "Epoch: 41/500 Iteration: 375 Validation loss: 0.470296 Validation acc: 0.833333\n",
      "Epoch: 42/500 Iteration: 380 Train loss: 0.534531 Train acc: 0.818333\n",
      "Epoch: 42/500 Iteration: 385 Train loss: 0.502654 Train acc: 0.826667\n",
      "Epoch: 43/500 Iteration: 390 Train loss: 0.463214 Train acc: 0.831667\n",
      "Epoch: 43/500 Iteration: 395 Train loss: 0.531816 Train acc: 0.805000\n",
      "Epoch: 44/500 Iteration: 400 Train loss: 0.483032 Train acc: 0.850000\n",
      "Epoch: 44/500 Iteration: 400 Validation loss: 0.458797 Validation acc: 0.840833\n",
      "Epoch: 44/500 Iteration: 405 Train loss: 0.444024 Train acc: 0.856667\n",
      "Epoch: 45/500 Iteration: 410 Train loss: 0.485166 Train acc: 0.833333\n",
      "Epoch: 46/500 Iteration: 415 Train loss: 0.439044 Train acc: 0.848333\n",
      "Epoch: 46/500 Iteration: 420 Train loss: 0.471901 Train acc: 0.841667\n",
      "Epoch: 47/500 Iteration: 425 Train loss: 0.472475 Train acc: 0.850000\n",
      "Epoch: 47/500 Iteration: 425 Validation loss: 0.441067 Validation acc: 0.839167\n",
      "Epoch: 47/500 Iteration: 430 Train loss: 0.451263 Train acc: 0.851667\n",
      "Epoch: 48/500 Iteration: 435 Train loss: 0.455886 Train acc: 0.846667\n",
      "Epoch: 48/500 Iteration: 440 Train loss: 0.521019 Train acc: 0.835000\n",
      "Epoch: 49/500 Iteration: 445 Train loss: 0.453653 Train acc: 0.863333\n",
      "Epoch: 49/500 Iteration: 450 Train loss: 0.430641 Train acc: 0.858333\n",
      "Epoch: 49/500 Iteration: 450 Validation loss: 0.428773 Validation acc: 0.842500\n",
      "Epoch: 50/500 Iteration: 455 Train loss: 0.464223 Train acc: 0.850000\n",
      "Epoch: 51/500 Iteration: 460 Train loss: 0.464818 Train acc: 0.841667\n",
      "Epoch: 51/500 Iteration: 465 Train loss: 0.461829 Train acc: 0.843333\n",
      "Epoch: 52/500 Iteration: 470 Train loss: 0.446569 Train acc: 0.856667\n",
      "Epoch: 52/500 Iteration: 475 Train loss: 0.432965 Train acc: 0.853333\n",
      "Epoch: 52/500 Iteration: 475 Validation loss: 0.421311 Validation acc: 0.842500\n",
      "Epoch: 53/500 Iteration: 480 Train loss: 0.418303 Train acc: 0.855000\n",
      "Epoch: 53/500 Iteration: 485 Train loss: 0.469551 Train acc: 0.851667\n",
      "Epoch: 54/500 Iteration: 490 Train loss: 0.422675 Train acc: 0.866667\n",
      "Epoch: 54/500 Iteration: 495 Train loss: 0.570170 Train acc: 0.830000\n",
      "Epoch: 55/500 Iteration: 500 Train loss: 0.478358 Train acc: 0.850000\n",
      "Epoch: 55/500 Iteration: 500 Validation loss: 0.418586 Validation acc: 0.843333\n",
      "Epoch: 56/500 Iteration: 505 Train loss: 0.476937 Train acc: 0.836667\n",
      "Epoch: 56/500 Iteration: 510 Train loss: 0.422809 Train acc: 0.850000\n",
      "Epoch: 57/500 Iteration: 515 Train loss: 0.478687 Train acc: 0.825000\n",
      "Epoch: 57/500 Iteration: 520 Train loss: 0.438093 Train acc: 0.838333\n",
      "Epoch: 58/500 Iteration: 525 Train loss: 0.414613 Train acc: 0.848333\n",
      "Epoch: 58/500 Iteration: 525 Validation loss: 0.398915 Validation acc: 0.852500\n",
      "Epoch: 58/500 Iteration: 530 Train loss: 0.462914 Train acc: 0.845000\n",
      "Epoch: 59/500 Iteration: 535 Train loss: 0.394636 Train acc: 0.873333\n",
      "Epoch: 59/500 Iteration: 540 Train loss: 0.403358 Train acc: 0.876667\n",
      "Epoch: 60/500 Iteration: 545 Train loss: 0.424686 Train acc: 0.863333\n",
      "Epoch: 61/500 Iteration: 550 Train loss: 0.374800 Train acc: 0.881667\n",
      "Epoch: 61/500 Iteration: 550 Validation loss: 0.396012 Validation acc: 0.850833\n",
      "Epoch: 61/500 Iteration: 555 Train loss: 0.390163 Train acc: 0.861667\n",
      "Epoch: 62/500 Iteration: 560 Train loss: 0.430437 Train acc: 0.855000\n",
      "Epoch: 62/500 Iteration: 565 Train loss: 0.381071 Train acc: 0.865000\n",
      "Epoch: 63/500 Iteration: 570 Train loss: 0.349393 Train acc: 0.880000\n",
      "Epoch: 63/500 Iteration: 575 Train loss: 0.453797 Train acc: 0.848333\n",
      "Epoch: 63/500 Iteration: 575 Validation loss: 0.377281 Validation acc: 0.861667\n",
      "Epoch: 64/500 Iteration: 580 Train loss: 0.358675 Train acc: 0.888333\n",
      "Epoch: 64/500 Iteration: 585 Train loss: 0.379089 Train acc: 0.873333\n",
      "Epoch: 65/500 Iteration: 590 Train loss: 0.400415 Train acc: 0.868333\n",
      "Epoch: 66/500 Iteration: 595 Train loss: 0.365846 Train acc: 0.870000\n",
      "Epoch: 66/500 Iteration: 600 Train loss: 0.396366 Train acc: 0.863333\n",
      "Epoch: 66/500 Iteration: 600 Validation loss: 0.384638 Validation acc: 0.866667\n",
      "Epoch: 67/500 Iteration: 605 Train loss: 0.402113 Train acc: 0.858333\n",
      "Epoch: 67/500 Iteration: 610 Train loss: 0.402962 Train acc: 0.876667\n",
      "Epoch: 68/500 Iteration: 615 Train loss: 0.367965 Train acc: 0.870000\n",
      "Epoch: 68/500 Iteration: 620 Train loss: 0.436863 Train acc: 0.848333\n",
      "Epoch: 69/500 Iteration: 625 Train loss: 0.325446 Train acc: 0.906667\n",
      "Epoch: 69/500 Iteration: 625 Validation loss: 0.376130 Validation acc: 0.864167\n",
      "Epoch: 69/500 Iteration: 630 Train loss: 0.356266 Train acc: 0.895000\n",
      "Epoch: 70/500 Iteration: 635 Train loss: 0.393300 Train acc: 0.878333\n",
      "Epoch: 71/500 Iteration: 640 Train loss: 0.341038 Train acc: 0.886667\n",
      "Epoch: 71/500 Iteration: 645 Train loss: 0.343656 Train acc: 0.890000\n",
      "Epoch: 72/500 Iteration: 650 Train loss: 0.379187 Train acc: 0.885000\n",
      "Epoch: 72/500 Iteration: 650 Validation loss: 0.369238 Validation acc: 0.874167\n",
      "Epoch: 72/500 Iteration: 655 Train loss: 0.374136 Train acc: 0.883333\n",
      "Epoch: 73/500 Iteration: 660 Train loss: 0.348023 Train acc: 0.883333\n",
      "Epoch: 73/500 Iteration: 665 Train loss: 0.383975 Train acc: 0.870000\n",
      "Epoch: 74/500 Iteration: 670 Train loss: 0.317936 Train acc: 0.891667\n",
      "Epoch: 74/500 Iteration: 675 Train loss: 0.356392 Train acc: 0.883333\n",
      "Epoch: 74/500 Iteration: 675 Validation loss: 0.366965 Validation acc: 0.871667\n",
      "Epoch: 75/500 Iteration: 680 Train loss: 0.384770 Train acc: 0.870000\n",
      "Epoch: 76/500 Iteration: 685 Train loss: 0.316471 Train acc: 0.903333\n",
      "Epoch: 76/500 Iteration: 690 Train loss: 0.339558 Train acc: 0.893333\n",
      "Epoch: 77/500 Iteration: 695 Train loss: 0.365865 Train acc: 0.885000\n",
      "Epoch: 77/500 Iteration: 700 Train loss: 0.339918 Train acc: 0.891667\n",
      "Epoch: 77/500 Iteration: 700 Validation loss: 0.359179 Validation acc: 0.874167\n",
      "Epoch: 78/500 Iteration: 705 Train loss: 0.332567 Train acc: 0.893333\n",
      "Epoch: 78/500 Iteration: 710 Train loss: 0.388252 Train acc: 0.863333\n",
      "Epoch: 79/500 Iteration: 715 Train loss: 0.312522 Train acc: 0.913333\n",
      "Epoch: 79/500 Iteration: 720 Train loss: 0.299157 Train acc: 0.905000\n",
      "Epoch: 80/500 Iteration: 725 Train loss: 0.381244 Train acc: 0.883333\n",
      "Epoch: 80/500 Iteration: 725 Validation loss: 0.360218 Validation acc: 0.875833\n",
      "Epoch: 81/500 Iteration: 730 Train loss: 0.313051 Train acc: 0.903333\n",
      "Epoch: 81/500 Iteration: 735 Train loss: 0.313605 Train acc: 0.890000\n",
      "Epoch: 82/500 Iteration: 740 Train loss: 0.377155 Train acc: 0.875000\n",
      "Epoch: 82/500 Iteration: 745 Train loss: 0.332483 Train acc: 0.886667\n",
      "Epoch: 83/500 Iteration: 750 Train loss: 0.334584 Train acc: 0.901667\n",
      "Epoch: 83/500 Iteration: 750 Validation loss: 0.365749 Validation acc: 0.870833\n",
      "Epoch: 83/500 Iteration: 755 Train loss: 0.364991 Train acc: 0.888333\n",
      "Epoch: 84/500 Iteration: 760 Train loss: 0.302082 Train acc: 0.923333\n",
      "Epoch: 84/500 Iteration: 765 Train loss: 0.300206 Train acc: 0.913333\n",
      "Epoch: 85/500 Iteration: 770 Train loss: 0.349643 Train acc: 0.883333\n",
      "Epoch: 86/500 Iteration: 775 Train loss: 0.284405 Train acc: 0.916667\n",
      "Epoch: 86/500 Iteration: 775 Validation loss: 0.363475 Validation acc: 0.873333\n",
      "Epoch: 86/500 Iteration: 780 Train loss: 0.328934 Train acc: 0.896667\n",
      "Epoch: 87/500 Iteration: 785 Train loss: 0.385025 Train acc: 0.873333\n",
      "Epoch: 87/500 Iteration: 790 Train loss: 0.317731 Train acc: 0.895000\n",
      "Epoch: 88/500 Iteration: 795 Train loss: 0.312806 Train acc: 0.903333\n",
      "Epoch: 88/500 Iteration: 800 Train loss: 0.377128 Train acc: 0.878333\n",
      "Epoch: 88/500 Iteration: 800 Validation loss: 0.362607 Validation acc: 0.871667\n",
      "Epoch: 89/500 Iteration: 805 Train loss: 0.292105 Train acc: 0.925000\n",
      "Epoch: 89/500 Iteration: 810 Train loss: 0.309225 Train acc: 0.901667\n",
      "Epoch: 90/500 Iteration: 815 Train loss: 0.338523 Train acc: 0.893333\n",
      "Epoch: 91/500 Iteration: 820 Train loss: 0.294639 Train acc: 0.905000\n",
      "Epoch: 91/500 Iteration: 825 Train loss: 0.321980 Train acc: 0.895000\n",
      "Epoch: 91/500 Iteration: 825 Validation loss: 0.352371 Validation acc: 0.880833\n",
      "Epoch: 92/500 Iteration: 830 Train loss: 0.380469 Train acc: 0.893333\n",
      "Epoch: 92/500 Iteration: 835 Train loss: 0.287186 Train acc: 0.916667\n",
      "Epoch: 93/500 Iteration: 840 Train loss: 0.316484 Train acc: 0.900000\n",
      "Epoch: 93/500 Iteration: 845 Train loss: 0.345150 Train acc: 0.890000\n",
      "Epoch: 94/500 Iteration: 850 Train loss: 0.268398 Train acc: 0.921667\n",
      "Epoch: 94/500 Iteration: 850 Validation loss: 0.350717 Validation acc: 0.879167\n",
      "Epoch: 94/500 Iteration: 855 Train loss: 0.286351 Train acc: 0.915000\n",
      "Epoch: 95/500 Iteration: 860 Train loss: 0.326691 Train acc: 0.896667\n",
      "Epoch: 96/500 Iteration: 865 Train loss: 0.267791 Train acc: 0.940000\n",
      "Epoch: 96/500 Iteration: 870 Train loss: 0.266545 Train acc: 0.903333\n",
      "Epoch: 97/500 Iteration: 875 Train loss: 0.327793 Train acc: 0.910000\n",
      "Epoch: 97/500 Iteration: 875 Validation loss: 0.355049 Validation acc: 0.871667\n",
      "Epoch: 97/500 Iteration: 880 Train loss: 0.274528 Train acc: 0.916667\n",
      "Epoch: 98/500 Iteration: 885 Train loss: 0.296662 Train acc: 0.915000\n",
      "Epoch: 98/500 Iteration: 890 Train loss: 0.333719 Train acc: 0.890000\n",
      "Epoch: 99/500 Iteration: 895 Train loss: 0.269742 Train acc: 0.928333\n",
      "Epoch: 99/500 Iteration: 900 Train loss: 0.305077 Train acc: 0.923333\n",
      "Epoch: 99/500 Iteration: 900 Validation loss: 0.362919 Validation acc: 0.870833\n",
      "Epoch: 100/500 Iteration: 905 Train loss: 0.318656 Train acc: 0.900000\n",
      "Epoch: 101/500 Iteration: 910 Train loss: 0.242703 Train acc: 0.936667\n",
      "Epoch: 101/500 Iteration: 915 Train loss: 0.260706 Train acc: 0.913333\n",
      "Epoch: 102/500 Iteration: 920 Train loss: 0.305934 Train acc: 0.895000\n",
      "Epoch: 102/500 Iteration: 925 Train loss: 0.261295 Train acc: 0.923333\n",
      "Epoch: 102/500 Iteration: 925 Validation loss: 0.359864 Validation acc: 0.875000\n",
      "Epoch: 103/500 Iteration: 930 Train loss: 0.307004 Train acc: 0.911667\n",
      "Epoch: 103/500 Iteration: 935 Train loss: 0.324836 Train acc: 0.898333\n",
      "Epoch: 104/500 Iteration: 940 Train loss: 0.251594 Train acc: 0.931667\n",
      "Epoch: 104/500 Iteration: 945 Train loss: 0.266585 Train acc: 0.928333\n",
      "Epoch: 105/500 Iteration: 950 Train loss: 0.290344 Train acc: 0.913333\n",
      "Epoch: 105/500 Iteration: 950 Validation loss: 0.330667 Validation acc: 0.875000\n",
      "Epoch: 106/500 Iteration: 955 Train loss: 0.245872 Train acc: 0.938333\n",
      "Epoch: 106/500 Iteration: 960 Train loss: 0.271894 Train acc: 0.920000\n",
      "Epoch: 107/500 Iteration: 965 Train loss: 0.319843 Train acc: 0.893333\n",
      "Epoch: 107/500 Iteration: 970 Train loss: 0.274709 Train acc: 0.911667\n",
      "Epoch: 108/500 Iteration: 975 Train loss: 0.276050 Train acc: 0.925000\n",
      "Epoch: 108/500 Iteration: 975 Validation loss: 0.333959 Validation acc: 0.886667\n",
      "Epoch: 108/500 Iteration: 980 Train loss: 0.329552 Train acc: 0.893333\n",
      "Epoch: 109/500 Iteration: 985 Train loss: 0.218937 Train acc: 0.936667\n",
      "Epoch: 109/500 Iteration: 990 Train loss: 0.292536 Train acc: 0.913333\n",
      "Epoch: 110/500 Iteration: 995 Train loss: 0.296321 Train acc: 0.905000\n",
      "Epoch: 111/500 Iteration: 1000 Train loss: 0.255799 Train acc: 0.928333\n",
      "Epoch: 111/500 Iteration: 1000 Validation loss: 0.346170 Validation acc: 0.876667\n",
      "Epoch: 111/500 Iteration: 1005 Train loss: 0.258472 Train acc: 0.913333\n",
      "Epoch: 112/500 Iteration: 1010 Train loss: 0.278106 Train acc: 0.906667\n",
      "Epoch: 112/500 Iteration: 1015 Train loss: 0.252105 Train acc: 0.918333\n",
      "Epoch: 113/500 Iteration: 1020 Train loss: 0.261620 Train acc: 0.915000\n",
      "Epoch: 113/500 Iteration: 1025 Train loss: 0.299172 Train acc: 0.911667\n",
      "Epoch: 113/500 Iteration: 1025 Validation loss: 0.343548 Validation acc: 0.883333\n",
      "Epoch: 114/500 Iteration: 1030 Train loss: 0.214273 Train acc: 0.946667\n",
      "Epoch: 114/500 Iteration: 1035 Train loss: 0.263882 Train acc: 0.911667\n",
      "Epoch: 115/500 Iteration: 1040 Train loss: 0.270600 Train acc: 0.923333\n",
      "Epoch: 116/500 Iteration: 1045 Train loss: 0.219233 Train acc: 0.946667\n",
      "Epoch: 116/500 Iteration: 1050 Train loss: 0.271087 Train acc: 0.913333\n",
      "Epoch: 116/500 Iteration: 1050 Validation loss: 0.335232 Validation acc: 0.881667\n",
      "Epoch: 117/500 Iteration: 1055 Train loss: 0.309447 Train acc: 0.916667\n",
      "Epoch: 117/500 Iteration: 1060 Train loss: 0.253355 Train acc: 0.915000\n",
      "Epoch: 118/500 Iteration: 1065 Train loss: 0.276399 Train acc: 0.916667\n",
      "Epoch: 118/500 Iteration: 1070 Train loss: 0.298109 Train acc: 0.908333\n",
      "Epoch: 119/500 Iteration: 1075 Train loss: 0.222090 Train acc: 0.940000\n",
      "Epoch: 119/500 Iteration: 1075 Validation loss: 0.349999 Validation acc: 0.880000\n",
      "Epoch: 119/500 Iteration: 1080 Train loss: 0.242653 Train acc: 0.931667\n",
      "Epoch: 120/500 Iteration: 1085 Train loss: 0.279397 Train acc: 0.916667\n",
      "Epoch: 121/500 Iteration: 1090 Train loss: 0.259543 Train acc: 0.925000\n",
      "Epoch: 121/500 Iteration: 1095 Train loss: 0.276358 Train acc: 0.908333\n",
      "Epoch: 122/500 Iteration: 1100 Train loss: 0.298673 Train acc: 0.908333\n",
      "Epoch: 122/500 Iteration: 1100 Validation loss: 0.357742 Validation acc: 0.878333\n",
      "Epoch: 122/500 Iteration: 1105 Train loss: 0.244992 Train acc: 0.923333\n",
      "Epoch: 123/500 Iteration: 1110 Train loss: 0.276404 Train acc: 0.918333\n",
      "Epoch: 123/500 Iteration: 1115 Train loss: 0.299705 Train acc: 0.908333\n",
      "Epoch: 124/500 Iteration: 1120 Train loss: 0.207057 Train acc: 0.941667\n",
      "Epoch: 124/500 Iteration: 1125 Train loss: 0.242626 Train acc: 0.931667\n",
      "Epoch: 124/500 Iteration: 1125 Validation loss: 0.330053 Validation acc: 0.887500\n",
      "Epoch: 125/500 Iteration: 1130 Train loss: 0.269060 Train acc: 0.915000\n",
      "Epoch: 126/500 Iteration: 1135 Train loss: 0.206068 Train acc: 0.940000\n",
      "Epoch: 126/500 Iteration: 1140 Train loss: 0.249704 Train acc: 0.930000\n",
      "Epoch: 127/500 Iteration: 1145 Train loss: 0.272818 Train acc: 0.925000\n",
      "Epoch: 127/500 Iteration: 1150 Train loss: 0.230770 Train acc: 0.938333\n",
      "Epoch: 127/500 Iteration: 1150 Validation loss: 0.329015 Validation acc: 0.882500\n",
      "Epoch: 128/500 Iteration: 1155 Train loss: 0.248295 Train acc: 0.926667\n",
      "Epoch: 128/500 Iteration: 1160 Train loss: 0.281413 Train acc: 0.915000\n",
      "Epoch: 129/500 Iteration: 1165 Train loss: 0.195240 Train acc: 0.945000\n",
      "Epoch: 129/500 Iteration: 1170 Train loss: 0.220562 Train acc: 0.928333\n",
      "Epoch: 130/500 Iteration: 1175 Train loss: 0.248232 Train acc: 0.928333\n",
      "Epoch: 130/500 Iteration: 1175 Validation loss: 0.326139 Validation acc: 0.890000\n",
      "Epoch: 131/500 Iteration: 1180 Train loss: 0.197212 Train acc: 0.951667\n",
      "Epoch: 131/500 Iteration: 1185 Train loss: 0.231747 Train acc: 0.933333\n",
      "Epoch: 132/500 Iteration: 1190 Train loss: 0.263766 Train acc: 0.918333\n",
      "Epoch: 132/500 Iteration: 1195 Train loss: 0.258378 Train acc: 0.921667\n",
      "Epoch: 133/500 Iteration: 1200 Train loss: 0.257113 Train acc: 0.921667\n",
      "Epoch: 133/500 Iteration: 1200 Validation loss: 0.324554 Validation acc: 0.890833\n",
      "Epoch: 133/500 Iteration: 1205 Train loss: 0.326470 Train acc: 0.888333\n",
      "Epoch: 134/500 Iteration: 1210 Train loss: 0.261952 Train acc: 0.920000\n",
      "Epoch: 134/500 Iteration: 1215 Train loss: 0.224653 Train acc: 0.933333\n",
      "Epoch: 135/500 Iteration: 1220 Train loss: 0.244635 Train acc: 0.933333\n",
      "Epoch: 136/500 Iteration: 1225 Train loss: 0.238841 Train acc: 0.933333\n",
      "Epoch: 136/500 Iteration: 1225 Validation loss: 0.301242 Validation acc: 0.896667\n",
      "Epoch: 136/500 Iteration: 1230 Train loss: 0.257614 Train acc: 0.923333\n",
      "Epoch: 137/500 Iteration: 1235 Train loss: 0.255912 Train acc: 0.928333\n",
      "Epoch: 137/500 Iteration: 1240 Train loss: 0.270095 Train acc: 0.916667\n",
      "Epoch: 138/500 Iteration: 1245 Train loss: 0.235473 Train acc: 0.930000\n",
      "Epoch: 138/500 Iteration: 1250 Train loss: 0.290817 Train acc: 0.911667\n",
      "Epoch: 138/500 Iteration: 1250 Validation loss: 0.318257 Validation acc: 0.896667\n",
      "Epoch: 139/500 Iteration: 1255 Train loss: 0.199547 Train acc: 0.931667\n",
      "Epoch: 139/500 Iteration: 1260 Train loss: 0.214762 Train acc: 0.933333\n",
      "Epoch: 140/500 Iteration: 1265 Train loss: 0.249354 Train acc: 0.928333\n",
      "Epoch: 141/500 Iteration: 1270 Train loss: 0.216896 Train acc: 0.940000\n",
      "Epoch: 141/500 Iteration: 1275 Train loss: 0.222996 Train acc: 0.928333\n",
      "Epoch: 141/500 Iteration: 1275 Validation loss: 0.310977 Validation acc: 0.894167\n",
      "Epoch: 142/500 Iteration: 1280 Train loss: 0.260433 Train acc: 0.916667\n",
      "Epoch: 142/500 Iteration: 1285 Train loss: 0.227100 Train acc: 0.936667\n",
      "Epoch: 143/500 Iteration: 1290 Train loss: 0.233427 Train acc: 0.935000\n",
      "Epoch: 143/500 Iteration: 1295 Train loss: 0.292888 Train acc: 0.901667\n",
      "Epoch: 144/500 Iteration: 1300 Train loss: 0.213139 Train acc: 0.926667\n",
      "Epoch: 144/500 Iteration: 1300 Validation loss: 0.348743 Validation acc: 0.881667\n",
      "Epoch: 144/500 Iteration: 1305 Train loss: 0.217795 Train acc: 0.936667\n",
      "Epoch: 145/500 Iteration: 1310 Train loss: 0.240047 Train acc: 0.935000\n",
      "Epoch: 146/500 Iteration: 1315 Train loss: 0.190478 Train acc: 0.940000\n",
      "Epoch: 146/500 Iteration: 1320 Train loss: 0.217467 Train acc: 0.931667\n",
      "Epoch: 147/500 Iteration: 1325 Train loss: 0.235583 Train acc: 0.935000\n",
      "Epoch: 147/500 Iteration: 1325 Validation loss: 0.303341 Validation acc: 0.897500\n",
      "Epoch: 147/500 Iteration: 1330 Train loss: 0.215180 Train acc: 0.933333\n",
      "Epoch: 148/500 Iteration: 1335 Train loss: 0.243386 Train acc: 0.925000\n",
      "Epoch: 148/500 Iteration: 1340 Train loss: 0.258791 Train acc: 0.920000\n",
      "Epoch: 149/500 Iteration: 1345 Train loss: 0.177592 Train acc: 0.948333\n",
      "Epoch: 149/500 Iteration: 1350 Train loss: 0.230885 Train acc: 0.931667\n",
      "Epoch: 149/500 Iteration: 1350 Validation loss: 0.316316 Validation acc: 0.895000\n",
      "Epoch: 150/500 Iteration: 1355 Train loss: 0.214052 Train acc: 0.935000\n",
      "Epoch: 151/500 Iteration: 1360 Train loss: 0.200439 Train acc: 0.938333\n",
      "Epoch: 151/500 Iteration: 1365 Train loss: 0.233442 Train acc: 0.921667\n",
      "Epoch: 152/500 Iteration: 1370 Train loss: 0.272801 Train acc: 0.926667\n",
      "Epoch: 152/500 Iteration: 1375 Train loss: 0.221984 Train acc: 0.931667\n",
      "Epoch: 152/500 Iteration: 1375 Validation loss: 0.328140 Validation acc: 0.886667\n",
      "Epoch: 153/500 Iteration: 1380 Train loss: 0.241969 Train acc: 0.913333\n",
      "Epoch: 153/500 Iteration: 1385 Train loss: 0.269313 Train acc: 0.920000\n",
      "Epoch: 154/500 Iteration: 1390 Train loss: 0.191494 Train acc: 0.938333\n",
      "Epoch: 154/500 Iteration: 1395 Train loss: 0.198369 Train acc: 0.946667\n",
      "Epoch: 155/500 Iteration: 1400 Train loss: 0.199148 Train acc: 0.940000\n",
      "Epoch: 155/500 Iteration: 1400 Validation loss: 0.303878 Validation acc: 0.896667\n",
      "Epoch: 156/500 Iteration: 1405 Train loss: 0.165388 Train acc: 0.953333\n",
      "Epoch: 156/500 Iteration: 1410 Train loss: 0.188840 Train acc: 0.936667\n",
      "Epoch: 157/500 Iteration: 1415 Train loss: 0.228327 Train acc: 0.931667\n",
      "Epoch: 157/500 Iteration: 1420 Train loss: 0.184621 Train acc: 0.945000\n",
      "Epoch: 158/500 Iteration: 1425 Train loss: 0.209907 Train acc: 0.941667\n",
      "Epoch: 158/500 Iteration: 1425 Validation loss: 0.305562 Validation acc: 0.900833\n",
      "Epoch: 158/500 Iteration: 1430 Train loss: 0.288341 Train acc: 0.901667\n",
      "Epoch: 159/500 Iteration: 1435 Train loss: 0.182805 Train acc: 0.948333\n",
      "Epoch: 159/500 Iteration: 1440 Train loss: 0.209413 Train acc: 0.936667\n",
      "Epoch: 160/500 Iteration: 1445 Train loss: 0.237749 Train acc: 0.930000\n",
      "Epoch: 161/500 Iteration: 1450 Train loss: 0.177182 Train acc: 0.943333\n",
      "Epoch: 161/500 Iteration: 1450 Validation loss: 0.326405 Validation acc: 0.891667\n",
      "Epoch: 161/500 Iteration: 1455 Train loss: 0.201662 Train acc: 0.926667\n",
      "Epoch: 162/500 Iteration: 1460 Train loss: 0.229458 Train acc: 0.926667\n",
      "Epoch: 162/500 Iteration: 1465 Train loss: 0.190797 Train acc: 0.941667\n",
      "Epoch: 163/500 Iteration: 1470 Train loss: 0.209553 Train acc: 0.930000\n",
      "Epoch: 163/500 Iteration: 1475 Train loss: 0.268451 Train acc: 0.910000\n",
      "Epoch: 163/500 Iteration: 1475 Validation loss: 0.298532 Validation acc: 0.904167\n",
      "Epoch: 164/500 Iteration: 1480 Train loss: 0.200566 Train acc: 0.938333\n",
      "Epoch: 164/500 Iteration: 1485 Train loss: 0.249066 Train acc: 0.918333\n",
      "Epoch: 165/500 Iteration: 1490 Train loss: 0.275285 Train acc: 0.925000\n",
      "Epoch: 166/500 Iteration: 1495 Train loss: 0.198304 Train acc: 0.941667\n",
      "Epoch: 166/500 Iteration: 1500 Train loss: 0.200544 Train acc: 0.935000\n",
      "Epoch: 166/500 Iteration: 1500 Validation loss: 0.298674 Validation acc: 0.900000\n",
      "Epoch: 167/500 Iteration: 1505 Train loss: 0.277838 Train acc: 0.921667\n",
      "Epoch: 167/500 Iteration: 1510 Train loss: 0.190984 Train acc: 0.946667\n",
      "Epoch: 168/500 Iteration: 1515 Train loss: 0.215559 Train acc: 0.931667\n",
      "Epoch: 168/500 Iteration: 1520 Train loss: 0.289428 Train acc: 0.910000\n",
      "Epoch: 169/500 Iteration: 1525 Train loss: 0.191298 Train acc: 0.936667\n",
      "Epoch: 169/500 Iteration: 1525 Validation loss: 0.298682 Validation acc: 0.892500\n",
      "Epoch: 169/500 Iteration: 1530 Train loss: 0.193013 Train acc: 0.938333\n",
      "Epoch: 170/500 Iteration: 1535 Train loss: 0.227586 Train acc: 0.945000\n",
      "Epoch: 171/500 Iteration: 1540 Train loss: 0.182815 Train acc: 0.948333\n",
      "Epoch: 171/500 Iteration: 1545 Train loss: 0.215089 Train acc: 0.930000\n",
      "Epoch: 172/500 Iteration: 1550 Train loss: 0.230869 Train acc: 0.940000\n",
      "Epoch: 172/500 Iteration: 1550 Validation loss: 0.288353 Validation acc: 0.907500\n",
      "Epoch: 172/500 Iteration: 1555 Train loss: 0.189301 Train acc: 0.945000\n",
      "Epoch: 173/500 Iteration: 1560 Train loss: 0.227576 Train acc: 0.928333\n",
      "Epoch: 173/500 Iteration: 1565 Train loss: 0.254684 Train acc: 0.926667\n",
      "Epoch: 174/500 Iteration: 1570 Train loss: 0.154002 Train acc: 0.956667\n",
      "Epoch: 174/500 Iteration: 1575 Train loss: 0.191843 Train acc: 0.951667\n",
      "Epoch: 174/500 Iteration: 1575 Validation loss: 0.289426 Validation acc: 0.899167\n",
      "Epoch: 175/500 Iteration: 1580 Train loss: 0.214756 Train acc: 0.935000\n",
      "Epoch: 176/500 Iteration: 1585 Train loss: 0.162502 Train acc: 0.956667\n",
      "Epoch: 176/500 Iteration: 1590 Train loss: 0.194933 Train acc: 0.936667\n",
      "Epoch: 177/500 Iteration: 1595 Train loss: 0.214900 Train acc: 0.938333\n",
      "Epoch: 177/500 Iteration: 1600 Train loss: 0.194027 Train acc: 0.948333\n",
      "Epoch: 177/500 Iteration: 1600 Validation loss: 0.282995 Validation acc: 0.905833\n",
      "Epoch: 178/500 Iteration: 1605 Train loss: 0.195997 Train acc: 0.933333\n",
      "Epoch: 178/500 Iteration: 1610 Train loss: 0.231336 Train acc: 0.925000\n",
      "Epoch: 179/500 Iteration: 1615 Train loss: 0.165795 Train acc: 0.950000\n",
      "Epoch: 179/500 Iteration: 1620 Train loss: 0.159684 Train acc: 0.950000\n",
      "Epoch: 180/500 Iteration: 1625 Train loss: 0.203778 Train acc: 0.935000\n",
      "Epoch: 180/500 Iteration: 1625 Validation loss: 0.301368 Validation acc: 0.905833\n",
      "Epoch: 181/500 Iteration: 1630 Train loss: 0.156209 Train acc: 0.960000\n",
      "Epoch: 181/500 Iteration: 1635 Train loss: 0.169203 Train acc: 0.946667\n",
      "Epoch: 182/500 Iteration: 1640 Train loss: 0.213347 Train acc: 0.938333\n",
      "Epoch: 182/500 Iteration: 1645 Train loss: 0.180249 Train acc: 0.945000\n",
      "Epoch: 183/500 Iteration: 1650 Train loss: 0.200844 Train acc: 0.940000\n",
      "Epoch: 183/500 Iteration: 1650 Validation loss: 0.318701 Validation acc: 0.898333\n",
      "Epoch: 183/500 Iteration: 1655 Train loss: 0.224450 Train acc: 0.925000\n",
      "Epoch: 184/500 Iteration: 1660 Train loss: 0.141184 Train acc: 0.956667\n",
      "Epoch: 184/500 Iteration: 1665 Train loss: 0.210927 Train acc: 0.945000\n",
      "Epoch: 185/500 Iteration: 1670 Train loss: 0.190385 Train acc: 0.941667\n",
      "Epoch: 186/500 Iteration: 1675 Train loss: 0.142158 Train acc: 0.963333\n",
      "Epoch: 186/500 Iteration: 1675 Validation loss: 0.291756 Validation acc: 0.904167\n",
      "Epoch: 186/500 Iteration: 1680 Train loss: 0.147042 Train acc: 0.961667\n",
      "Epoch: 187/500 Iteration: 1685 Train loss: 0.229912 Train acc: 0.936667\n",
      "Epoch: 187/500 Iteration: 1690 Train loss: 0.180035 Train acc: 0.941667\n",
      "Epoch: 188/500 Iteration: 1695 Train loss: 0.204017 Train acc: 0.936667\n",
      "Epoch: 188/500 Iteration: 1700 Train loss: 0.221384 Train acc: 0.928333\n",
      "Epoch: 188/500 Iteration: 1700 Validation loss: 0.316983 Validation acc: 0.900833\n",
      "Epoch: 189/500 Iteration: 1705 Train loss: 0.160898 Train acc: 0.961667\n",
      "Epoch: 189/500 Iteration: 1710 Train loss: 0.201921 Train acc: 0.941667\n",
      "Epoch: 190/500 Iteration: 1715 Train loss: 0.203428 Train acc: 0.941667\n",
      "Epoch: 191/500 Iteration: 1720 Train loss: 0.151708 Train acc: 0.963333\n",
      "Epoch: 191/500 Iteration: 1725 Train loss: 0.166807 Train acc: 0.946667\n",
      "Epoch: 191/500 Iteration: 1725 Validation loss: 0.305378 Validation acc: 0.907500\n",
      "Epoch: 192/500 Iteration: 1730 Train loss: 0.198938 Train acc: 0.950000\n",
      "Epoch: 192/500 Iteration: 1735 Train loss: 0.173330 Train acc: 0.945000\n",
      "Epoch: 193/500 Iteration: 1740 Train loss: 0.185516 Train acc: 0.943333\n",
      "Epoch: 193/500 Iteration: 1745 Train loss: 0.226616 Train acc: 0.916667\n",
      "Epoch: 194/500 Iteration: 1750 Train loss: 0.158508 Train acc: 0.946667\n",
      "Epoch: 194/500 Iteration: 1750 Validation loss: 0.295714 Validation acc: 0.906667\n",
      "Epoch: 194/500 Iteration: 1755 Train loss: 0.167605 Train acc: 0.951667\n",
      "Epoch: 195/500 Iteration: 1760 Train loss: 0.201167 Train acc: 0.938333\n",
      "Epoch: 196/500 Iteration: 1765 Train loss: 0.182876 Train acc: 0.940000\n",
      "Epoch: 196/500 Iteration: 1770 Train loss: 0.152333 Train acc: 0.958333\n",
      "Epoch: 197/500 Iteration: 1775 Train loss: 0.214319 Train acc: 0.951667\n",
      "Epoch: 197/500 Iteration: 1775 Validation loss: 0.301981 Validation acc: 0.905833\n",
      "Epoch: 197/500 Iteration: 1780 Train loss: 0.152409 Train acc: 0.958333\n",
      "Epoch: 198/500 Iteration: 1785 Train loss: 0.201072 Train acc: 0.945000\n",
      "Epoch: 198/500 Iteration: 1790 Train loss: 0.225346 Train acc: 0.923333\n",
      "Epoch: 199/500 Iteration: 1795 Train loss: 0.141641 Train acc: 0.973333\n",
      "Epoch: 199/500 Iteration: 1800 Train loss: 0.152081 Train acc: 0.953333\n",
      "Epoch: 199/500 Iteration: 1800 Validation loss: 0.306171 Validation acc: 0.908333\n",
      "Epoch: 200/500 Iteration: 1805 Train loss: 0.187700 Train acc: 0.950000\n",
      "Epoch: 201/500 Iteration: 1810 Train loss: 0.147100 Train acc: 0.956667\n",
      "Epoch: 201/500 Iteration: 1815 Train loss: 0.153409 Train acc: 0.956667\n",
      "Epoch: 202/500 Iteration: 1820 Train loss: 0.217952 Train acc: 0.938333\n",
      "Epoch: 202/500 Iteration: 1825 Train loss: 0.166212 Train acc: 0.951667\n",
      "Epoch: 202/500 Iteration: 1825 Validation loss: 0.307868 Validation acc: 0.907500\n",
      "Epoch: 203/500 Iteration: 1830 Train loss: 0.190651 Train acc: 0.946667\n",
      "Epoch: 203/500 Iteration: 1835 Train loss: 0.211540 Train acc: 0.936667\n",
      "Epoch: 204/500 Iteration: 1840 Train loss: 0.159144 Train acc: 0.945000\n",
      "Epoch: 204/500 Iteration: 1845 Train loss: 0.145863 Train acc: 0.956667\n",
      "Epoch: 205/500 Iteration: 1850 Train loss: 0.193338 Train acc: 0.943333\n",
      "Epoch: 205/500 Iteration: 1850 Validation loss: 0.318247 Validation acc: 0.906667\n",
      "Epoch: 206/500 Iteration: 1855 Train loss: 0.145419 Train acc: 0.965000\n",
      "Epoch: 206/500 Iteration: 1860 Train loss: 0.154992 Train acc: 0.951667\n",
      "Epoch: 207/500 Iteration: 1865 Train loss: 0.190940 Train acc: 0.948333\n",
      "Epoch: 207/500 Iteration: 1870 Train loss: 0.167681 Train acc: 0.953333\n",
      "Epoch: 208/500 Iteration: 1875 Train loss: 0.183476 Train acc: 0.945000\n",
      "Epoch: 208/500 Iteration: 1875 Validation loss: 0.312327 Validation acc: 0.910833\n",
      "Epoch: 208/500 Iteration: 1880 Train loss: 0.217241 Train acc: 0.931667\n",
      "Epoch: 209/500 Iteration: 1885 Train loss: 0.126163 Train acc: 0.961667\n",
      "Epoch: 209/500 Iteration: 1890 Train loss: 0.167313 Train acc: 0.956667\n",
      "Epoch: 210/500 Iteration: 1895 Train loss: 0.165396 Train acc: 0.950000\n",
      "Epoch: 211/500 Iteration: 1900 Train loss: 0.124635 Train acc: 0.970000\n",
      "Epoch: 211/500 Iteration: 1900 Validation loss: 0.321327 Validation acc: 0.908333\n",
      "Epoch: 211/500 Iteration: 1905 Train loss: 0.133160 Train acc: 0.963333\n",
      "Epoch: 212/500 Iteration: 1910 Train loss: 0.187348 Train acc: 0.961667\n",
      "Epoch: 212/500 Iteration: 1915 Train loss: 0.159468 Train acc: 0.951667\n",
      "Epoch: 213/500 Iteration: 1920 Train loss: 0.176186 Train acc: 0.953333\n",
      "Epoch: 213/500 Iteration: 1925 Train loss: 0.222572 Train acc: 0.928333\n",
      "Epoch: 213/500 Iteration: 1925 Validation loss: 0.337934 Validation acc: 0.900833\n",
      "Epoch: 214/500 Iteration: 1930 Train loss: 0.153057 Train acc: 0.945000\n",
      "Epoch: 214/500 Iteration: 1935 Train loss: 0.160311 Train acc: 0.955000\n",
      "Epoch: 215/500 Iteration: 1940 Train loss: 0.155077 Train acc: 0.953333\n",
      "Epoch: 216/500 Iteration: 1945 Train loss: 0.137921 Train acc: 0.961667\n",
      "Epoch: 216/500 Iteration: 1950 Train loss: 0.151001 Train acc: 0.955000\n",
      "Epoch: 216/500 Iteration: 1950 Validation loss: 0.336174 Validation acc: 0.905833\n",
      "Epoch: 217/500 Iteration: 1955 Train loss: 0.244337 Train acc: 0.930000\n",
      "Epoch: 217/500 Iteration: 1960 Train loss: 0.154180 Train acc: 0.953333\n",
      "Epoch: 218/500 Iteration: 1965 Train loss: 0.174302 Train acc: 0.950000\n",
      "Epoch: 218/500 Iteration: 1970 Train loss: 0.201365 Train acc: 0.926667\n",
      "Epoch: 219/500 Iteration: 1975 Train loss: 0.135265 Train acc: 0.971667\n",
      "Epoch: 219/500 Iteration: 1975 Validation loss: 0.341921 Validation acc: 0.896667\n",
      "Epoch: 219/500 Iteration: 1980 Train loss: 0.149037 Train acc: 0.956667\n",
      "Epoch: 220/500 Iteration: 1985 Train loss: 0.231846 Train acc: 0.935000\n",
      "Epoch: 221/500 Iteration: 1990 Train loss: 0.222732 Train acc: 0.950000\n",
      "Epoch: 221/500 Iteration: 1995 Train loss: 0.189919 Train acc: 0.945000\n",
      "Epoch: 222/500 Iteration: 2000 Train loss: 0.228366 Train acc: 0.931667\n",
      "Epoch: 222/500 Iteration: 2000 Validation loss: 0.326167 Validation acc: 0.901667\n",
      "Epoch: 222/500 Iteration: 2005 Train loss: 0.159495 Train acc: 0.950000\n",
      "Epoch: 223/500 Iteration: 2010 Train loss: 0.173398 Train acc: 0.950000\n",
      "Epoch: 223/500 Iteration: 2015 Train loss: 0.204987 Train acc: 0.945000\n",
      "Epoch: 224/500 Iteration: 2020 Train loss: 0.132195 Train acc: 0.961667\n",
      "Epoch: 224/500 Iteration: 2025 Train loss: 0.161122 Train acc: 0.951667\n",
      "Epoch: 224/500 Iteration: 2025 Validation loss: 0.321349 Validation acc: 0.910000\n",
      "Epoch: 225/500 Iteration: 2030 Train loss: 0.198156 Train acc: 0.948333\n",
      "Epoch: 226/500 Iteration: 2035 Train loss: 0.155681 Train acc: 0.958333\n",
      "Epoch: 226/500 Iteration: 2040 Train loss: 0.123335 Train acc: 0.965000\n",
      "Epoch: 227/500 Iteration: 2045 Train loss: 0.191783 Train acc: 0.943333\n",
      "Epoch: 227/500 Iteration: 2050 Train loss: 0.133340 Train acc: 0.961667\n",
      "Epoch: 227/500 Iteration: 2050 Validation loss: 0.311515 Validation acc: 0.909167\n",
      "Epoch: 228/500 Iteration: 2055 Train loss: 0.171311 Train acc: 0.956667\n",
      "Epoch: 228/500 Iteration: 2060 Train loss: 0.183474 Train acc: 0.943333\n",
      "Epoch: 229/500 Iteration: 2065 Train loss: 0.120431 Train acc: 0.963333\n",
      "Epoch: 229/500 Iteration: 2070 Train loss: 0.142867 Train acc: 0.955000\n",
      "Epoch: 230/500 Iteration: 2075 Train loss: 0.176524 Train acc: 0.946667\n",
      "Epoch: 230/500 Iteration: 2075 Validation loss: 0.329770 Validation acc: 0.900000\n",
      "Epoch: 231/500 Iteration: 2080 Train loss: 0.140183 Train acc: 0.970000\n",
      "Epoch: 231/500 Iteration: 2085 Train loss: 0.153524 Train acc: 0.960000\n",
      "Epoch: 232/500 Iteration: 2090 Train loss: 0.191872 Train acc: 0.951667\n",
      "Epoch: 232/500 Iteration: 2095 Train loss: 0.143903 Train acc: 0.960000\n",
      "Epoch: 233/500 Iteration: 2100 Train loss: 0.157871 Train acc: 0.953333\n",
      "Epoch: 233/500 Iteration: 2100 Validation loss: 0.334011 Validation acc: 0.897500\n",
      "Epoch: 233/500 Iteration: 2105 Train loss: 0.208305 Train acc: 0.931667\n",
      "Epoch: 234/500 Iteration: 2110 Train loss: 0.165510 Train acc: 0.956667\n",
      "Epoch: 234/500 Iteration: 2115 Train loss: 0.162308 Train acc: 0.958333\n",
      "Epoch: 235/500 Iteration: 2120 Train loss: 0.193213 Train acc: 0.946667\n",
      "Epoch: 236/500 Iteration: 2125 Train loss: 0.143859 Train acc: 0.965000\n",
      "Epoch: 236/500 Iteration: 2125 Validation loss: 0.307642 Validation acc: 0.914167\n",
      "Epoch: 236/500 Iteration: 2130 Train loss: 0.127810 Train acc: 0.960000\n",
      "Epoch: 237/500 Iteration: 2135 Train loss: 0.168478 Train acc: 0.956667\n",
      "Epoch: 237/500 Iteration: 2140 Train loss: 0.149203 Train acc: 0.953333\n",
      "Epoch: 238/500 Iteration: 2145 Train loss: 0.168689 Train acc: 0.943333\n",
      "Epoch: 238/500 Iteration: 2150 Train loss: 0.195217 Train acc: 0.935000\n",
      "Epoch: 238/500 Iteration: 2150 Validation loss: 0.313397 Validation acc: 0.904167\n",
      "Epoch: 239/500 Iteration: 2155 Train loss: 0.109719 Train acc: 0.965000\n",
      "Epoch: 239/500 Iteration: 2160 Train loss: 0.162056 Train acc: 0.960000\n",
      "Epoch: 240/500 Iteration: 2165 Train loss: 0.165705 Train acc: 0.951667\n",
      "Epoch: 241/500 Iteration: 2170 Train loss: 0.114219 Train acc: 0.968333\n",
      "Epoch: 241/500 Iteration: 2175 Train loss: 0.120224 Train acc: 0.963333\n",
      "Epoch: 241/500 Iteration: 2175 Validation loss: 0.320305 Validation acc: 0.908333\n",
      "Epoch: 242/500 Iteration: 2180 Train loss: 0.166711 Train acc: 0.955000\n",
      "Epoch: 242/500 Iteration: 2185 Train loss: 0.134793 Train acc: 0.958333\n",
      "Epoch: 243/500 Iteration: 2190 Train loss: 0.152503 Train acc: 0.950000\n",
      "Epoch: 243/500 Iteration: 2195 Train loss: 0.187031 Train acc: 0.946667\n",
      "Epoch: 244/500 Iteration: 2200 Train loss: 0.119020 Train acc: 0.970000\n",
      "Epoch: 244/500 Iteration: 2200 Validation loss: 0.326981 Validation acc: 0.903333\n",
      "Epoch: 244/500 Iteration: 2205 Train loss: 0.149004 Train acc: 0.963333\n",
      "Epoch: 245/500 Iteration: 2210 Train loss: 0.151097 Train acc: 0.956667\n",
      "Epoch: 246/500 Iteration: 2215 Train loss: 0.121445 Train acc: 0.975000\n",
      "Epoch: 246/500 Iteration: 2220 Train loss: 0.132442 Train acc: 0.975000\n",
      "Epoch: 247/500 Iteration: 2225 Train loss: 0.187344 Train acc: 0.951667\n",
      "Epoch: 247/500 Iteration: 2225 Validation loss: 0.359473 Validation acc: 0.904167\n",
      "Epoch: 247/500 Iteration: 2230 Train loss: 0.153663 Train acc: 0.958333\n",
      "Epoch: 248/500 Iteration: 2235 Train loss: 0.144222 Train acc: 0.961667\n",
      "Epoch: 248/500 Iteration: 2240 Train loss: 0.195221 Train acc: 0.948333\n",
      "Epoch: 249/500 Iteration: 2245 Train loss: 0.184425 Train acc: 0.941667\n",
      "Epoch: 249/500 Iteration: 2250 Train loss: 0.188267 Train acc: 0.943333\n",
      "Epoch: 249/500 Iteration: 2250 Validation loss: 0.326079 Validation acc: 0.905833\n",
      "Epoch: 250/500 Iteration: 2255 Train loss: 0.196286 Train acc: 0.953333\n",
      "Epoch: 251/500 Iteration: 2260 Train loss: 0.146949 Train acc: 0.961667\n",
      "Epoch: 251/500 Iteration: 2265 Train loss: 0.135016 Train acc: 0.961667\n",
      "Epoch: 252/500 Iteration: 2270 Train loss: 0.205807 Train acc: 0.945000\n",
      "Epoch: 252/500 Iteration: 2275 Train loss: 0.170703 Train acc: 0.950000\n",
      "Epoch: 252/500 Iteration: 2275 Validation loss: 0.321903 Validation acc: 0.905000\n",
      "Epoch: 253/500 Iteration: 2280 Train loss: 0.164686 Train acc: 0.946667\n",
      "Epoch: 253/500 Iteration: 2285 Train loss: 0.204888 Train acc: 0.936667\n",
      "Epoch: 254/500 Iteration: 2290 Train loss: 0.143789 Train acc: 0.960000\n",
      "Epoch: 254/500 Iteration: 2295 Train loss: 0.174939 Train acc: 0.945000\n",
      "Epoch: 255/500 Iteration: 2300 Train loss: 0.177892 Train acc: 0.955000\n",
      "Epoch: 255/500 Iteration: 2300 Validation loss: 0.341231 Validation acc: 0.898333\n",
      "Epoch: 256/500 Iteration: 2305 Train loss: 0.130047 Train acc: 0.973333\n",
      "Epoch: 256/500 Iteration: 2310 Train loss: 0.132812 Train acc: 0.958333\n",
      "Epoch: 257/500 Iteration: 2315 Train loss: 0.209623 Train acc: 0.938333\n",
      "Epoch: 257/500 Iteration: 2320 Train loss: 0.134440 Train acc: 0.961667\n",
      "Epoch: 258/500 Iteration: 2325 Train loss: 0.138564 Train acc: 0.963333\n",
      "Epoch: 258/500 Iteration: 2325 Validation loss: 0.331892 Validation acc: 0.907500\n",
      "Epoch: 258/500 Iteration: 2330 Train loss: 0.173145 Train acc: 0.950000\n",
      "Epoch: 259/500 Iteration: 2335 Train loss: 0.091626 Train acc: 0.973333\n",
      "Epoch: 259/500 Iteration: 2340 Train loss: 0.146497 Train acc: 0.958333\n",
      "Epoch: 260/500 Iteration: 2345 Train loss: 0.153354 Train acc: 0.951667\n",
      "Epoch: 261/500 Iteration: 2350 Train loss: 0.116347 Train acc: 0.970000\n",
      "Epoch: 261/500 Iteration: 2350 Validation loss: 0.328435 Validation acc: 0.907500\n",
      "Epoch: 261/500 Iteration: 2355 Train loss: 0.140364 Train acc: 0.960000\n",
      "Epoch: 262/500 Iteration: 2360 Train loss: 0.209096 Train acc: 0.938333\n",
      "Epoch: 262/500 Iteration: 2365 Train loss: 0.118397 Train acc: 0.968333\n",
      "Epoch: 263/500 Iteration: 2370 Train loss: 0.141185 Train acc: 0.953333\n",
      "Epoch: 263/500 Iteration: 2375 Train loss: 0.193930 Train acc: 0.940000\n",
      "Epoch: 263/500 Iteration: 2375 Validation loss: 0.314570 Validation acc: 0.914167\n",
      "Epoch: 264/500 Iteration: 2380 Train loss: 0.105144 Train acc: 0.971667\n",
      "Epoch: 264/500 Iteration: 2385 Train loss: 0.146665 Train acc: 0.966667\n",
      "Epoch: 265/500 Iteration: 2390 Train loss: 0.140236 Train acc: 0.958333\n",
      "Epoch: 266/500 Iteration: 2395 Train loss: 0.104356 Train acc: 0.976667\n",
      "Epoch: 266/500 Iteration: 2400 Train loss: 0.124262 Train acc: 0.963333\n",
      "Epoch: 266/500 Iteration: 2400 Validation loss: 0.333495 Validation acc: 0.905833\n",
      "Epoch: 267/500 Iteration: 2405 Train loss: 0.166983 Train acc: 0.956667\n",
      "Epoch: 267/500 Iteration: 2410 Train loss: 0.115158 Train acc: 0.968333\n",
      "Epoch: 268/500 Iteration: 2415 Train loss: 0.147902 Train acc: 0.951667\n",
      "Epoch: 268/500 Iteration: 2420 Train loss: 0.210568 Train acc: 0.928333\n",
      "Epoch: 269/500 Iteration: 2425 Train loss: 0.123078 Train acc: 0.966667\n",
      "Epoch: 269/500 Iteration: 2425 Validation loss: 0.340445 Validation acc: 0.904167\n",
      "Epoch: 269/500 Iteration: 2430 Train loss: 0.138926 Train acc: 0.955000\n",
      "Epoch: 270/500 Iteration: 2435 Train loss: 0.147200 Train acc: 0.960000\n",
      "Epoch: 271/500 Iteration: 2440 Train loss: 0.110213 Train acc: 0.976667\n",
      "Epoch: 271/500 Iteration: 2445 Train loss: 0.113315 Train acc: 0.973333\n",
      "Epoch: 272/500 Iteration: 2450 Train loss: 0.209209 Train acc: 0.930000\n",
      "Epoch: 272/500 Iteration: 2450 Validation loss: 0.310153 Validation acc: 0.910833\n",
      "Epoch: 272/500 Iteration: 2455 Train loss: 0.118559 Train acc: 0.965000\n",
      "Epoch: 273/500 Iteration: 2460 Train loss: 0.126380 Train acc: 0.958333\n",
      "Epoch: 273/500 Iteration: 2465 Train loss: 0.162597 Train acc: 0.953333\n",
      "Epoch: 274/500 Iteration: 2470 Train loss: 0.111714 Train acc: 0.971667\n",
      "Epoch: 274/500 Iteration: 2475 Train loss: 0.138056 Train acc: 0.965000\n",
      "Epoch: 274/500 Iteration: 2475 Validation loss: 0.326077 Validation acc: 0.907500\n",
      "Epoch: 275/500 Iteration: 2480 Train loss: 0.146905 Train acc: 0.950000\n",
      "Epoch: 276/500 Iteration: 2485 Train loss: 0.160195 Train acc: 0.961667\n",
      "Epoch: 276/500 Iteration: 2490 Train loss: 0.153308 Train acc: 0.963333\n",
      "Epoch: 277/500 Iteration: 2495 Train loss: 0.183119 Train acc: 0.948333\n",
      "Epoch: 277/500 Iteration: 2500 Train loss: 0.149438 Train acc: 0.961667\n",
      "Epoch: 277/500 Iteration: 2500 Validation loss: 0.315930 Validation acc: 0.907500\n",
      "Epoch: 278/500 Iteration: 2505 Train loss: 0.159315 Train acc: 0.953333\n",
      "Epoch: 278/500 Iteration: 2510 Train loss: 0.186148 Train acc: 0.940000\n",
      "Epoch: 279/500 Iteration: 2515 Train loss: 0.114918 Train acc: 0.963333\n",
      "Epoch: 279/500 Iteration: 2520 Train loss: 0.140384 Train acc: 0.958333\n",
      "Epoch: 280/500 Iteration: 2525 Train loss: 0.148022 Train acc: 0.963333\n",
      "Epoch: 280/500 Iteration: 2525 Validation loss: 0.355995 Validation acc: 0.900833\n",
      "Epoch: 281/500 Iteration: 2530 Train loss: 0.130943 Train acc: 0.965000\n",
      "Epoch: 281/500 Iteration: 2535 Train loss: 0.106910 Train acc: 0.971667\n",
      "Epoch: 282/500 Iteration: 2540 Train loss: 0.161198 Train acc: 0.956667\n",
      "Epoch: 282/500 Iteration: 2545 Train loss: 0.127231 Train acc: 0.968333\n",
      "Epoch: 283/500 Iteration: 2550 Train loss: 0.133194 Train acc: 0.965000\n",
      "Epoch: 283/500 Iteration: 2550 Validation loss: 0.338413 Validation acc: 0.910000\n",
      "Epoch: 283/500 Iteration: 2555 Train loss: 0.150668 Train acc: 0.955000\n",
      "Epoch: 284/500 Iteration: 2560 Train loss: 0.097523 Train acc: 0.975000\n",
      "Epoch: 284/500 Iteration: 2565 Train loss: 0.110853 Train acc: 0.965000\n",
      "Epoch: 285/500 Iteration: 2570 Train loss: 0.167607 Train acc: 0.948333\n",
      "Epoch: 286/500 Iteration: 2575 Train loss: 0.112386 Train acc: 0.971667\n",
      "Epoch: 286/500 Iteration: 2575 Validation loss: 0.342479 Validation acc: 0.910000\n",
      "Epoch: 286/500 Iteration: 2580 Train loss: 0.107379 Train acc: 0.968333\n",
      "Epoch: 287/500 Iteration: 2585 Train loss: 0.184362 Train acc: 0.953333\n",
      "Epoch: 287/500 Iteration: 2590 Train loss: 0.126594 Train acc: 0.963333\n",
      "Epoch: 288/500 Iteration: 2595 Train loss: 0.131588 Train acc: 0.966667\n",
      "Epoch: 288/500 Iteration: 2600 Train loss: 0.156357 Train acc: 0.950000\n",
      "Epoch: 288/500 Iteration: 2600 Validation loss: 0.354935 Validation acc: 0.906667\n",
      "Epoch: 289/500 Iteration: 2605 Train loss: 0.105305 Train acc: 0.970000\n",
      "Epoch: 289/500 Iteration: 2610 Train loss: 0.113900 Train acc: 0.970000\n",
      "Epoch: 290/500 Iteration: 2615 Train loss: 0.122820 Train acc: 0.966667\n",
      "Epoch: 291/500 Iteration: 2620 Train loss: 0.112952 Train acc: 0.968333\n",
      "Epoch: 291/500 Iteration: 2625 Train loss: 0.113658 Train acc: 0.973333\n",
      "Epoch: 291/500 Iteration: 2625 Validation loss: 0.361889 Validation acc: 0.901667\n",
      "Epoch: 292/500 Iteration: 2630 Train loss: 0.181087 Train acc: 0.960000\n",
      "Epoch: 292/500 Iteration: 2635 Train loss: 0.121741 Train acc: 0.963333\n",
      "Epoch: 293/500 Iteration: 2640 Train loss: 0.110550 Train acc: 0.965000\n",
      "Epoch: 293/500 Iteration: 2645 Train loss: 0.137183 Train acc: 0.956667\n",
      "Epoch: 294/500 Iteration: 2650 Train loss: 0.101967 Train acc: 0.968333\n",
      "Epoch: 294/500 Iteration: 2650 Validation loss: 0.364045 Validation acc: 0.902500\n",
      "Epoch: 294/500 Iteration: 2655 Train loss: 0.134867 Train acc: 0.963333\n",
      "Epoch: 295/500 Iteration: 2660 Train loss: 0.140742 Train acc: 0.956667\n",
      "Epoch: 296/500 Iteration: 2665 Train loss: 0.126426 Train acc: 0.973333\n",
      "Epoch: 296/500 Iteration: 2670 Train loss: 0.124993 Train acc: 0.963333\n",
      "Epoch: 297/500 Iteration: 2675 Train loss: 0.178635 Train acc: 0.953333\n",
      "Epoch: 297/500 Iteration: 2675 Validation loss: 0.341847 Validation acc: 0.907500\n",
      "Epoch: 297/500 Iteration: 2680 Train loss: 0.112866 Train acc: 0.963333\n",
      "Epoch: 298/500 Iteration: 2685 Train loss: 0.127824 Train acc: 0.958333\n",
      "Epoch: 298/500 Iteration: 2690 Train loss: 0.171544 Train acc: 0.951667\n",
      "Epoch: 299/500 Iteration: 2695 Train loss: 0.101919 Train acc: 0.971667\n",
      "Epoch: 299/500 Iteration: 2700 Train loss: 0.139011 Train acc: 0.965000\n",
      "Epoch: 299/500 Iteration: 2700 Validation loss: 0.380551 Validation acc: 0.898333\n",
      "Epoch: 300/500 Iteration: 2705 Train loss: 0.130940 Train acc: 0.951667\n",
      "Epoch: 301/500 Iteration: 2710 Train loss: 0.114180 Train acc: 0.978333\n",
      "Epoch: 301/500 Iteration: 2715 Train loss: 0.112608 Train acc: 0.960000\n",
      "Epoch: 302/500 Iteration: 2720 Train loss: 0.193713 Train acc: 0.950000\n",
      "Epoch: 302/500 Iteration: 2725 Train loss: 0.124402 Train acc: 0.971667\n",
      "Epoch: 302/500 Iteration: 2725 Validation loss: 0.355260 Validation acc: 0.903333\n",
      "Epoch: 303/500 Iteration: 2730 Train loss: 0.141305 Train acc: 0.965000\n",
      "Epoch: 303/500 Iteration: 2735 Train loss: 0.177049 Train acc: 0.948333\n",
      "Epoch: 304/500 Iteration: 2740 Train loss: 0.094446 Train acc: 0.971667\n",
      "Epoch: 304/500 Iteration: 2745 Train loss: 0.116511 Train acc: 0.968333\n",
      "Epoch: 305/500 Iteration: 2750 Train loss: 0.102609 Train acc: 0.971667\n",
      "Epoch: 305/500 Iteration: 2750 Validation loss: 0.351106 Validation acc: 0.907500\n",
      "Epoch: 306/500 Iteration: 2755 Train loss: 0.105256 Train acc: 0.971667\n",
      "Epoch: 306/500 Iteration: 2760 Train loss: 0.105482 Train acc: 0.968333\n",
      "Epoch: 307/500 Iteration: 2765 Train loss: 0.136796 Train acc: 0.968333\n",
      "Epoch: 307/500 Iteration: 2770 Train loss: 0.106752 Train acc: 0.971667\n",
      "Epoch: 308/500 Iteration: 2775 Train loss: 0.097141 Train acc: 0.968333\n",
      "Epoch: 308/500 Iteration: 2775 Validation loss: 0.372467 Validation acc: 0.905000\n",
      "Epoch: 308/500 Iteration: 2780 Train loss: 0.147927 Train acc: 0.958333\n",
      "Epoch: 309/500 Iteration: 2785 Train loss: 0.096984 Train acc: 0.976667\n",
      "Epoch: 309/500 Iteration: 2790 Train loss: 0.110853 Train acc: 0.970000\n",
      "Epoch: 310/500 Iteration: 2795 Train loss: 0.120567 Train acc: 0.966667\n",
      "Epoch: 311/500 Iteration: 2800 Train loss: 0.121174 Train acc: 0.965000\n",
      "Epoch: 311/500 Iteration: 2800 Validation loss: 0.353903 Validation acc: 0.900833\n",
      "Epoch: 311/500 Iteration: 2805 Train loss: 0.079122 Train acc: 0.980000\n",
      "Epoch: 312/500 Iteration: 2810 Train loss: 0.154794 Train acc: 0.960000\n",
      "Epoch: 312/500 Iteration: 2815 Train loss: 0.122687 Train acc: 0.956667\n",
      "Epoch: 313/500 Iteration: 2820 Train loss: 0.153002 Train acc: 0.958333\n",
      "Epoch: 313/500 Iteration: 2825 Train loss: 0.183611 Train acc: 0.945000\n",
      "Epoch: 313/500 Iteration: 2825 Validation loss: 0.382311 Validation acc: 0.897500\n",
      "Epoch: 314/500 Iteration: 2830 Train loss: 0.126102 Train acc: 0.963333\n",
      "Epoch: 314/500 Iteration: 2835 Train loss: 0.132558 Train acc: 0.950000\n",
      "Epoch: 315/500 Iteration: 2840 Train loss: 0.144911 Train acc: 0.961667\n",
      "Epoch: 316/500 Iteration: 2845 Train loss: 0.099374 Train acc: 0.970000\n",
      "Epoch: 316/500 Iteration: 2850 Train loss: 0.093228 Train acc: 0.978333\n",
      "Epoch: 316/500 Iteration: 2850 Validation loss: 0.351912 Validation acc: 0.905833\n",
      "Epoch: 317/500 Iteration: 2855 Train loss: 0.151850 Train acc: 0.953333\n",
      "Epoch: 317/500 Iteration: 2860 Train loss: 0.120231 Train acc: 0.970000\n",
      "Epoch: 318/500 Iteration: 2865 Train loss: 0.101541 Train acc: 0.973333\n",
      "Epoch: 318/500 Iteration: 2870 Train loss: 0.133022 Train acc: 0.968333\n",
      "Epoch: 319/500 Iteration: 2875 Train loss: 0.091383 Train acc: 0.971667\n",
      "Epoch: 319/500 Iteration: 2875 Validation loss: 0.373906 Validation acc: 0.905000\n",
      "Epoch: 319/500 Iteration: 2880 Train loss: 0.105619 Train acc: 0.963333\n",
      "Epoch: 320/500 Iteration: 2885 Train loss: 0.114426 Train acc: 0.970000\n",
      "Epoch: 321/500 Iteration: 2890 Train loss: 0.122670 Train acc: 0.970000\n",
      "Epoch: 321/500 Iteration: 2895 Train loss: 0.103436 Train acc: 0.973333\n",
      "Epoch: 322/500 Iteration: 2900 Train loss: 0.154537 Train acc: 0.963333\n",
      "Epoch: 322/500 Iteration: 2900 Validation loss: 0.353335 Validation acc: 0.905833\n",
      "Epoch: 322/500 Iteration: 2905 Train loss: 0.098096 Train acc: 0.973333\n",
      "Epoch: 323/500 Iteration: 2910 Train loss: 0.104799 Train acc: 0.971667\n",
      "Epoch: 323/500 Iteration: 2915 Train loss: 0.127188 Train acc: 0.971667\n",
      "Epoch: 324/500 Iteration: 2920 Train loss: 0.081971 Train acc: 0.981667\n",
      "Epoch: 324/500 Iteration: 2925 Train loss: 0.125172 Train acc: 0.966667\n",
      "Epoch: 324/500 Iteration: 2925 Validation loss: 0.352341 Validation acc: 0.907500\n",
      "Epoch: 325/500 Iteration: 2930 Train loss: 0.108859 Train acc: 0.971667\n",
      "Epoch: 326/500 Iteration: 2935 Train loss: 0.090447 Train acc: 0.980000\n",
      "Epoch: 326/500 Iteration: 2940 Train loss: 0.089557 Train acc: 0.975000\n",
      "Epoch: 327/500 Iteration: 2945 Train loss: 0.165359 Train acc: 0.966667\n",
      "Epoch: 327/500 Iteration: 2950 Train loss: 0.085616 Train acc: 0.978333\n",
      "Epoch: 327/500 Iteration: 2950 Validation loss: 0.359910 Validation acc: 0.904167\n",
      "Epoch: 328/500 Iteration: 2955 Train loss: 0.100141 Train acc: 0.966667\n",
      "Epoch: 328/500 Iteration: 2960 Train loss: 0.109153 Train acc: 0.971667\n",
      "Epoch: 329/500 Iteration: 2965 Train loss: 0.070125 Train acc: 0.985000\n",
      "Epoch: 329/500 Iteration: 2970 Train loss: 0.106046 Train acc: 0.978333\n",
      "Epoch: 330/500 Iteration: 2975 Train loss: 0.107872 Train acc: 0.973333\n",
      "Epoch: 330/500 Iteration: 2975 Validation loss: 0.359499 Validation acc: 0.902500\n",
      "Epoch: 331/500 Iteration: 2980 Train loss: 0.089096 Train acc: 0.981667\n",
      "Epoch: 331/500 Iteration: 2985 Train loss: 0.089538 Train acc: 0.975000\n",
      "Epoch: 332/500 Iteration: 2990 Train loss: 0.160789 Train acc: 0.960000\n",
      "Epoch: 332/500 Iteration: 2995 Train loss: 0.098431 Train acc: 0.966667\n",
      "Epoch: 333/500 Iteration: 3000 Train loss: 0.087275 Train acc: 0.973333\n",
      "Epoch: 333/500 Iteration: 3000 Validation loss: 0.375834 Validation acc: 0.903333\n",
      "Epoch: 333/500 Iteration: 3005 Train loss: 0.135828 Train acc: 0.956667\n",
      "Epoch: 334/500 Iteration: 3010 Train loss: 0.078346 Train acc: 0.981667\n",
      "Epoch: 334/500 Iteration: 3015 Train loss: 0.111752 Train acc: 0.973333\n",
      "Epoch: 335/500 Iteration: 3020 Train loss: 0.116379 Train acc: 0.971667\n",
      "Epoch: 336/500 Iteration: 3025 Train loss: 0.098259 Train acc: 0.976667\n",
      "Epoch: 336/500 Iteration: 3025 Validation loss: 0.364017 Validation acc: 0.906667\n",
      "Epoch: 336/500 Iteration: 3030 Train loss: 0.092283 Train acc: 0.980000\n",
      "Epoch: 337/500 Iteration: 3035 Train loss: 0.151946 Train acc: 0.960000\n",
      "Epoch: 337/500 Iteration: 3040 Train loss: 0.116543 Train acc: 0.968333\n",
      "Epoch: 338/500 Iteration: 3045 Train loss: 0.105590 Train acc: 0.973333\n",
      "Epoch: 338/500 Iteration: 3050 Train loss: 0.120361 Train acc: 0.965000\n",
      "Epoch: 338/500 Iteration: 3050 Validation loss: 0.369359 Validation acc: 0.904167\n",
      "Epoch: 339/500 Iteration: 3055 Train loss: 0.068987 Train acc: 0.980000\n",
      "Epoch: 339/500 Iteration: 3060 Train loss: 0.110278 Train acc: 0.975000\n",
      "Epoch: 340/500 Iteration: 3065 Train loss: 0.099114 Train acc: 0.971667\n",
      "Epoch: 341/500 Iteration: 3070 Train loss: 0.100416 Train acc: 0.971667\n",
      "Epoch: 341/500 Iteration: 3075 Train loss: 0.064655 Train acc: 0.985000\n",
      "Epoch: 341/500 Iteration: 3075 Validation loss: 0.359484 Validation acc: 0.905000\n",
      "Epoch: 342/500 Iteration: 3080 Train loss: 0.146754 Train acc: 0.970000\n",
      "Epoch: 342/500 Iteration: 3085 Train loss: 0.086238 Train acc: 0.978333\n",
      "Epoch: 343/500 Iteration: 3090 Train loss: 0.095420 Train acc: 0.971667\n",
      "Epoch: 343/500 Iteration: 3095 Train loss: 0.132354 Train acc: 0.965000\n",
      "Epoch: 344/500 Iteration: 3100 Train loss: 0.071600 Train acc: 0.980000\n",
      "Epoch: 344/500 Iteration: 3100 Validation loss: 0.369817 Validation acc: 0.911667\n",
      "Epoch: 344/500 Iteration: 3105 Train loss: 0.125642 Train acc: 0.968333\n",
      "Epoch: 345/500 Iteration: 3110 Train loss: 0.092078 Train acc: 0.975000\n",
      "Epoch: 346/500 Iteration: 3115 Train loss: 0.099119 Train acc: 0.975000\n",
      "Epoch: 346/500 Iteration: 3120 Train loss: 0.079511 Train acc: 0.975000\n",
      "Epoch: 347/500 Iteration: 3125 Train loss: 0.105603 Train acc: 0.970000\n",
      "Epoch: 347/500 Iteration: 3125 Validation loss: 0.400095 Validation acc: 0.900833\n",
      "Epoch: 347/500 Iteration: 3130 Train loss: 0.088364 Train acc: 0.980000\n",
      "Epoch: 348/500 Iteration: 3135 Train loss: 0.092763 Train acc: 0.973333\n",
      "Epoch: 348/500 Iteration: 3140 Train loss: 0.123122 Train acc: 0.956667\n",
      "Epoch: 349/500 Iteration: 3145 Train loss: 0.065176 Train acc: 0.986667\n",
      "Epoch: 349/500 Iteration: 3150 Train loss: 0.102044 Train acc: 0.971667\n",
      "Epoch: 349/500 Iteration: 3150 Validation loss: 0.362899 Validation acc: 0.903333\n",
      "Epoch: 350/500 Iteration: 3155 Train loss: 0.109254 Train acc: 0.966667\n",
      "Epoch: 351/500 Iteration: 3160 Train loss: 0.105761 Train acc: 0.973333\n",
      "Epoch: 351/500 Iteration: 3165 Train loss: 0.099632 Train acc: 0.971667\n",
      "Epoch: 352/500 Iteration: 3170 Train loss: 0.141377 Train acc: 0.968333\n",
      "Epoch: 352/500 Iteration: 3175 Train loss: 0.107853 Train acc: 0.971667\n",
      "Epoch: 352/500 Iteration: 3175 Validation loss: 0.359581 Validation acc: 0.907500\n",
      "Epoch: 353/500 Iteration: 3180 Train loss: 0.089764 Train acc: 0.978333\n",
      "Epoch: 353/500 Iteration: 3185 Train loss: 0.130732 Train acc: 0.963333\n",
      "Epoch: 354/500 Iteration: 3190 Train loss: 0.084226 Train acc: 0.978333\n",
      "Epoch: 354/500 Iteration: 3195 Train loss: 0.117376 Train acc: 0.968333\n",
      "Epoch: 355/500 Iteration: 3200 Train loss: 0.084324 Train acc: 0.975000\n",
      "Epoch: 355/500 Iteration: 3200 Validation loss: 0.359512 Validation acc: 0.904167\n",
      "Epoch: 356/500 Iteration: 3205 Train loss: 0.094191 Train acc: 0.980000\n",
      "Epoch: 356/500 Iteration: 3210 Train loss: 0.066189 Train acc: 0.983333\n",
      "Epoch: 357/500 Iteration: 3215 Train loss: 0.126540 Train acc: 0.963333\n",
      "Epoch: 357/500 Iteration: 3220 Train loss: 0.082128 Train acc: 0.980000\n",
      "Epoch: 358/500 Iteration: 3225 Train loss: 0.073873 Train acc: 0.978333\n",
      "Epoch: 358/500 Iteration: 3225 Validation loss: 0.388192 Validation acc: 0.905833\n",
      "Epoch: 358/500 Iteration: 3230 Train loss: 0.130220 Train acc: 0.965000\n",
      "Epoch: 359/500 Iteration: 3235 Train loss: 0.066217 Train acc: 0.978333\n",
      "Epoch: 359/500 Iteration: 3240 Train loss: 0.111714 Train acc: 0.971667\n",
      "Epoch: 360/500 Iteration: 3245 Train loss: 0.101367 Train acc: 0.971667\n",
      "Epoch: 361/500 Iteration: 3250 Train loss: 0.102329 Train acc: 0.971667\n",
      "Epoch: 361/500 Iteration: 3250 Validation loss: 0.362644 Validation acc: 0.914167\n",
      "Epoch: 361/500 Iteration: 3255 Train loss: 0.095208 Train acc: 0.976667\n",
      "Epoch: 362/500 Iteration: 3260 Train loss: 0.138556 Train acc: 0.963333\n",
      "Epoch: 362/500 Iteration: 3265 Train loss: 0.101125 Train acc: 0.965000\n",
      "Epoch: 363/500 Iteration: 3270 Train loss: 0.078476 Train acc: 0.978333\n",
      "Epoch: 363/500 Iteration: 3275 Train loss: 0.113110 Train acc: 0.961667\n",
      "Epoch: 363/500 Iteration: 3275 Validation loss: 0.349358 Validation acc: 0.906667\n",
      "Epoch: 364/500 Iteration: 3280 Train loss: 0.076863 Train acc: 0.976667\n",
      "Epoch: 364/500 Iteration: 3285 Train loss: 0.104837 Train acc: 0.975000\n",
      "Epoch: 365/500 Iteration: 3290 Train loss: 0.098260 Train acc: 0.965000\n",
      "Epoch: 366/500 Iteration: 3295 Train loss: 0.099352 Train acc: 0.973333\n",
      "Epoch: 366/500 Iteration: 3300 Train loss: 0.074885 Train acc: 0.986667\n",
      "Epoch: 366/500 Iteration: 3300 Validation loss: 0.370582 Validation acc: 0.909167\n",
      "Epoch: 367/500 Iteration: 3305 Train loss: 0.114362 Train acc: 0.970000\n",
      "Epoch: 367/500 Iteration: 3310 Train loss: 0.073982 Train acc: 0.978333\n",
      "Epoch: 368/500 Iteration: 3315 Train loss: 0.096095 Train acc: 0.976667\n",
      "Epoch: 368/500 Iteration: 3320 Train loss: 0.109610 Train acc: 0.965000\n",
      "Epoch: 369/500 Iteration: 3325 Train loss: 0.081122 Train acc: 0.975000\n",
      "Epoch: 369/500 Iteration: 3325 Validation loss: 0.354710 Validation acc: 0.910833\n",
      "Epoch: 369/500 Iteration: 3330 Train loss: 0.105006 Train acc: 0.973333\n",
      "Epoch: 370/500 Iteration: 3335 Train loss: 0.106966 Train acc: 0.970000\n",
      "Epoch: 371/500 Iteration: 3340 Train loss: 0.097431 Train acc: 0.981667\n",
      "Epoch: 371/500 Iteration: 3345 Train loss: 0.066258 Train acc: 0.985000\n",
      "Epoch: 372/500 Iteration: 3350 Train loss: 0.147112 Train acc: 0.970000\n",
      "Epoch: 372/500 Iteration: 3350 Validation loss: 0.385346 Validation acc: 0.900833\n",
      "Epoch: 372/500 Iteration: 3355 Train loss: 0.087923 Train acc: 0.980000\n",
      "Epoch: 373/500 Iteration: 3360 Train loss: 0.115965 Train acc: 0.970000\n",
      "Epoch: 373/500 Iteration: 3365 Train loss: 0.155558 Train acc: 0.946667\n",
      "Epoch: 374/500 Iteration: 3370 Train loss: 0.073555 Train acc: 0.976667\n",
      "Epoch: 374/500 Iteration: 3375 Train loss: 0.105281 Train acc: 0.968333\n",
      "Epoch: 374/500 Iteration: 3375 Validation loss: 0.377670 Validation acc: 0.907500\n",
      "Epoch: 375/500 Iteration: 3380 Train loss: 0.090719 Train acc: 0.980000\n",
      "Epoch: 376/500 Iteration: 3385 Train loss: 0.090512 Train acc: 0.976667\n",
      "Epoch: 376/500 Iteration: 3390 Train loss: 0.106932 Train acc: 0.971667\n",
      "Epoch: 377/500 Iteration: 3395 Train loss: 0.114373 Train acc: 0.970000\n",
      "Epoch: 377/500 Iteration: 3400 Train loss: 0.088770 Train acc: 0.976667\n",
      "Epoch: 377/500 Iteration: 3400 Validation loss: 0.352014 Validation acc: 0.913333\n",
      "Epoch: 378/500 Iteration: 3405 Train loss: 0.092834 Train acc: 0.966667\n",
      "Epoch: 378/500 Iteration: 3410 Train loss: 0.130266 Train acc: 0.961667\n",
      "Epoch: 379/500 Iteration: 3415 Train loss: 0.069470 Train acc: 0.983333\n",
      "Epoch: 379/500 Iteration: 3420 Train loss: 0.103446 Train acc: 0.973333\n",
      "Epoch: 380/500 Iteration: 3425 Train loss: 0.106860 Train acc: 0.973333\n",
      "Epoch: 380/500 Iteration: 3425 Validation loss: 0.367330 Validation acc: 0.908333\n",
      "Epoch: 381/500 Iteration: 3430 Train loss: 0.118611 Train acc: 0.973333\n",
      "Epoch: 381/500 Iteration: 3435 Train loss: 0.077969 Train acc: 0.985000\n",
      "Epoch: 382/500 Iteration: 3440 Train loss: 0.111794 Train acc: 0.976667\n",
      "Epoch: 382/500 Iteration: 3445 Train loss: 0.081720 Train acc: 0.975000\n",
      "Epoch: 383/500 Iteration: 3450 Train loss: 0.083620 Train acc: 0.980000\n",
      "Epoch: 383/500 Iteration: 3450 Validation loss: 0.381530 Validation acc: 0.908333\n",
      "Epoch: 383/500 Iteration: 3455 Train loss: 0.116984 Train acc: 0.958333\n",
      "Epoch: 384/500 Iteration: 3460 Train loss: 0.059729 Train acc: 0.983333\n",
      "Epoch: 384/500 Iteration: 3465 Train loss: 0.103813 Train acc: 0.975000\n",
      "Epoch: 385/500 Iteration: 3470 Train loss: 0.077686 Train acc: 0.976667\n",
      "Epoch: 386/500 Iteration: 3475 Train loss: 0.093093 Train acc: 0.980000\n",
      "Epoch: 386/500 Iteration: 3475 Validation loss: 0.384541 Validation acc: 0.911667\n",
      "Epoch: 386/500 Iteration: 3480 Train loss: 0.082219 Train acc: 0.970000\n",
      "Epoch: 387/500 Iteration: 3485 Train loss: 0.128602 Train acc: 0.968333\n",
      "Epoch: 387/500 Iteration: 3490 Train loss: 0.107719 Train acc: 0.973333\n",
      "Epoch: 388/500 Iteration: 3495 Train loss: 0.066254 Train acc: 0.980000\n",
      "Epoch: 388/500 Iteration: 3500 Train loss: 0.092440 Train acc: 0.973333\n",
      "Epoch: 388/500 Iteration: 3500 Validation loss: 0.363947 Validation acc: 0.913333\n",
      "Epoch: 389/500 Iteration: 3505 Train loss: 0.072143 Train acc: 0.975000\n",
      "Epoch: 389/500 Iteration: 3510 Train loss: 0.098636 Train acc: 0.973333\n",
      "Epoch: 390/500 Iteration: 3515 Train loss: 0.084885 Train acc: 0.975000\n",
      "Epoch: 391/500 Iteration: 3520 Train loss: 0.095144 Train acc: 0.975000\n",
      "Epoch: 391/500 Iteration: 3525 Train loss: 0.104512 Train acc: 0.971667\n",
      "Epoch: 391/500 Iteration: 3525 Validation loss: 0.374654 Validation acc: 0.907500\n",
      "Epoch: 392/500 Iteration: 3530 Train loss: 0.144956 Train acc: 0.970000\n",
      "Epoch: 392/500 Iteration: 3535 Train loss: 0.069161 Train acc: 0.980000\n",
      "Epoch: 393/500 Iteration: 3540 Train loss: 0.088770 Train acc: 0.976667\n",
      "Epoch: 393/500 Iteration: 3545 Train loss: 0.103639 Train acc: 0.970000\n",
      "Epoch: 394/500 Iteration: 3550 Train loss: 0.074130 Train acc: 0.970000\n",
      "Epoch: 394/500 Iteration: 3550 Validation loss: 0.375141 Validation acc: 0.905000\n",
      "Epoch: 394/500 Iteration: 3555 Train loss: 0.099329 Train acc: 0.976667\n",
      "Epoch: 395/500 Iteration: 3560 Train loss: 0.098971 Train acc: 0.973333\n",
      "Epoch: 396/500 Iteration: 3565 Train loss: 0.117698 Train acc: 0.970000\n",
      "Epoch: 396/500 Iteration: 3570 Train loss: 0.087489 Train acc: 0.978333\n",
      "Epoch: 397/500 Iteration: 3575 Train loss: 0.101025 Train acc: 0.975000\n",
      "Epoch: 397/500 Iteration: 3575 Validation loss: 0.389789 Validation acc: 0.912500\n",
      "Epoch: 397/500 Iteration: 3580 Train loss: 0.094746 Train acc: 0.973333\n",
      "Epoch: 398/500 Iteration: 3585 Train loss: 0.083365 Train acc: 0.978333\n",
      "Epoch: 398/500 Iteration: 3590 Train loss: 0.132776 Train acc: 0.961667\n",
      "Epoch: 399/500 Iteration: 3595 Train loss: 0.111269 Train acc: 0.976667\n",
      "Epoch: 399/500 Iteration: 3600 Train loss: 0.131643 Train acc: 0.961667\n",
      "Epoch: 399/500 Iteration: 3600 Validation loss: 0.399056 Validation acc: 0.903333\n",
      "Epoch: 400/500 Iteration: 3605 Train loss: 0.113308 Train acc: 0.970000\n",
      "Epoch: 401/500 Iteration: 3610 Train loss: 0.111634 Train acc: 0.978333\n",
      "Epoch: 401/500 Iteration: 3615 Train loss: 0.198054 Train acc: 0.936667\n",
      "Epoch: 402/500 Iteration: 3620 Train loss: 0.245974 Train acc: 0.938333\n",
      "Epoch: 402/500 Iteration: 3625 Train loss: 0.134597 Train acc: 0.958333\n",
      "Epoch: 402/500 Iteration: 3625 Validation loss: 0.378709 Validation acc: 0.895833\n",
      "Epoch: 403/500 Iteration: 3630 Train loss: 0.109241 Train acc: 0.971667\n",
      "Epoch: 403/500 Iteration: 3635 Train loss: 0.145735 Train acc: 0.955000\n",
      "Epoch: 404/500 Iteration: 3640 Train loss: 0.109199 Train acc: 0.968333\n",
      "Epoch: 404/500 Iteration: 3645 Train loss: 0.128307 Train acc: 0.963333\n",
      "Epoch: 405/500 Iteration: 3650 Train loss: 0.113166 Train acc: 0.971667\n",
      "Epoch: 405/500 Iteration: 3650 Validation loss: 0.348510 Validation acc: 0.905833\n",
      "Epoch: 406/500 Iteration: 3655 Train loss: 0.104242 Train acc: 0.976667\n",
      "Epoch: 406/500 Iteration: 3660 Train loss: 0.098240 Train acc: 0.970000\n",
      "Epoch: 407/500 Iteration: 3665 Train loss: 0.160391 Train acc: 0.956667\n",
      "Epoch: 407/500 Iteration: 3670 Train loss: 0.063838 Train acc: 0.986667\n",
      "Epoch: 408/500 Iteration: 3675 Train loss: 0.095948 Train acc: 0.968333\n",
      "Epoch: 408/500 Iteration: 3675 Validation loss: 0.352193 Validation acc: 0.911667\n",
      "Epoch: 408/500 Iteration: 3680 Train loss: 0.132810 Train acc: 0.960000\n",
      "Epoch: 409/500 Iteration: 3685 Train loss: 0.071952 Train acc: 0.976667\n",
      "Epoch: 409/500 Iteration: 3690 Train loss: 0.130921 Train acc: 0.971667\n",
      "Epoch: 410/500 Iteration: 3695 Train loss: 0.149422 Train acc: 0.960000\n",
      "Epoch: 411/500 Iteration: 3700 Train loss: 0.105063 Train acc: 0.973333\n",
      "Epoch: 411/500 Iteration: 3700 Validation loss: 0.371791 Validation acc: 0.905000\n",
      "Epoch: 411/500 Iteration: 3705 Train loss: 0.092568 Train acc: 0.970000\n",
      "Epoch: 412/500 Iteration: 3710 Train loss: 0.138432 Train acc: 0.963333\n",
      "Epoch: 412/500 Iteration: 3715 Train loss: 0.081306 Train acc: 0.973333\n",
      "Epoch: 413/500 Iteration: 3720 Train loss: 0.073354 Train acc: 0.976667\n",
      "Epoch: 413/500 Iteration: 3725 Train loss: 0.112616 Train acc: 0.968333\n",
      "Epoch: 413/500 Iteration: 3725 Validation loss: 0.371231 Validation acc: 0.904167\n",
      "Epoch: 414/500 Iteration: 3730 Train loss: 0.071483 Train acc: 0.978333\n",
      "Epoch: 414/500 Iteration: 3735 Train loss: 0.094191 Train acc: 0.976667\n",
      "Epoch: 415/500 Iteration: 3740 Train loss: 0.076319 Train acc: 0.973333\n",
      "Epoch: 416/500 Iteration: 3745 Train loss: 0.085553 Train acc: 0.981667\n",
      "Epoch: 416/500 Iteration: 3750 Train loss: 0.069996 Train acc: 0.980000\n",
      "Epoch: 416/500 Iteration: 3750 Validation loss: 0.375728 Validation acc: 0.905000\n",
      "Epoch: 417/500 Iteration: 3755 Train loss: 0.126236 Train acc: 0.975000\n",
      "Epoch: 417/500 Iteration: 3760 Train loss: 0.070015 Train acc: 0.983333\n",
      "Epoch: 418/500 Iteration: 3765 Train loss: 0.055543 Train acc: 0.993333\n",
      "Epoch: 418/500 Iteration: 3770 Train loss: 0.093200 Train acc: 0.973333\n",
      "Epoch: 419/500 Iteration: 3775 Train loss: 0.056189 Train acc: 0.978333\n",
      "Epoch: 419/500 Iteration: 3775 Validation loss: 0.385714 Validation acc: 0.906667\n",
      "Epoch: 419/500 Iteration: 3780 Train loss: 0.102540 Train acc: 0.971667\n",
      "Epoch: 420/500 Iteration: 3785 Train loss: 0.077833 Train acc: 0.975000\n",
      "Epoch: 421/500 Iteration: 3790 Train loss: 0.101822 Train acc: 0.980000\n",
      "Epoch: 421/500 Iteration: 3795 Train loss: 0.067660 Train acc: 0.981667\n",
      "Epoch: 422/500 Iteration: 3800 Train loss: 0.133994 Train acc: 0.970000\n",
      "Epoch: 422/500 Iteration: 3800 Validation loss: 0.382186 Validation acc: 0.905000\n",
      "Epoch: 422/500 Iteration: 3805 Train loss: 0.060848 Train acc: 0.985000\n",
      "Epoch: 423/500 Iteration: 3810 Train loss: 0.065505 Train acc: 0.985000\n",
      "Epoch: 423/500 Iteration: 3815 Train loss: 0.097480 Train acc: 0.975000\n",
      "Epoch: 424/500 Iteration: 3820 Train loss: 0.053089 Train acc: 0.988333\n",
      "Epoch: 424/500 Iteration: 3825 Train loss: 0.089621 Train acc: 0.978333\n",
      "Epoch: 424/500 Iteration: 3825 Validation loss: 0.378083 Validation acc: 0.903333\n",
      "Epoch: 425/500 Iteration: 3830 Train loss: 0.067802 Train acc: 0.983333\n",
      "Epoch: 426/500 Iteration: 3835 Train loss: 0.082154 Train acc: 0.981667\n",
      "Epoch: 426/500 Iteration: 3840 Train loss: 0.057175 Train acc: 0.986667\n",
      "Epoch: 427/500 Iteration: 3845 Train loss: 0.123642 Train acc: 0.971667\n",
      "Epoch: 427/500 Iteration: 3850 Train loss: 0.069272 Train acc: 0.983333\n",
      "Epoch: 427/500 Iteration: 3850 Validation loss: 0.405499 Validation acc: 0.905833\n",
      "Epoch: 428/500 Iteration: 3855 Train loss: 0.083682 Train acc: 0.980000\n",
      "Epoch: 428/500 Iteration: 3860 Train loss: 0.100441 Train acc: 0.970000\n",
      "Epoch: 429/500 Iteration: 3865 Train loss: 0.052533 Train acc: 0.985000\n",
      "Epoch: 429/500 Iteration: 3870 Train loss: 0.080536 Train acc: 0.973333\n",
      "Epoch: 430/500 Iteration: 3875 Train loss: 0.071644 Train acc: 0.981667\n",
      "Epoch: 430/500 Iteration: 3875 Validation loss: 0.394874 Validation acc: 0.906667\n",
      "Epoch: 431/500 Iteration: 3880 Train loss: 0.076347 Train acc: 0.985000\n",
      "Epoch: 431/500 Iteration: 3885 Train loss: 0.061102 Train acc: 0.986667\n",
      "Epoch: 432/500 Iteration: 3890 Train loss: 0.116728 Train acc: 0.976667\n",
      "Epoch: 432/500 Iteration: 3895 Train loss: 0.066014 Train acc: 0.986667\n",
      "Epoch: 433/500 Iteration: 3900 Train loss: 0.083434 Train acc: 0.976667\n",
      "Epoch: 433/500 Iteration: 3900 Validation loss: 0.374008 Validation acc: 0.911667\n",
      "Epoch: 433/500 Iteration: 3905 Train loss: 0.088515 Train acc: 0.973333\n",
      "Epoch: 434/500 Iteration: 3910 Train loss: 0.064312 Train acc: 0.980000\n",
      "Epoch: 434/500 Iteration: 3915 Train loss: 0.074025 Train acc: 0.983333\n",
      "Epoch: 435/500 Iteration: 3920 Train loss: 0.099484 Train acc: 0.968333\n",
      "Epoch: 436/500 Iteration: 3925 Train loss: 0.121353 Train acc: 0.973333\n",
      "Epoch: 436/500 Iteration: 3925 Validation loss: 0.418065 Validation acc: 0.904167\n",
      "Epoch: 436/500 Iteration: 3930 Train loss: 0.121814 Train acc: 0.968333\n",
      "Epoch: 437/500 Iteration: 3935 Train loss: 0.139756 Train acc: 0.968333\n",
      "Epoch: 437/500 Iteration: 3940 Train loss: 0.110243 Train acc: 0.965000\n",
      "Epoch: 438/500 Iteration: 3945 Train loss: 0.099327 Train acc: 0.978333\n",
      "Epoch: 438/500 Iteration: 3950 Train loss: 0.118436 Train acc: 0.956667\n",
      "Epoch: 438/500 Iteration: 3950 Validation loss: 0.364090 Validation acc: 0.912500\n",
      "Epoch: 439/500 Iteration: 3955 Train loss: 0.067257 Train acc: 0.978333\n",
      "Epoch: 439/500 Iteration: 3960 Train loss: 0.111307 Train acc: 0.970000\n",
      "Epoch: 440/500 Iteration: 3965 Train loss: 0.074149 Train acc: 0.983333\n",
      "Epoch: 441/500 Iteration: 3970 Train loss: 0.075878 Train acc: 0.985000\n",
      "Epoch: 441/500 Iteration: 3975 Train loss: 0.072216 Train acc: 0.980000\n",
      "Epoch: 441/500 Iteration: 3975 Validation loss: 0.380774 Validation acc: 0.902500\n",
      "Epoch: 442/500 Iteration: 3980 Train loss: 0.121504 Train acc: 0.976667\n",
      "Epoch: 442/500 Iteration: 3985 Train loss: 0.072041 Train acc: 0.976667\n",
      "Epoch: 443/500 Iteration: 3990 Train loss: 0.070449 Train acc: 0.973333\n",
      "Epoch: 443/500 Iteration: 3995 Train loss: 0.089784 Train acc: 0.978333\n",
      "Epoch: 444/500 Iteration: 4000 Train loss: 0.049333 Train acc: 0.991667\n",
      "Epoch: 444/500 Iteration: 4000 Validation loss: 0.376770 Validation acc: 0.908333\n",
      "Epoch: 444/500 Iteration: 4005 Train loss: 0.121261 Train acc: 0.965000\n",
      "Epoch: 445/500 Iteration: 4010 Train loss: 0.078562 Train acc: 0.975000\n",
      "Epoch: 446/500 Iteration: 4015 Train loss: 0.080741 Train acc: 0.988333\n",
      "Epoch: 446/500 Iteration: 4020 Train loss: 0.082428 Train acc: 0.980000\n",
      "Epoch: 447/500 Iteration: 4025 Train loss: 0.130269 Train acc: 0.971667\n",
      "Epoch: 447/500 Iteration: 4025 Validation loss: 0.397093 Validation acc: 0.905833\n",
      "Epoch: 447/500 Iteration: 4030 Train loss: 0.049327 Train acc: 0.986667\n",
      "Epoch: 448/500 Iteration: 4035 Train loss: 0.036798 Train acc: 0.991667\n",
      "Epoch: 448/500 Iteration: 4040 Train loss: 0.106113 Train acc: 0.968333\n",
      "Epoch: 449/500 Iteration: 4045 Train loss: 0.063600 Train acc: 0.986667\n",
      "Epoch: 449/500 Iteration: 4050 Train loss: 0.092150 Train acc: 0.980000\n",
      "Epoch: 449/500 Iteration: 4050 Validation loss: 0.397578 Validation acc: 0.903333\n",
      "Epoch: 450/500 Iteration: 4055 Train loss: 0.085474 Train acc: 0.981667\n",
      "Epoch: 451/500 Iteration: 4060 Train loss: 0.103709 Train acc: 0.975000\n",
      "Epoch: 451/500 Iteration: 4065 Train loss: 0.074135 Train acc: 0.981667\n",
      "Epoch: 452/500 Iteration: 4070 Train loss: 0.135186 Train acc: 0.965000\n",
      "Epoch: 452/500 Iteration: 4075 Train loss: 0.091287 Train acc: 0.975000\n",
      "Epoch: 452/500 Iteration: 4075 Validation loss: 0.376635 Validation acc: 0.905000\n",
      "Epoch: 453/500 Iteration: 4080 Train loss: 0.070362 Train acc: 0.976667\n",
      "Epoch: 453/500 Iteration: 4085 Train loss: 0.098753 Train acc: 0.973333\n",
      "Epoch: 454/500 Iteration: 4090 Train loss: 0.068835 Train acc: 0.981667\n",
      "Epoch: 454/500 Iteration: 4095 Train loss: 0.082042 Train acc: 0.985000\n",
      "Epoch: 455/500 Iteration: 4100 Train loss: 0.081852 Train acc: 0.976667\n",
      "Epoch: 455/500 Iteration: 4100 Validation loss: 0.395628 Validation acc: 0.903333\n",
      "Epoch: 456/500 Iteration: 4105 Train loss: 0.074530 Train acc: 0.985000\n",
      "Epoch: 456/500 Iteration: 4110 Train loss: 0.057711 Train acc: 0.983333\n",
      "Epoch: 457/500 Iteration: 4115 Train loss: 0.128953 Train acc: 0.971667\n",
      "Epoch: 457/500 Iteration: 4120 Train loss: 0.070643 Train acc: 0.980000\n",
      "Epoch: 458/500 Iteration: 4125 Train loss: 0.049362 Train acc: 0.988333\n",
      "Epoch: 458/500 Iteration: 4125 Validation loss: 0.404592 Validation acc: 0.906667\n",
      "Epoch: 458/500 Iteration: 4130 Train loss: 0.106204 Train acc: 0.968333\n",
      "Epoch: 459/500 Iteration: 4135 Train loss: 0.072355 Train acc: 0.980000\n",
      "Epoch: 459/500 Iteration: 4140 Train loss: 0.094491 Train acc: 0.978333\n",
      "Epoch: 460/500 Iteration: 4145 Train loss: 0.059378 Train acc: 0.981667\n",
      "Epoch: 461/500 Iteration: 4150 Train loss: 0.065873 Train acc: 0.988333\n",
      "Epoch: 461/500 Iteration: 4150 Validation loss: 0.416077 Validation acc: 0.903333\n",
      "Epoch: 461/500 Iteration: 4155 Train loss: 0.057324 Train acc: 0.985000\n",
      "Epoch: 462/500 Iteration: 4160 Train loss: 0.088440 Train acc: 0.985000\n",
      "Epoch: 462/500 Iteration: 4165 Train loss: 0.042956 Train acc: 0.990000\n",
      "Epoch: 463/500 Iteration: 4170 Train loss: 0.063166 Train acc: 0.986667\n",
      "Epoch: 463/500 Iteration: 4175 Train loss: 0.083700 Train acc: 0.978333\n",
      "Epoch: 463/500 Iteration: 4175 Validation loss: 0.401817 Validation acc: 0.906667\n",
      "Epoch: 464/500 Iteration: 4180 Train loss: 0.059513 Train acc: 0.980000\n",
      "Epoch: 464/500 Iteration: 4185 Train loss: 0.084872 Train acc: 0.978333\n",
      "Epoch: 465/500 Iteration: 4190 Train loss: 0.078968 Train acc: 0.973333\n",
      "Epoch: 466/500 Iteration: 4195 Train loss: 0.088235 Train acc: 0.983333\n",
      "Epoch: 466/500 Iteration: 4200 Train loss: 0.071765 Train acc: 0.978333\n",
      "Epoch: 466/500 Iteration: 4200 Validation loss: 0.410158 Validation acc: 0.906667\n",
      "Epoch: 467/500 Iteration: 4205 Train loss: 0.120980 Train acc: 0.963333\n",
      "Epoch: 467/500 Iteration: 4210 Train loss: 0.075124 Train acc: 0.976667\n",
      "Epoch: 468/500 Iteration: 4215 Train loss: 0.067991 Train acc: 0.980000\n",
      "Epoch: 468/500 Iteration: 4220 Train loss: 0.102991 Train acc: 0.970000\n",
      "Epoch: 469/500 Iteration: 4225 Train loss: 0.053125 Train acc: 0.986667\n",
      "Epoch: 469/500 Iteration: 4225 Validation loss: 0.403722 Validation acc: 0.908333\n",
      "Epoch: 469/500 Iteration: 4230 Train loss: 0.079997 Train acc: 0.981667\n",
      "Epoch: 470/500 Iteration: 4235 Train loss: 0.055773 Train acc: 0.985000\n",
      "Epoch: 471/500 Iteration: 4240 Train loss: 0.080959 Train acc: 0.985000\n",
      "Epoch: 471/500 Iteration: 4245 Train loss: 0.046292 Train acc: 0.988333\n",
      "Epoch: 472/500 Iteration: 4250 Train loss: 0.079279 Train acc: 0.985000\n",
      "Epoch: 472/500 Iteration: 4250 Validation loss: 0.432822 Validation acc: 0.900833\n",
      "Epoch: 472/500 Iteration: 4255 Train loss: 0.043733 Train acc: 0.995000\n",
      "Epoch: 473/500 Iteration: 4260 Train loss: 0.044858 Train acc: 0.988333\n",
      "Epoch: 473/500 Iteration: 4265 Train loss: 0.078517 Train acc: 0.975000\n",
      "Epoch: 474/500 Iteration: 4270 Train loss: 0.046471 Train acc: 0.985000\n",
      "Epoch: 474/500 Iteration: 4275 Train loss: 0.080327 Train acc: 0.983333\n",
      "Epoch: 474/500 Iteration: 4275 Validation loss: 0.416580 Validation acc: 0.901667\n",
      "Epoch: 475/500 Iteration: 4280 Train loss: 0.042315 Train acc: 0.990000\n",
      "Epoch: 476/500 Iteration: 4285 Train loss: 0.068840 Train acc: 0.983333\n",
      "Epoch: 476/500 Iteration: 4290 Train loss: 0.057290 Train acc: 0.981667\n",
      "Epoch: 477/500 Iteration: 4295 Train loss: 0.086137 Train acc: 0.981667\n",
      "Epoch: 477/500 Iteration: 4300 Train loss: 0.046267 Train acc: 0.988333\n",
      "Epoch: 477/500 Iteration: 4300 Validation loss: 0.405539 Validation acc: 0.907500\n",
      "Epoch: 478/500 Iteration: 4305 Train loss: 0.076512 Train acc: 0.983333\n",
      "Epoch: 478/500 Iteration: 4310 Train loss: 0.084051 Train acc: 0.978333\n",
      "Epoch: 479/500 Iteration: 4315 Train loss: 0.087022 Train acc: 0.976667\n",
      "Epoch: 479/500 Iteration: 4320 Train loss: 0.107860 Train acc: 0.971667\n",
      "Epoch: 480/500 Iteration: 4325 Train loss: 0.076437 Train acc: 0.985000\n",
      "Epoch: 480/500 Iteration: 4325 Validation loss: 0.384160 Validation acc: 0.915000\n",
      "Epoch: 481/500 Iteration: 4330 Train loss: 0.062296 Train acc: 0.990000\n",
      "Epoch: 481/500 Iteration: 4335 Train loss: 0.078288 Train acc: 0.976667\n",
      "Epoch: 482/500 Iteration: 4340 Train loss: 0.105437 Train acc: 0.976667\n",
      "Epoch: 482/500 Iteration: 4345 Train loss: 0.080330 Train acc: 0.971667\n",
      "Epoch: 483/500 Iteration: 4350 Train loss: 0.061953 Train acc: 0.983333\n",
      "Epoch: 483/500 Iteration: 4350 Validation loss: 0.410058 Validation acc: 0.908333\n",
      "Epoch: 483/500 Iteration: 4355 Train loss: 0.086225 Train acc: 0.973333\n",
      "Epoch: 484/500 Iteration: 4360 Train loss: 0.036841 Train acc: 0.993333\n",
      "Epoch: 484/500 Iteration: 4365 Train loss: 0.088289 Train acc: 0.981667\n",
      "Epoch: 485/500 Iteration: 4370 Train loss: 0.059185 Train acc: 0.983333\n",
      "Epoch: 486/500 Iteration: 4375 Train loss: 0.064570 Train acc: 0.991667\n",
      "Epoch: 486/500 Iteration: 4375 Validation loss: 0.440450 Validation acc: 0.905833\n",
      "Epoch: 486/500 Iteration: 4380 Train loss: 0.045201 Train acc: 0.986667\n",
      "Epoch: 487/500 Iteration: 4385 Train loss: 0.108484 Train acc: 0.980000\n",
      "Epoch: 487/500 Iteration: 4390 Train loss: 0.044646 Train acc: 0.986667\n",
      "Epoch: 488/500 Iteration: 4395 Train loss: 0.048391 Train acc: 0.986667\n",
      "Epoch: 488/500 Iteration: 4400 Train loss: 0.084662 Train acc: 0.978333\n",
      "Epoch: 488/500 Iteration: 4400 Validation loss: 0.434102 Validation acc: 0.907500\n",
      "Epoch: 489/500 Iteration: 4405 Train loss: 0.043077 Train acc: 0.988333\n",
      "Epoch: 489/500 Iteration: 4410 Train loss: 0.077060 Train acc: 0.978333\n",
      "Epoch: 490/500 Iteration: 4415 Train loss: 0.053324 Train acc: 0.981667\n",
      "Epoch: 491/500 Iteration: 4420 Train loss: 0.070377 Train acc: 0.988333\n",
      "Epoch: 491/500 Iteration: 4425 Train loss: 0.047817 Train acc: 0.990000\n",
      "Epoch: 491/500 Iteration: 4425 Validation loss: 0.422834 Validation acc: 0.907500\n",
      "Epoch: 492/500 Iteration: 4430 Train loss: 0.091922 Train acc: 0.978333\n",
      "Epoch: 492/500 Iteration: 4435 Train loss: 0.039424 Train acc: 0.990000\n",
      "Epoch: 493/500 Iteration: 4440 Train loss: 0.042900 Train acc: 0.986667\n",
      "Epoch: 493/500 Iteration: 4445 Train loss: 0.086115 Train acc: 0.980000\n",
      "Epoch: 494/500 Iteration: 4450 Train loss: 0.053880 Train acc: 0.983333\n",
      "Epoch: 494/500 Iteration: 4450 Validation loss: 0.455358 Validation acc: 0.904167\n",
      "Epoch: 494/500 Iteration: 4455 Train loss: 0.073222 Train acc: 0.980000\n",
      "Epoch: 495/500 Iteration: 4460 Train loss: 0.071310 Train acc: 0.980000\n",
      "Epoch: 496/500 Iteration: 4465 Train loss: 0.061290 Train acc: 0.983333\n",
      "Epoch: 496/500 Iteration: 4470 Train loss: 0.049147 Train acc: 0.988333\n",
      "Epoch: 497/500 Iteration: 4475 Train loss: 0.095674 Train acc: 0.973333\n",
      "Epoch: 497/500 Iteration: 4475 Validation loss: 0.441132 Validation acc: 0.903333\n",
      "Epoch: 497/500 Iteration: 4480 Train loss: 0.046935 Train acc: 0.986667\n",
      "Epoch: 498/500 Iteration: 4485 Train loss: 0.051531 Train acc: 0.983333\n",
      "Epoch: 498/500 Iteration: 4490 Train loss: 0.073554 Train acc: 0.981667\n",
      "Epoch: 499/500 Iteration: 4495 Train loss: 0.036250 Train acc: 0.991667\n",
      "Epoch: 499/500 Iteration: 4500 Train loss: 0.074456 Train acc: 0.985000\n",
      "Epoch: 499/500 Iteration: 4500 Validation loss: 0.427676 Validation acc: 0.902500\n"
     ]
    }
   ],
   "source": [
    "validation_acc = []\n",
    "validation_loss = []\n",
    "\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        # Initialize \n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        # Loop over batches\n",
    "        for x,y in get_batches(X_tr, y_tr, batch_size):\n",
    "            \n",
    "            # Feed dictionary\n",
    "            feed = {inputs_ : x, labels_ : y, keep_prob_ : 0.5, \n",
    "                    initial_state : state, learning_rate_ : learning_rate}\n",
    "            \n",
    "            loss, _ , state, acc = sess.run([cost, optimizer, final_state, accuracy], \n",
    "                                             feed_dict = feed)\n",
    "            train_acc.append(acc)\n",
    "            train_loss.append(loss)\n",
    "            \n",
    "            # Print at each 5 iters\n",
    "            if (iteration % 5 == 0):\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {:d}\".format(iteration),\n",
    "                      \"Train loss: {:6f}\".format(loss),\n",
    "                      \"Train acc: {:.6f}\".format(acc))\n",
    "            \n",
    "            # Compute validation loss at every 25 iterations\n",
    "            if (iteration%25 == 0):\n",
    "                \n",
    "                # Initiate for validation set\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                \n",
    "                val_acc_ = []\n",
    "                val_loss_ = []\n",
    "                for x_v, y_v in get_batches(X_vld, y_vld, batch_size):\n",
    "                    # Feed\n",
    "                    feed = {inputs_ : x_v, labels_ : y_v, keep_prob_ : 1.0, initial_state : val_state}\n",
    "                    \n",
    "                    # Loss\n",
    "                    loss_v, state_v, acc_v = sess.run([cost, final_state, accuracy], feed_dict = feed)\n",
    "                    \n",
    "                    val_acc_.append(acc_v)\n",
    "                    val_loss_.append(loss_v)\n",
    "                \n",
    "                # Print info\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {:d}\".format(iteration),\n",
    "                      \"Validation loss: {:6f}\".format(np.mean(val_loss_)),\n",
    "                      \"Validation acc: {:.6f}\".format(np.mean(val_acc_)))\n",
    "                \n",
    "                # Store\n",
    "                validation_acc.append(np.mean(val_acc_))\n",
    "                validation_loss.append(np.mean(val_loss_))\n",
    "            \n",
    "            # Iterate \n",
    "            iteration += 1\n",
    "    \n",
    "    saver.save(sess,\"checkpoints/eeg.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAF3CAYAAAC2bHyQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff73da91cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training and test loss\n",
    "t = np.arange(iteration-1)\n",
    "\n",
    "plt.figure(figsize = (6,6))\n",
    "plt.plot(t, np.array(train_loss), 'r-', t[t % 25 == 0], np.array(validation_loss), 'b*')\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAF3CAYAAABKeVdaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKSmIiEja/wc9SD3n+1rpNwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff6ba441630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Accuracies\n",
    "plt.figure(figsize = (6,6))\n",
    "\n",
    "plt.plot(t, np.array(train_acc), 'r-', t[t % 25 == 0], validation_acc, 'b*')\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"Accuray\")\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.947778\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # Restore\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    \n",
    "    for x_t, y_t in get_batches(X_test, y_test, batch_size):\n",
    "        feed = {inputs_: x_t,\n",
    "                labels_: y_t,\n",
    "                keep_prob_: 1,\n",
    "                initial_state: test_state}\n",
    "        \n",
    "        batch_acc, test_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.6f}\".format(np.mean(test_acc)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

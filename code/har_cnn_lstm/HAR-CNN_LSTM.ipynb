{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# HAR CNN + LSTM training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import os\n",
    "from utils.utilities import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, labels_train, list_ch_train = read_data(data_path=\"./data/\", split=\"train\") # train\n",
    "X_test, labels_test, list_ch_test = read_data(data_path=\"./data/\", split=\"train\") # test\n",
    "\n",
    "assert list_ch_train == list_ch_test, \"Mistmatch in channels!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test = standardize(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_tr, X_vld, lab_tr, lab_vld = train_test_split(X_train, labels_train, test_size = 0.2,\n",
    "                                                stratify = labels_train, random_state = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "One-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_tr = one_hot(lab_tr)\n",
    "y_vld = one_hot(lab_vld)\n",
    "y_test = one_hot(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lstm_size = 36         # 2 times the amount of channels\n",
    "lstm_layers = 2        # Number of layers\n",
    "batch_size = 600       # Batch size\n",
    "seq_len = 128          # Number of steps\n",
    "learning_rate = 0.0005  # Learning rate (default is 0.001)\n",
    "epochs = 500\n",
    "\n",
    "# Fixed\n",
    "n_classes = 6\n",
    "n_channels = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Construct the graph\n",
    "Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "# Construct placeholders\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.float32, [None, seq_len, n_channels], name = 'inputs')\n",
    "    labels_ = tf.placeholder(tf.float32, [None, n_classes], name = 'labels')\n",
    "    keep_prob_ = tf.placeholder(tf.float32, name = 'keep')\n",
    "    learning_rate_ = tf.placeholder(tf.float32, name = 'learning_rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Build Convolutional Layer(s)\n",
    "\n",
    "Questions: \n",
    "* Should we use a different activation? Like tf.nn.tanh?\n",
    "* Should we use pooling? average or max?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Convolutional layers\n",
    "with graph.as_default():\n",
    "    # (batch, 128, 9) --> (batch, 128, 18)\n",
    "    conv1 = tf.layers.conv1d(inputs=inputs_, filters=18, kernel_size=2, strides=1, \n",
    "                             padding='same', activation = tf.nn.relu)\n",
    "    n_ch = n_channels *2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, pass to LSTM cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    # Construct the LSTM inputs and LSTM cells\n",
    "    lstm_in = tf.transpose(conv1, [1,0,2]) # reshape into (seq_len, batch, channels)\n",
    "    lstm_in = tf.reshape(lstm_in, [-1, n_ch]) # Now (seq_len*N, n_channels)\n",
    "    \n",
    "    # To cells\n",
    "    lstm_in = tf.layers.dense(lstm_in, lstm_size, activation=tf.nn.relu) # or tf.nn.relu, tf.nn.sigmoid, tf.nn.tanh?\n",
    "    \n",
    "    # Open up the tensor into a list of seq_len pieces\n",
    "    lstm_in = tf.split(lstm_in, seq_len, 0)\n",
    "    \n",
    "    # Add LSTM layers\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob_)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Define forward pass and cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    outputs, final_state = tf.contrib.rnn.static_rnn(cell, lstm_in, dtype=tf.float32,\n",
    "                                                     initial_state = initial_state)\n",
    "    \n",
    "    # We only need the last output tensor to pass into a classifier\n",
    "    logits = tf.layers.dense(outputs[-1], n_classes, name='logits')\n",
    "    \n",
    "    # Cost function and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_))\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate_).minimize(cost) # No grad clipping\n",
    "    \n",
    "    # Grad clipping\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate_)\n",
    "\n",
    "    gradients = train_op.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients]\n",
    "    optimizer = train_op.apply_gradients(capped_gradients)\n",
    "    \n",
    "    # Accuracy\n",
    "    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if (os.path.exists('checkpoints-crnn') == False):\n",
    "    !mkdir checkpoints-crnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/500 Iteration: 5 Train loss: 1.781484 Train acc: 0.180000\n",
      "Epoch: 1/500 Iteration: 10 Train loss: 1.782946 Train acc: 0.211667\n",
      "Epoch: 1/500 Iteration: 15 Train loss: 1.771402 Train acc: 0.218333\n",
      "Epoch: 2/500 Iteration: 20 Train loss: 1.757840 Train acc: 0.253333\n",
      "Epoch: 2/500 Iteration: 25 Train loss: 1.754544 Train acc: 0.243333\n",
      "Epoch: 2/500 Iteration: 25 Validation loss: 1.750391 Validation acc: 0.252500\n",
      "Epoch: 3/500 Iteration: 30 Train loss: 1.745703 Train acc: 0.221667\n",
      "Epoch: 3/500 Iteration: 35 Train loss: 1.695912 Train acc: 0.255000\n",
      "Epoch: 4/500 Iteration: 40 Train loss: 1.643995 Train acc: 0.281667\n",
      "Epoch: 4/500 Iteration: 45 Train loss: 1.606355 Train acc: 0.335000\n",
      "Epoch: 5/500 Iteration: 50 Train loss: 1.558417 Train acc: 0.353333\n",
      "Epoch: 5/500 Iteration: 50 Validation loss: 1.528886 Validation acc: 0.363333\n",
      "Epoch: 6/500 Iteration: 55 Train loss: 1.504717 Train acc: 0.370000\n",
      "Epoch: 6/500 Iteration: 60 Train loss: 1.413111 Train acc: 0.416667\n",
      "Epoch: 7/500 Iteration: 65 Train loss: 1.428666 Train acc: 0.381667\n",
      "Epoch: 7/500 Iteration: 70 Train loss: 1.374544 Train acc: 0.446667\n",
      "Epoch: 8/500 Iteration: 75 Train loss: 1.376252 Train acc: 0.423333\n",
      "Epoch: 8/500 Iteration: 75 Validation loss: 1.278552 Validation acc: 0.487500\n",
      "Epoch: 8/500 Iteration: 80 Train loss: 1.302475 Train acc: 0.481667\n",
      "Epoch: 9/500 Iteration: 85 Train loss: 1.258461 Train acc: 0.495000\n",
      "Epoch: 9/500 Iteration: 90 Train loss: 1.228469 Train acc: 0.520000\n",
      "Epoch: 10/500 Iteration: 95 Train loss: 1.217640 Train acc: 0.546667\n",
      "Epoch: 11/500 Iteration: 100 Train loss: 1.131598 Train acc: 0.551667\n",
      "Epoch: 11/500 Iteration: 100 Validation loss: 1.115273 Validation acc: 0.592500\n",
      "Epoch: 11/500 Iteration: 105 Train loss: 1.134572 Train acc: 0.521667\n",
      "Epoch: 12/500 Iteration: 110 Train loss: 1.111442 Train acc: 0.561667\n",
      "Epoch: 12/500 Iteration: 115 Train loss: 1.105901 Train acc: 0.555000\n",
      "Epoch: 13/500 Iteration: 120 Train loss: 1.031167 Train acc: 0.598333\n",
      "Epoch: 13/500 Iteration: 125 Train loss: 1.041766 Train acc: 0.595000\n",
      "Epoch: 13/500 Iteration: 125 Validation loss: 0.984697 Validation acc: 0.610000\n",
      "Epoch: 14/500 Iteration: 130 Train loss: 0.993013 Train acc: 0.606667\n",
      "Epoch: 14/500 Iteration: 135 Train loss: 0.961765 Train acc: 0.623333\n",
      "Epoch: 15/500 Iteration: 140 Train loss: 0.982203 Train acc: 0.625000\n",
      "Epoch: 16/500 Iteration: 145 Train loss: 0.905634 Train acc: 0.643333\n",
      "Epoch: 16/500 Iteration: 150 Train loss: 0.919746 Train acc: 0.621667\n",
      "Epoch: 16/500 Iteration: 150 Validation loss: 0.879875 Validation acc: 0.640833\n",
      "Epoch: 17/500 Iteration: 155 Train loss: 0.885103 Train acc: 0.626667\n",
      "Epoch: 17/500 Iteration: 160 Train loss: 0.869108 Train acc: 0.618333\n",
      "Epoch: 18/500 Iteration: 165 Train loss: 0.940528 Train acc: 0.623333\n",
      "Epoch: 18/500 Iteration: 170 Train loss: 0.886188 Train acc: 0.635000\n",
      "Epoch: 19/500 Iteration: 175 Train loss: 0.844387 Train acc: 0.666667\n",
      "Epoch: 19/500 Iteration: 175 Validation loss: 0.802261 Validation acc: 0.648333\n",
      "Epoch: 19/500 Iteration: 180 Train loss: 0.820399 Train acc: 0.666667\n",
      "Epoch: 20/500 Iteration: 185 Train loss: 0.865050 Train acc: 0.646667\n",
      "Epoch: 21/500 Iteration: 190 Train loss: 0.786258 Train acc: 0.655000\n",
      "Epoch: 21/500 Iteration: 195 Train loss: 0.794591 Train acc: 0.663333\n",
      "Epoch: 22/500 Iteration: 200 Train loss: 0.749244 Train acc: 0.683333\n",
      "Epoch: 22/500 Iteration: 200 Validation loss: 0.731043 Validation acc: 0.678333\n",
      "Epoch: 22/500 Iteration: 205 Train loss: 0.736548 Train acc: 0.696667\n",
      "Epoch: 23/500 Iteration: 210 Train loss: 0.701898 Train acc: 0.691667\n",
      "Epoch: 23/500 Iteration: 215 Train loss: 0.769133 Train acc: 0.655000\n",
      "Epoch: 24/500 Iteration: 220 Train loss: 0.731970 Train acc: 0.691667\n",
      "Epoch: 24/500 Iteration: 225 Train loss: 0.708367 Train acc: 0.681667\n",
      "Epoch: 24/500 Iteration: 225 Validation loss: 0.692759 Validation acc: 0.693333\n",
      "Epoch: 25/500 Iteration: 230 Train loss: 0.771254 Train acc: 0.683333\n",
      "Epoch: 26/500 Iteration: 235 Train loss: 0.695128 Train acc: 0.698333\n",
      "Epoch: 26/500 Iteration: 240 Train loss: 0.675349 Train acc: 0.720000\n",
      "Epoch: 27/500 Iteration: 245 Train loss: 0.669383 Train acc: 0.720000\n",
      "Epoch: 27/500 Iteration: 250 Train loss: 0.674260 Train acc: 0.688333\n",
      "Epoch: 27/500 Iteration: 250 Validation loss: 0.663778 Validation acc: 0.700833\n",
      "Epoch: 28/500 Iteration: 255 Train loss: 0.674341 Train acc: 0.701667\n",
      "Epoch: 28/500 Iteration: 260 Train loss: 0.700384 Train acc: 0.686667\n",
      "Epoch: 29/500 Iteration: 265 Train loss: 0.683774 Train acc: 0.703333\n",
      "Epoch: 29/500 Iteration: 270 Train loss: 0.666805 Train acc: 0.711667\n",
      "Epoch: 30/500 Iteration: 275 Train loss: 0.717349 Train acc: 0.713333\n",
      "Epoch: 30/500 Iteration: 275 Validation loss: 0.643996 Validation acc: 0.710833\n",
      "Epoch: 31/500 Iteration: 280 Train loss: 0.645938 Train acc: 0.720000\n",
      "Epoch: 31/500 Iteration: 285 Train loss: 0.707171 Train acc: 0.691667\n",
      "Epoch: 32/500 Iteration: 290 Train loss: 0.827390 Train acc: 0.668333\n",
      "Epoch: 32/500 Iteration: 295 Train loss: 0.704561 Train acc: 0.685000\n",
      "Epoch: 33/500 Iteration: 300 Train loss: 0.665948 Train acc: 0.733333\n",
      "Epoch: 33/500 Iteration: 300 Validation loss: 0.644812 Validation acc: 0.708333\n",
      "Epoch: 33/500 Iteration: 305 Train loss: 0.687012 Train acc: 0.738333\n",
      "Epoch: 34/500 Iteration: 310 Train loss: 0.626039 Train acc: 0.730000\n",
      "Epoch: 34/500 Iteration: 315 Train loss: 0.625336 Train acc: 0.736667\n",
      "Epoch: 35/500 Iteration: 320 Train loss: 0.689874 Train acc: 0.711667\n",
      "Epoch: 36/500 Iteration: 325 Train loss: 0.601727 Train acc: 0.741667\n",
      "Epoch: 36/500 Iteration: 325 Validation loss: 0.614717 Validation acc: 0.726667\n",
      "Epoch: 36/500 Iteration: 330 Train loss: 0.663945 Train acc: 0.731667\n",
      "Epoch: 37/500 Iteration: 335 Train loss: 0.602683 Train acc: 0.746667\n",
      "Epoch: 37/500 Iteration: 340 Train loss: 0.657569 Train acc: 0.715000\n",
      "Epoch: 38/500 Iteration: 345 Train loss: 0.592835 Train acc: 0.750000\n",
      "Epoch: 38/500 Iteration: 350 Train loss: 0.631893 Train acc: 0.716667\n",
      "Epoch: 38/500 Iteration: 350 Validation loss: 0.571782 Validation acc: 0.749167\n",
      "Epoch: 39/500 Iteration: 355 Train loss: 0.594702 Train acc: 0.751667\n",
      "Epoch: 39/500 Iteration: 360 Train loss: 0.580837 Train acc: 0.735000\n",
      "Epoch: 40/500 Iteration: 365 Train loss: 0.634857 Train acc: 0.718333\n",
      "Epoch: 41/500 Iteration: 370 Train loss: 0.556960 Train acc: 0.776667\n",
      "Epoch: 41/500 Iteration: 375 Train loss: 0.580788 Train acc: 0.765000\n",
      "Epoch: 41/500 Iteration: 375 Validation loss: 0.531978 Validation acc: 0.766667\n",
      "Epoch: 42/500 Iteration: 380 Train loss: 0.576243 Train acc: 0.760000\n",
      "Epoch: 42/500 Iteration: 385 Train loss: 0.572867 Train acc: 0.758333\n",
      "Epoch: 43/500 Iteration: 390 Train loss: 0.554854 Train acc: 0.773333\n",
      "Epoch: 43/500 Iteration: 395 Train loss: 0.592762 Train acc: 0.760000\n",
      "Epoch: 44/500 Iteration: 400 Train loss: 0.529723 Train acc: 0.785000\n",
      "Epoch: 44/500 Iteration: 400 Validation loss: 0.528186 Validation acc: 0.760833\n",
      "Epoch: 44/500 Iteration: 405 Train loss: 0.532659 Train acc: 0.780000\n",
      "Epoch: 45/500 Iteration: 410 Train loss: 0.580788 Train acc: 0.750000\n",
      "Epoch: 46/500 Iteration: 415 Train loss: 0.527624 Train acc: 0.783333\n",
      "Epoch: 46/500 Iteration: 420 Train loss: 0.545829 Train acc: 0.798333\n",
      "Epoch: 47/500 Iteration: 425 Train loss: 0.504303 Train acc: 0.798333\n",
      "Epoch: 47/500 Iteration: 425 Validation loss: 0.524443 Validation acc: 0.783333\n",
      "Epoch: 47/500 Iteration: 430 Train loss: 0.538834 Train acc: 0.761667\n",
      "Epoch: 48/500 Iteration: 435 Train loss: 0.498698 Train acc: 0.805000\n",
      "Epoch: 48/500 Iteration: 440 Train loss: 0.556856 Train acc: 0.768333\n",
      "Epoch: 49/500 Iteration: 445 Train loss: 0.507083 Train acc: 0.796667\n",
      "Epoch: 49/500 Iteration: 450 Train loss: 0.524326 Train acc: 0.791667\n",
      "Epoch: 49/500 Iteration: 450 Validation loss: 0.503181 Validation acc: 0.779167\n",
      "Epoch: 50/500 Iteration: 455 Train loss: 0.554826 Train acc: 0.786667\n",
      "Epoch: 51/500 Iteration: 460 Train loss: 0.475576 Train acc: 0.823333\n",
      "Epoch: 51/500 Iteration: 465 Train loss: 0.529583 Train acc: 0.786667\n",
      "Epoch: 52/500 Iteration: 470 Train loss: 0.500613 Train acc: 0.808333\n",
      "Epoch: 52/500 Iteration: 475 Train loss: 0.507155 Train acc: 0.793333\n",
      "Epoch: 52/500 Iteration: 475 Validation loss: 0.466891 Validation acc: 0.804167\n",
      "Epoch: 53/500 Iteration: 480 Train loss: 0.498840 Train acc: 0.805000\n",
      "Epoch: 53/500 Iteration: 485 Train loss: 0.525524 Train acc: 0.793333\n",
      "Epoch: 54/500 Iteration: 490 Train loss: 0.468106 Train acc: 0.808333\n",
      "Epoch: 54/500 Iteration: 495 Train loss: 0.471300 Train acc: 0.808333\n",
      "Epoch: 55/500 Iteration: 500 Train loss: 0.517086 Train acc: 0.795000\n",
      "Epoch: 55/500 Iteration: 500 Validation loss: 0.456061 Validation acc: 0.814167\n",
      "Epoch: 56/500 Iteration: 505 Train loss: 0.451415 Train acc: 0.823333\n",
      "Epoch: 56/500 Iteration: 510 Train loss: 0.475799 Train acc: 0.820000\n",
      "Epoch: 57/500 Iteration: 515 Train loss: 0.483364 Train acc: 0.806667\n",
      "Epoch: 57/500 Iteration: 520 Train loss: 0.480555 Train acc: 0.816667\n",
      "Epoch: 58/500 Iteration: 525 Train loss: 0.460646 Train acc: 0.823333\n",
      "Epoch: 58/500 Iteration: 525 Validation loss: 0.452251 Validation acc: 0.805833\n",
      "Epoch: 58/500 Iteration: 530 Train loss: 0.501358 Train acc: 0.796667\n",
      "Epoch: 59/500 Iteration: 535 Train loss: 0.433424 Train acc: 0.843333\n",
      "Epoch: 59/500 Iteration: 540 Train loss: 0.454517 Train acc: 0.826667\n",
      "Epoch: 60/500 Iteration: 545 Train loss: 0.518326 Train acc: 0.781667\n",
      "Epoch: 61/500 Iteration: 550 Train loss: 0.448716 Train acc: 0.810000\n",
      "Epoch: 61/500 Iteration: 550 Validation loss: 0.439875 Validation acc: 0.816667\n",
      "Epoch: 61/500 Iteration: 555 Train loss: 0.452500 Train acc: 0.831667\n",
      "Epoch: 62/500 Iteration: 560 Train loss: 0.478288 Train acc: 0.810000\n",
      "Epoch: 62/500 Iteration: 565 Train loss: 0.491022 Train acc: 0.795000\n",
      "Epoch: 63/500 Iteration: 570 Train loss: 0.442993 Train acc: 0.830000\n",
      "Epoch: 63/500 Iteration: 575 Train loss: 0.491780 Train acc: 0.806667\n",
      "Epoch: 63/500 Iteration: 575 Validation loss: 0.449141 Validation acc: 0.800833\n",
      "Epoch: 64/500 Iteration: 580 Train loss: 0.417828 Train acc: 0.846667\n",
      "Epoch: 64/500 Iteration: 585 Train loss: 0.413246 Train acc: 0.843333\n",
      "Epoch: 65/500 Iteration: 590 Train loss: 0.460451 Train acc: 0.826667\n",
      "Epoch: 66/500 Iteration: 595 Train loss: 0.442075 Train acc: 0.840000\n",
      "Epoch: 66/500 Iteration: 600 Train loss: 0.452685 Train acc: 0.820000\n",
      "Epoch: 66/500 Iteration: 600 Validation loss: 0.416037 Validation acc: 0.823333\n",
      "Epoch: 67/500 Iteration: 605 Train loss: 0.471168 Train acc: 0.800000\n",
      "Epoch: 67/500 Iteration: 610 Train loss: 0.512679 Train acc: 0.803333\n",
      "Epoch: 68/500 Iteration: 615 Train loss: 0.450283 Train acc: 0.828333\n",
      "Epoch: 68/500 Iteration: 620 Train loss: 0.497925 Train acc: 0.803333\n",
      "Epoch: 69/500 Iteration: 625 Train loss: 0.483565 Train acc: 0.821667\n",
      "Epoch: 69/500 Iteration: 625 Validation loss: 0.422369 Validation acc: 0.818333\n",
      "Epoch: 69/500 Iteration: 630 Train loss: 0.416775 Train acc: 0.838333\n",
      "Epoch: 70/500 Iteration: 635 Train loss: 0.429810 Train acc: 0.818333\n",
      "Epoch: 71/500 Iteration: 640 Train loss: 0.400229 Train acc: 0.838333\n",
      "Epoch: 71/500 Iteration: 645 Train loss: 0.440640 Train acc: 0.831667\n",
      "Epoch: 72/500 Iteration: 650 Train loss: 0.445075 Train acc: 0.815000\n",
      "Epoch: 72/500 Iteration: 650 Validation loss: 0.403405 Validation acc: 0.820833\n",
      "Epoch: 72/500 Iteration: 655 Train loss: 0.428560 Train acc: 0.826667\n",
      "Epoch: 73/500 Iteration: 660 Train loss: 0.416062 Train acc: 0.830000\n",
      "Epoch: 73/500 Iteration: 665 Train loss: 0.438903 Train acc: 0.841667\n",
      "Epoch: 74/500 Iteration: 670 Train loss: 0.374968 Train acc: 0.855000\n",
      "Epoch: 74/500 Iteration: 675 Train loss: 0.386539 Train acc: 0.860000\n",
      "Epoch: 74/500 Iteration: 675 Validation loss: 0.394500 Validation acc: 0.835000\n",
      "Epoch: 75/500 Iteration: 680 Train loss: 0.398311 Train acc: 0.843333\n",
      "Epoch: 76/500 Iteration: 685 Train loss: 0.382185 Train acc: 0.856667\n",
      "Epoch: 76/500 Iteration: 690 Train loss: 0.385387 Train acc: 0.858333\n",
      "Epoch: 77/500 Iteration: 695 Train loss: 0.405428 Train acc: 0.830000\n",
      "Epoch: 77/500 Iteration: 700 Train loss: 0.401833 Train acc: 0.841667\n",
      "Epoch: 77/500 Iteration: 700 Validation loss: 0.400300 Validation acc: 0.833333\n",
      "Epoch: 78/500 Iteration: 705 Train loss: 0.410501 Train acc: 0.845000\n",
      "Epoch: 78/500 Iteration: 710 Train loss: 0.445911 Train acc: 0.830000\n",
      "Epoch: 79/500 Iteration: 715 Train loss: 0.354460 Train acc: 0.876667\n",
      "Epoch: 79/500 Iteration: 720 Train loss: 0.363182 Train acc: 0.871667\n",
      "Epoch: 80/500 Iteration: 725 Train loss: 0.385185 Train acc: 0.866667\n",
      "Epoch: 80/500 Iteration: 725 Validation loss: 0.396468 Validation acc: 0.833333\n",
      "Epoch: 81/500 Iteration: 730 Train loss: 0.362092 Train acc: 0.870000\n",
      "Epoch: 81/500 Iteration: 735 Train loss: 0.390200 Train acc: 0.853333\n",
      "Epoch: 82/500 Iteration: 740 Train loss: 0.429724 Train acc: 0.828333\n",
      "Epoch: 82/500 Iteration: 745 Train loss: 0.396698 Train acc: 0.846667\n",
      "Epoch: 83/500 Iteration: 750 Train loss: 0.401712 Train acc: 0.850000\n",
      "Epoch: 83/500 Iteration: 750 Validation loss: 0.386778 Validation acc: 0.842500\n",
      "Epoch: 83/500 Iteration: 755 Train loss: 0.417576 Train acc: 0.835000\n",
      "Epoch: 84/500 Iteration: 760 Train loss: 0.349307 Train acc: 0.870000\n",
      "Epoch: 84/500 Iteration: 765 Train loss: 0.351497 Train acc: 0.886667\n",
      "Epoch: 85/500 Iteration: 770 Train loss: 0.370568 Train acc: 0.858333\n",
      "Epoch: 86/500 Iteration: 775 Train loss: 0.349397 Train acc: 0.881667\n",
      "Epoch: 86/500 Iteration: 775 Validation loss: 0.376437 Validation acc: 0.842500\n",
      "Epoch: 86/500 Iteration: 780 Train loss: 0.365356 Train acc: 0.876667\n",
      "Epoch: 87/500 Iteration: 785 Train loss: 0.400161 Train acc: 0.843333\n",
      "Epoch: 87/500 Iteration: 790 Train loss: 0.402156 Train acc: 0.855000\n",
      "Epoch: 88/500 Iteration: 795 Train loss: 0.381090 Train acc: 0.856667\n",
      "Epoch: 88/500 Iteration: 800 Train loss: 0.390328 Train acc: 0.851667\n",
      "Epoch: 88/500 Iteration: 800 Validation loss: 0.369970 Validation acc: 0.845833\n",
      "Epoch: 89/500 Iteration: 805 Train loss: 0.338965 Train acc: 0.886667\n",
      "Epoch: 89/500 Iteration: 810 Train loss: 0.349594 Train acc: 0.865000\n",
      "Epoch: 90/500 Iteration: 815 Train loss: 0.365884 Train acc: 0.861667\n",
      "Epoch: 91/500 Iteration: 820 Train loss: 0.337214 Train acc: 0.880000\n",
      "Epoch: 91/500 Iteration: 825 Train loss: 0.350730 Train acc: 0.880000\n",
      "Epoch: 91/500 Iteration: 825 Validation loss: 0.382971 Validation acc: 0.849167\n",
      "Epoch: 92/500 Iteration: 830 Train loss: 0.365940 Train acc: 0.851667\n",
      "Epoch: 92/500 Iteration: 835 Train loss: 0.379462 Train acc: 0.871667\n",
      "Epoch: 93/500 Iteration: 840 Train loss: 0.369650 Train acc: 0.870000\n",
      "Epoch: 93/500 Iteration: 845 Train loss: 0.419312 Train acc: 0.836667\n",
      "Epoch: 94/500 Iteration: 850 Train loss: 0.325353 Train acc: 0.878333\n",
      "Epoch: 94/500 Iteration: 850 Validation loss: 0.394608 Validation acc: 0.831667\n",
      "Epoch: 94/500 Iteration: 855 Train loss: 0.414779 Train acc: 0.846667\n",
      "Epoch: 95/500 Iteration: 860 Train loss: 0.347139 Train acc: 0.875000\n",
      "Epoch: 96/500 Iteration: 865 Train loss: 0.317351 Train acc: 0.896667\n",
      "Epoch: 96/500 Iteration: 870 Train loss: 0.354583 Train acc: 0.878333\n",
      "Epoch: 97/500 Iteration: 875 Train loss: 0.384023 Train acc: 0.841667\n",
      "Epoch: 97/500 Iteration: 875 Validation loss: 0.403517 Validation acc: 0.838333\n",
      "Epoch: 97/500 Iteration: 880 Train loss: 0.435891 Train acc: 0.825000\n",
      "Epoch: 98/500 Iteration: 885 Train loss: 0.374739 Train acc: 0.866667\n",
      "Epoch: 98/500 Iteration: 890 Train loss: 0.396442 Train acc: 0.856667\n",
      "Epoch: 99/500 Iteration: 895 Train loss: 0.319743 Train acc: 0.888333\n",
      "Epoch: 99/500 Iteration: 900 Train loss: 0.329660 Train acc: 0.875000\n",
      "Epoch: 99/500 Iteration: 900 Validation loss: 0.372034 Validation acc: 0.852500\n",
      "Epoch: 100/500 Iteration: 905 Train loss: 0.334932 Train acc: 0.873333\n",
      "Epoch: 101/500 Iteration: 910 Train loss: 0.317137 Train acc: 0.888333\n",
      "Epoch: 101/500 Iteration: 915 Train loss: 0.349662 Train acc: 0.871667\n",
      "Epoch: 102/500 Iteration: 920 Train loss: 0.378022 Train acc: 0.846667\n",
      "Epoch: 102/500 Iteration: 925 Train loss: 0.360160 Train acc: 0.870000\n",
      "Epoch: 102/500 Iteration: 925 Validation loss: 0.363194 Validation acc: 0.857500\n",
      "Epoch: 103/500 Iteration: 930 Train loss: 0.333554 Train acc: 0.886667\n",
      "Epoch: 103/500 Iteration: 935 Train loss: 0.408043 Train acc: 0.850000\n",
      "Epoch: 104/500 Iteration: 940 Train loss: 0.317971 Train acc: 0.866667\n",
      "Epoch: 104/500 Iteration: 945 Train loss: 0.328239 Train acc: 0.878333\n",
      "Epoch: 105/500 Iteration: 950 Train loss: 0.348658 Train acc: 0.875000\n",
      "Epoch: 105/500 Iteration: 950 Validation loss: 0.384706 Validation acc: 0.843333\n",
      "Epoch: 106/500 Iteration: 955 Train loss: 0.341670 Train acc: 0.871667\n",
      "Epoch: 106/500 Iteration: 960 Train loss: 0.355566 Train acc: 0.866667\n",
      "Epoch: 107/500 Iteration: 965 Train loss: 0.394367 Train acc: 0.836667\n",
      "Epoch: 107/500 Iteration: 970 Train loss: 0.330188 Train acc: 0.876667\n",
      "Epoch: 108/500 Iteration: 975 Train loss: 0.329955 Train acc: 0.883333\n",
      "Epoch: 108/500 Iteration: 975 Validation loss: 0.357247 Validation acc: 0.857500\n",
      "Epoch: 108/500 Iteration: 980 Train loss: 0.370860 Train acc: 0.860000\n",
      "Epoch: 109/500 Iteration: 985 Train loss: 0.317776 Train acc: 0.888333\n",
      "Epoch: 109/500 Iteration: 990 Train loss: 0.330577 Train acc: 0.885000\n",
      "Epoch: 110/500 Iteration: 995 Train loss: 0.310815 Train acc: 0.878333\n",
      "Epoch: 111/500 Iteration: 1000 Train loss: 0.302290 Train acc: 0.895000\n",
      "Epoch: 111/500 Iteration: 1000 Validation loss: 0.357955 Validation acc: 0.859167\n",
      "Epoch: 111/500 Iteration: 1005 Train loss: 0.357667 Train acc: 0.878333\n",
      "Epoch: 112/500 Iteration: 1010 Train loss: 0.382457 Train acc: 0.848333\n",
      "Epoch: 112/500 Iteration: 1015 Train loss: 0.376653 Train acc: 0.841667\n",
      "Epoch: 113/500 Iteration: 1020 Train loss: 0.313449 Train acc: 0.885000\n",
      "Epoch: 113/500 Iteration: 1025 Train loss: 0.354057 Train acc: 0.860000\n",
      "Epoch: 113/500 Iteration: 1025 Validation loss: 0.342519 Validation acc: 0.868333\n",
      "Epoch: 114/500 Iteration: 1030 Train loss: 0.290256 Train acc: 0.891667\n",
      "Epoch: 114/500 Iteration: 1035 Train loss: 0.333821 Train acc: 0.875000\n",
      "Epoch: 115/500 Iteration: 1040 Train loss: 0.320572 Train acc: 0.871667\n",
      "Epoch: 116/500 Iteration: 1045 Train loss: 0.324844 Train acc: 0.883333\n",
      "Epoch: 116/500 Iteration: 1050 Train loss: 0.319269 Train acc: 0.891667\n",
      "Epoch: 116/500 Iteration: 1050 Validation loss: 0.340452 Validation acc: 0.870833\n",
      "Epoch: 117/500 Iteration: 1055 Train loss: 0.332356 Train acc: 0.878333\n",
      "Epoch: 117/500 Iteration: 1060 Train loss: 0.315452 Train acc: 0.890000\n",
      "Epoch: 118/500 Iteration: 1065 Train loss: 0.314581 Train acc: 0.891667\n",
      "Epoch: 118/500 Iteration: 1070 Train loss: 0.358664 Train acc: 0.880000\n",
      "Epoch: 119/500 Iteration: 1075 Train loss: 0.292103 Train acc: 0.896667\n",
      "Epoch: 119/500 Iteration: 1075 Validation loss: 0.385206 Validation acc: 0.846667\n",
      "Epoch: 119/500 Iteration: 1080 Train loss: 0.322890 Train acc: 0.873333\n",
      "Epoch: 120/500 Iteration: 1085 Train loss: 0.382930 Train acc: 0.856667\n",
      "Epoch: 121/500 Iteration: 1090 Train loss: 0.378312 Train acc: 0.850000\n",
      "Epoch: 121/500 Iteration: 1095 Train loss: 0.333652 Train acc: 0.880000\n",
      "Epoch: 122/500 Iteration: 1100 Train loss: 0.324521 Train acc: 0.880000\n",
      "Epoch: 122/500 Iteration: 1100 Validation loss: 0.342350 Validation acc: 0.866667\n",
      "Epoch: 122/500 Iteration: 1105 Train loss: 0.322668 Train acc: 0.886667\n",
      "Epoch: 123/500 Iteration: 1110 Train loss: 0.329279 Train acc: 0.898333\n",
      "Epoch: 123/500 Iteration: 1115 Train loss: 0.344915 Train acc: 0.876667\n",
      "Epoch: 124/500 Iteration: 1120 Train loss: 0.301722 Train acc: 0.891667\n",
      "Epoch: 124/500 Iteration: 1125 Train loss: 0.309570 Train acc: 0.890000\n",
      "Epoch: 124/500 Iteration: 1125 Validation loss: 0.339382 Validation acc: 0.870000\n",
      "Epoch: 125/500 Iteration: 1130 Train loss: 0.312002 Train acc: 0.883333\n",
      "Epoch: 126/500 Iteration: 1135 Train loss: 0.296673 Train acc: 0.901667\n",
      "Epoch: 126/500 Iteration: 1140 Train loss: 0.312070 Train acc: 0.886667\n",
      "Epoch: 127/500 Iteration: 1145 Train loss: 0.317295 Train acc: 0.866667\n",
      "Epoch: 127/500 Iteration: 1150 Train loss: 0.291429 Train acc: 0.905000\n",
      "Epoch: 127/500 Iteration: 1150 Validation loss: 0.339727 Validation acc: 0.873333\n",
      "Epoch: 128/500 Iteration: 1155 Train loss: 0.301720 Train acc: 0.898333\n",
      "Epoch: 128/500 Iteration: 1160 Train loss: 0.325891 Train acc: 0.883333\n",
      "Epoch: 129/500 Iteration: 1165 Train loss: 0.276080 Train acc: 0.900000\n",
      "Epoch: 129/500 Iteration: 1170 Train loss: 0.275012 Train acc: 0.898333\n",
      "Epoch: 130/500 Iteration: 1175 Train loss: 0.274700 Train acc: 0.905000\n",
      "Epoch: 130/500 Iteration: 1175 Validation loss: 0.343837 Validation acc: 0.870000\n",
      "Epoch: 131/500 Iteration: 1180 Train loss: 0.288185 Train acc: 0.910000\n",
      "Epoch: 131/500 Iteration: 1185 Train loss: 0.281290 Train acc: 0.900000\n",
      "Epoch: 132/500 Iteration: 1190 Train loss: 0.308941 Train acc: 0.875000\n",
      "Epoch: 132/500 Iteration: 1195 Train loss: 0.301795 Train acc: 0.885000\n",
      "Epoch: 133/500 Iteration: 1200 Train loss: 0.292278 Train acc: 0.893333\n",
      "Epoch: 133/500 Iteration: 1200 Validation loss: 0.354856 Validation acc: 0.865000\n",
      "Epoch: 133/500 Iteration: 1205 Train loss: 0.315643 Train acc: 0.901667\n",
      "Epoch: 134/500 Iteration: 1210 Train loss: 0.257950 Train acc: 0.910000\n",
      "Epoch: 134/500 Iteration: 1215 Train loss: 0.296693 Train acc: 0.895000\n",
      "Epoch: 135/500 Iteration: 1220 Train loss: 0.274044 Train acc: 0.906667\n",
      "Epoch: 136/500 Iteration: 1225 Train loss: 0.267591 Train acc: 0.898333\n",
      "Epoch: 136/500 Iteration: 1225 Validation loss: 0.346279 Validation acc: 0.867500\n",
      "Epoch: 136/500 Iteration: 1230 Train loss: 0.273116 Train acc: 0.908333\n",
      "Epoch: 137/500 Iteration: 1235 Train loss: 0.303759 Train acc: 0.893333\n",
      "Epoch: 137/500 Iteration: 1240 Train loss: 0.301069 Train acc: 0.883333\n",
      "Epoch: 138/500 Iteration: 1245 Train loss: 0.299695 Train acc: 0.890000\n",
      "Epoch: 138/500 Iteration: 1250 Train loss: 0.307509 Train acc: 0.893333\n",
      "Epoch: 138/500 Iteration: 1250 Validation loss: 0.370119 Validation acc: 0.855833\n",
      "Epoch: 139/500 Iteration: 1255 Train loss: 0.285132 Train acc: 0.895000\n",
      "Epoch: 139/500 Iteration: 1260 Train loss: 0.302528 Train acc: 0.893333\n",
      "Epoch: 140/500 Iteration: 1265 Train loss: 0.295324 Train acc: 0.891667\n",
      "Epoch: 141/500 Iteration: 1270 Train loss: 0.297054 Train acc: 0.891667\n",
      "Epoch: 141/500 Iteration: 1275 Train loss: 0.288097 Train acc: 0.890000\n",
      "Epoch: 141/500 Iteration: 1275 Validation loss: 0.350477 Validation acc: 0.862500\n",
      "Epoch: 142/500 Iteration: 1280 Train loss: 0.321613 Train acc: 0.883333\n",
      "Epoch: 142/500 Iteration: 1285 Train loss: 0.295770 Train acc: 0.891667\n",
      "Epoch: 143/500 Iteration: 1290 Train loss: 0.346457 Train acc: 0.878333\n",
      "Epoch: 143/500 Iteration: 1295 Train loss: 0.343633 Train acc: 0.873333\n",
      "Epoch: 144/500 Iteration: 1300 Train loss: 0.237113 Train acc: 0.916667\n",
      "Epoch: 144/500 Iteration: 1300 Validation loss: 0.338369 Validation acc: 0.875833\n",
      "Epoch: 144/500 Iteration: 1305 Train loss: 0.296694 Train acc: 0.891667\n",
      "Epoch: 145/500 Iteration: 1310 Train loss: 0.263430 Train acc: 0.910000\n",
      "Epoch: 146/500 Iteration: 1315 Train loss: 0.273537 Train acc: 0.901667\n",
      "Epoch: 146/500 Iteration: 1320 Train loss: 0.273012 Train acc: 0.906667\n",
      "Epoch: 147/500 Iteration: 1325 Train loss: 0.303887 Train acc: 0.876667\n",
      "Epoch: 147/500 Iteration: 1325 Validation loss: 0.335208 Validation acc: 0.870000\n",
      "Epoch: 147/500 Iteration: 1330 Train loss: 0.275948 Train acc: 0.895000\n",
      "Epoch: 148/500 Iteration: 1335 Train loss: 0.305383 Train acc: 0.893333\n",
      "Epoch: 148/500 Iteration: 1340 Train loss: 0.334275 Train acc: 0.885000\n",
      "Epoch: 149/500 Iteration: 1345 Train loss: 0.264492 Train acc: 0.905000\n",
      "Epoch: 149/500 Iteration: 1350 Train loss: 0.285102 Train acc: 0.906667\n",
      "Epoch: 149/500 Iteration: 1350 Validation loss: 0.333582 Validation acc: 0.877500\n",
      "Epoch: 150/500 Iteration: 1355 Train loss: 0.258899 Train acc: 0.903333\n",
      "Epoch: 151/500 Iteration: 1360 Train loss: 0.272942 Train acc: 0.901667\n",
      "Epoch: 151/500 Iteration: 1365 Train loss: 0.253741 Train acc: 0.913333\n",
      "Epoch: 152/500 Iteration: 1370 Train loss: 0.276835 Train acc: 0.898333\n",
      "Epoch: 152/500 Iteration: 1375 Train loss: 0.254928 Train acc: 0.906667\n",
      "Epoch: 152/500 Iteration: 1375 Validation loss: 0.327505 Validation acc: 0.876667\n",
      "Epoch: 153/500 Iteration: 1380 Train loss: 0.252323 Train acc: 0.911667\n",
      "Epoch: 153/500 Iteration: 1385 Train loss: 0.282316 Train acc: 0.910000\n",
      "Epoch: 154/500 Iteration: 1390 Train loss: 0.225090 Train acc: 0.935000\n",
      "Epoch: 154/500 Iteration: 1395 Train loss: 0.277168 Train acc: 0.906667\n",
      "Epoch: 155/500 Iteration: 1400 Train loss: 0.235284 Train acc: 0.920000\n",
      "Epoch: 155/500 Iteration: 1400 Validation loss: 0.340499 Validation acc: 0.876667\n",
      "Epoch: 156/500 Iteration: 1405 Train loss: 0.251963 Train acc: 0.901667\n",
      "Epoch: 156/500 Iteration: 1410 Train loss: 0.269922 Train acc: 0.911667\n",
      "Epoch: 157/500 Iteration: 1415 Train loss: 0.291465 Train acc: 0.891667\n",
      "Epoch: 157/500 Iteration: 1420 Train loss: 0.272932 Train acc: 0.896667\n",
      "Epoch: 158/500 Iteration: 1425 Train loss: 0.266779 Train acc: 0.911667\n",
      "Epoch: 158/500 Iteration: 1425 Validation loss: 0.343634 Validation acc: 0.873333\n",
      "Epoch: 158/500 Iteration: 1430 Train loss: 0.296707 Train acc: 0.900000\n",
      "Epoch: 159/500 Iteration: 1435 Train loss: 0.223403 Train acc: 0.920000\n",
      "Epoch: 159/500 Iteration: 1440 Train loss: 0.262201 Train acc: 0.913333\n",
      "Epoch: 160/500 Iteration: 1445 Train loss: 0.246386 Train acc: 0.915000\n",
      "Epoch: 161/500 Iteration: 1450 Train loss: 0.231955 Train acc: 0.923333\n",
      "Epoch: 161/500 Iteration: 1450 Validation loss: 0.313776 Validation acc: 0.879167\n",
      "Epoch: 161/500 Iteration: 1455 Train loss: 0.218607 Train acc: 0.926667\n",
      "Epoch: 162/500 Iteration: 1460 Train loss: 0.257339 Train acc: 0.908333\n",
      "Epoch: 162/500 Iteration: 1465 Train loss: 0.247254 Train acc: 0.895000\n",
      "Epoch: 163/500 Iteration: 1470 Train loss: 0.242924 Train acc: 0.913333\n",
      "Epoch: 163/500 Iteration: 1475 Train loss: 0.295430 Train acc: 0.893333\n",
      "Epoch: 163/500 Iteration: 1475 Validation loss: 0.328689 Validation acc: 0.885000\n",
      "Epoch: 164/500 Iteration: 1480 Train loss: 0.254809 Train acc: 0.911667\n",
      "Epoch: 164/500 Iteration: 1485 Train loss: 0.256883 Train acc: 0.913333\n",
      "Epoch: 165/500 Iteration: 1490 Train loss: 0.243315 Train acc: 0.913333\n",
      "Epoch: 166/500 Iteration: 1495 Train loss: 0.231891 Train acc: 0.923333\n",
      "Epoch: 166/500 Iteration: 1500 Train loss: 0.222286 Train acc: 0.936667\n",
      "Epoch: 166/500 Iteration: 1500 Validation loss: 0.339126 Validation acc: 0.880000\n",
      "Epoch: 167/500 Iteration: 1505 Train loss: 0.255378 Train acc: 0.906667\n",
      "Epoch: 167/500 Iteration: 1510 Train loss: 0.262289 Train acc: 0.908333\n",
      "Epoch: 168/500 Iteration: 1515 Train loss: 0.245675 Train acc: 0.920000\n",
      "Epoch: 168/500 Iteration: 1520 Train loss: 0.258295 Train acc: 0.910000\n",
      "Epoch: 169/500 Iteration: 1525 Train loss: 0.246443 Train acc: 0.911667\n",
      "Epoch: 169/500 Iteration: 1525 Validation loss: 0.382592 Validation acc: 0.861667\n",
      "Epoch: 169/500 Iteration: 1530 Train loss: 0.305017 Train acc: 0.890000\n",
      "Epoch: 170/500 Iteration: 1535 Train loss: 0.256278 Train acc: 0.896667\n",
      "Epoch: 171/500 Iteration: 1540 Train loss: 0.273353 Train acc: 0.896667\n",
      "Epoch: 171/500 Iteration: 1545 Train loss: 0.301295 Train acc: 0.905000\n",
      "Epoch: 172/500 Iteration: 1550 Train loss: 0.301215 Train acc: 0.881667\n",
      "Epoch: 172/500 Iteration: 1550 Validation loss: 0.349641 Validation acc: 0.875000\n",
      "Epoch: 172/500 Iteration: 1555 Train loss: 0.268459 Train acc: 0.911667\n",
      "Epoch: 173/500 Iteration: 1560 Train loss: 0.291349 Train acc: 0.901667\n",
      "Epoch: 173/500 Iteration: 1565 Train loss: 0.299987 Train acc: 0.895000\n",
      "Epoch: 174/500 Iteration: 1570 Train loss: 0.236683 Train acc: 0.923333\n",
      "Epoch: 174/500 Iteration: 1575 Train loss: 0.268269 Train acc: 0.913333\n",
      "Epoch: 174/500 Iteration: 1575 Validation loss: 0.314639 Validation acc: 0.875000\n",
      "Epoch: 175/500 Iteration: 1580 Train loss: 0.227844 Train acc: 0.928333\n",
      "Epoch: 176/500 Iteration: 1585 Train loss: 0.212987 Train acc: 0.930000\n",
      "Epoch: 176/500 Iteration: 1590 Train loss: 0.215188 Train acc: 0.925000\n",
      "Epoch: 177/500 Iteration: 1595 Train loss: 0.269004 Train acc: 0.901667\n",
      "Epoch: 177/500 Iteration: 1600 Train loss: 0.268104 Train acc: 0.905000\n",
      "Epoch: 177/500 Iteration: 1600 Validation loss: 0.324788 Validation acc: 0.888333\n",
      "Epoch: 178/500 Iteration: 1605 Train loss: 0.246088 Train acc: 0.930000\n",
      "Epoch: 178/500 Iteration: 1610 Train loss: 0.251640 Train acc: 0.905000\n",
      "Epoch: 179/500 Iteration: 1615 Train loss: 0.209089 Train acc: 0.928333\n",
      "Epoch: 179/500 Iteration: 1620 Train loss: 0.247703 Train acc: 0.910000\n",
      "Epoch: 180/500 Iteration: 1625 Train loss: 0.237269 Train acc: 0.913333\n",
      "Epoch: 180/500 Iteration: 1625 Validation loss: 0.340162 Validation acc: 0.871667\n",
      "Epoch: 181/500 Iteration: 1630 Train loss: 0.229634 Train acc: 0.921667\n",
      "Epoch: 181/500 Iteration: 1635 Train loss: 0.206394 Train acc: 0.918333\n",
      "Epoch: 182/500 Iteration: 1640 Train loss: 0.231975 Train acc: 0.920000\n",
      "Epoch: 182/500 Iteration: 1645 Train loss: 0.220017 Train acc: 0.928333\n",
      "Epoch: 183/500 Iteration: 1650 Train loss: 0.229125 Train acc: 0.921667\n",
      "Epoch: 183/500 Iteration: 1650 Validation loss: 0.372942 Validation acc: 0.858333\n",
      "Epoch: 183/500 Iteration: 1655 Train loss: 0.245543 Train acc: 0.911667\n",
      "Epoch: 184/500 Iteration: 1660 Train loss: 0.221830 Train acc: 0.910000\n",
      "Epoch: 184/500 Iteration: 1665 Train loss: 0.276519 Train acc: 0.901667\n",
      "Epoch: 185/500 Iteration: 1670 Train loss: 0.243670 Train acc: 0.898333\n",
      "Epoch: 186/500 Iteration: 1675 Train loss: 0.228274 Train acc: 0.910000\n",
      "Epoch: 186/500 Iteration: 1675 Validation loss: 0.337216 Validation acc: 0.882500\n",
      "Epoch: 186/500 Iteration: 1680 Train loss: 0.223817 Train acc: 0.921667\n",
      "Epoch: 187/500 Iteration: 1685 Train loss: 0.237076 Train acc: 0.910000\n",
      "Epoch: 187/500 Iteration: 1690 Train loss: 0.219530 Train acc: 0.915000\n",
      "Epoch: 188/500 Iteration: 1695 Train loss: 0.225311 Train acc: 0.931667\n",
      "Epoch: 188/500 Iteration: 1700 Train loss: 0.236042 Train acc: 0.920000\n",
      "Epoch: 188/500 Iteration: 1700 Validation loss: 0.316849 Validation acc: 0.894167\n",
      "Epoch: 189/500 Iteration: 1705 Train loss: 0.173578 Train acc: 0.933333\n",
      "Epoch: 189/500 Iteration: 1710 Train loss: 0.234360 Train acc: 0.925000\n",
      "Epoch: 190/500 Iteration: 1715 Train loss: 0.219617 Train acc: 0.928333\n",
      "Epoch: 191/500 Iteration: 1720 Train loss: 0.193044 Train acc: 0.933333\n",
      "Epoch: 191/500 Iteration: 1725 Train loss: 0.223782 Train acc: 0.930000\n",
      "Epoch: 191/500 Iteration: 1725 Validation loss: 0.322579 Validation acc: 0.884167\n",
      "Epoch: 192/500 Iteration: 1730 Train loss: 0.231147 Train acc: 0.928333\n",
      "Epoch: 192/500 Iteration: 1735 Train loss: 0.234752 Train acc: 0.911667\n",
      "Epoch: 193/500 Iteration: 1740 Train loss: 0.225909 Train acc: 0.933333\n",
      "Epoch: 193/500 Iteration: 1745 Train loss: 0.236921 Train acc: 0.921667\n",
      "Epoch: 194/500 Iteration: 1750 Train loss: 0.172900 Train acc: 0.940000\n",
      "Epoch: 194/500 Iteration: 1750 Validation loss: 0.331340 Validation acc: 0.885833\n",
      "Epoch: 194/500 Iteration: 1755 Train loss: 0.234236 Train acc: 0.918333\n",
      "Epoch: 195/500 Iteration: 1760 Train loss: 0.178894 Train acc: 0.935000\n",
      "Epoch: 196/500 Iteration: 1765 Train loss: 0.195946 Train acc: 0.938333\n",
      "Epoch: 196/500 Iteration: 1770 Train loss: 0.220518 Train acc: 0.928333\n",
      "Epoch: 197/500 Iteration: 1775 Train loss: 0.253586 Train acc: 0.923333\n",
      "Epoch: 197/500 Iteration: 1775 Validation loss: 0.326499 Validation acc: 0.880833\n",
      "Epoch: 197/500 Iteration: 1780 Train loss: 0.222393 Train acc: 0.920000\n",
      "Epoch: 198/500 Iteration: 1785 Train loss: 0.237386 Train acc: 0.925000\n",
      "Epoch: 198/500 Iteration: 1790 Train loss: 0.235793 Train acc: 0.920000\n",
      "Epoch: 199/500 Iteration: 1795 Train loss: 0.198505 Train acc: 0.943333\n",
      "Epoch: 199/500 Iteration: 1800 Train loss: 0.230423 Train acc: 0.931667\n",
      "Epoch: 199/500 Iteration: 1800 Validation loss: 0.331198 Validation acc: 0.891667\n",
      "Epoch: 200/500 Iteration: 1805 Train loss: 0.207937 Train acc: 0.933333\n",
      "Epoch: 201/500 Iteration: 1810 Train loss: 0.175643 Train acc: 0.948333\n",
      "Epoch: 201/500 Iteration: 1815 Train loss: 0.178410 Train acc: 0.941667\n",
      "Epoch: 202/500 Iteration: 1820 Train loss: 0.211660 Train acc: 0.926667\n",
      "Epoch: 202/500 Iteration: 1825 Train loss: 0.196914 Train acc: 0.926667\n",
      "Epoch: 202/500 Iteration: 1825 Validation loss: 0.305916 Validation acc: 0.897500\n",
      "Epoch: 203/500 Iteration: 1830 Train loss: 0.191779 Train acc: 0.943333\n",
      "Epoch: 203/500 Iteration: 1835 Train loss: 0.190850 Train acc: 0.936667\n",
      "Epoch: 204/500 Iteration: 1840 Train loss: 0.179227 Train acc: 0.943333\n",
      "Epoch: 204/500 Iteration: 1845 Train loss: 0.240830 Train acc: 0.923333\n",
      "Epoch: 205/500 Iteration: 1850 Train loss: 0.177662 Train acc: 0.945000\n",
      "Epoch: 205/500 Iteration: 1850 Validation loss: 0.330418 Validation acc: 0.876667\n",
      "Epoch: 206/500 Iteration: 1855 Train loss: 0.184946 Train acc: 0.945000\n",
      "Epoch: 206/500 Iteration: 1860 Train loss: 0.196752 Train acc: 0.936667\n",
      "Epoch: 207/500 Iteration: 1865 Train loss: 0.220065 Train acc: 0.915000\n",
      "Epoch: 207/500 Iteration: 1870 Train loss: 0.186253 Train acc: 0.936667\n",
      "Epoch: 208/500 Iteration: 1875 Train loss: 0.206384 Train acc: 0.940000\n",
      "Epoch: 208/500 Iteration: 1875 Validation loss: 0.325264 Validation acc: 0.886667\n",
      "Epoch: 208/500 Iteration: 1880 Train loss: 0.201059 Train acc: 0.936667\n",
      "Epoch: 209/500 Iteration: 1885 Train loss: 0.159766 Train acc: 0.948333\n",
      "Epoch: 209/500 Iteration: 1890 Train loss: 0.211467 Train acc: 0.930000\n",
      "Epoch: 210/500 Iteration: 1895 Train loss: 0.207111 Train acc: 0.926667\n",
      "Epoch: 211/500 Iteration: 1900 Train loss: 0.175138 Train acc: 0.941667\n",
      "Epoch: 211/500 Iteration: 1900 Validation loss: 0.312228 Validation acc: 0.894167\n",
      "Epoch: 211/500 Iteration: 1905 Train loss: 0.182987 Train acc: 0.941667\n",
      "Epoch: 212/500 Iteration: 1910 Train loss: 0.196762 Train acc: 0.931667\n",
      "Epoch: 212/500 Iteration: 1915 Train loss: 0.196664 Train acc: 0.935000\n",
      "Epoch: 213/500 Iteration: 1920 Train loss: 0.189481 Train acc: 0.938333\n",
      "Epoch: 213/500 Iteration: 1925 Train loss: 0.184490 Train acc: 0.936667\n",
      "Epoch: 213/500 Iteration: 1925 Validation loss: 0.306015 Validation acc: 0.899167\n",
      "Epoch: 214/500 Iteration: 1930 Train loss: 0.144495 Train acc: 0.958333\n",
      "Epoch: 214/500 Iteration: 1935 Train loss: 0.207585 Train acc: 0.931667\n",
      "Epoch: 215/500 Iteration: 1940 Train loss: 0.170109 Train acc: 0.945000\n",
      "Epoch: 216/500 Iteration: 1945 Train loss: 0.165293 Train acc: 0.950000\n",
      "Epoch: 216/500 Iteration: 1950 Train loss: 0.177141 Train acc: 0.943333\n",
      "Epoch: 216/500 Iteration: 1950 Validation loss: 0.315725 Validation acc: 0.901667\n",
      "Epoch: 217/500 Iteration: 1955 Train loss: 0.188535 Train acc: 0.938333\n",
      "Epoch: 217/500 Iteration: 1960 Train loss: 0.178014 Train acc: 0.938333\n",
      "Epoch: 218/500 Iteration: 1965 Train loss: 0.288956 Train acc: 0.905000\n",
      "Epoch: 218/500 Iteration: 1970 Train loss: 0.268001 Train acc: 0.905000\n",
      "Epoch: 219/500 Iteration: 1975 Train loss: 0.206033 Train acc: 0.918333\n",
      "Epoch: 219/500 Iteration: 1975 Validation loss: 0.320122 Validation acc: 0.889167\n",
      "Epoch: 219/500 Iteration: 1980 Train loss: 0.248894 Train acc: 0.913333\n",
      "Epoch: 220/500 Iteration: 1985 Train loss: 0.191302 Train acc: 0.938333\n",
      "Epoch: 221/500 Iteration: 1990 Train loss: 0.196378 Train acc: 0.936667\n",
      "Epoch: 221/500 Iteration: 1995 Train loss: 0.188400 Train acc: 0.936667\n",
      "Epoch: 222/500 Iteration: 2000 Train loss: 0.222453 Train acc: 0.916667\n",
      "Epoch: 222/500 Iteration: 2000 Validation loss: 0.317267 Validation acc: 0.896667\n",
      "Epoch: 222/500 Iteration: 2005 Train loss: 0.205500 Train acc: 0.931667\n",
      "Epoch: 223/500 Iteration: 2010 Train loss: 0.197747 Train acc: 0.938333\n",
      "Epoch: 223/500 Iteration: 2015 Train loss: 0.217113 Train acc: 0.925000\n",
      "Epoch: 224/500 Iteration: 2020 Train loss: 0.161528 Train acc: 0.943333\n",
      "Epoch: 224/500 Iteration: 2025 Train loss: 0.202305 Train acc: 0.935000\n",
      "Epoch: 224/500 Iteration: 2025 Validation loss: 0.311873 Validation acc: 0.894167\n",
      "Epoch: 225/500 Iteration: 2030 Train loss: 0.177376 Train acc: 0.946667\n",
      "Epoch: 226/500 Iteration: 2035 Train loss: 0.182751 Train acc: 0.935000\n",
      "Epoch: 226/500 Iteration: 2040 Train loss: 0.200252 Train acc: 0.931667\n",
      "Epoch: 227/500 Iteration: 2045 Train loss: 0.243216 Train acc: 0.901667\n",
      "Epoch: 227/500 Iteration: 2050 Train loss: 0.185452 Train acc: 0.933333\n",
      "Epoch: 227/500 Iteration: 2050 Validation loss: 0.325327 Validation acc: 0.891667\n",
      "Epoch: 228/500 Iteration: 2055 Train loss: 0.197490 Train acc: 0.938333\n",
      "Epoch: 228/500 Iteration: 2060 Train loss: 0.211583 Train acc: 0.918333\n",
      "Epoch: 229/500 Iteration: 2065 Train loss: 0.158648 Train acc: 0.950000\n",
      "Epoch: 229/500 Iteration: 2070 Train loss: 0.211142 Train acc: 0.931667\n",
      "Epoch: 230/500 Iteration: 2075 Train loss: 0.154956 Train acc: 0.953333\n",
      "Epoch: 230/500 Iteration: 2075 Validation loss: 0.309279 Validation acc: 0.890833\n",
      "Epoch: 231/500 Iteration: 2080 Train loss: 0.170787 Train acc: 0.953333\n",
      "Epoch: 231/500 Iteration: 2085 Train loss: 0.172555 Train acc: 0.953333\n",
      "Epoch: 232/500 Iteration: 2090 Train loss: 0.227508 Train acc: 0.915000\n",
      "Epoch: 232/500 Iteration: 2095 Train loss: 0.216352 Train acc: 0.926667\n",
      "Epoch: 233/500 Iteration: 2100 Train loss: 0.259864 Train acc: 0.901667\n",
      "Epoch: 233/500 Iteration: 2100 Validation loss: 0.356050 Validation acc: 0.881667\n",
      "Epoch: 233/500 Iteration: 2105 Train loss: 0.256698 Train acc: 0.910000\n",
      "Epoch: 234/500 Iteration: 2110 Train loss: 0.184357 Train acc: 0.936667\n",
      "Epoch: 234/500 Iteration: 2115 Train loss: 0.228244 Train acc: 0.921667\n",
      "Epoch: 235/500 Iteration: 2120 Train loss: 0.182711 Train acc: 0.936667\n",
      "Epoch: 236/500 Iteration: 2125 Train loss: 0.203634 Train acc: 0.931667\n",
      "Epoch: 236/500 Iteration: 2125 Validation loss: 0.324562 Validation acc: 0.881667\n",
      "Epoch: 236/500 Iteration: 2130 Train loss: 0.221758 Train acc: 0.921667\n",
      "Epoch: 237/500 Iteration: 2135 Train loss: 0.203189 Train acc: 0.933333\n",
      "Epoch: 237/500 Iteration: 2140 Train loss: 0.200346 Train acc: 0.921667\n",
      "Epoch: 238/500 Iteration: 2145 Train loss: 0.178107 Train acc: 0.948333\n",
      "Epoch: 238/500 Iteration: 2150 Train loss: 0.206446 Train acc: 0.936667\n",
      "Epoch: 238/500 Iteration: 2150 Validation loss: 0.308531 Validation acc: 0.901667\n",
      "Epoch: 239/500 Iteration: 2155 Train loss: 0.148010 Train acc: 0.950000\n",
      "Epoch: 239/500 Iteration: 2160 Train loss: 0.197998 Train acc: 0.938333\n",
      "Epoch: 240/500 Iteration: 2165 Train loss: 0.151874 Train acc: 0.953333\n",
      "Epoch: 241/500 Iteration: 2170 Train loss: 0.136405 Train acc: 0.963333\n",
      "Epoch: 241/500 Iteration: 2175 Train loss: 0.153134 Train acc: 0.953333\n",
      "Epoch: 241/500 Iteration: 2175 Validation loss: 0.316545 Validation acc: 0.892500\n",
      "Epoch: 242/500 Iteration: 2180 Train loss: 0.174990 Train acc: 0.946667\n",
      "Epoch: 242/500 Iteration: 2185 Train loss: 0.192971 Train acc: 0.930000\n",
      "Epoch: 243/500 Iteration: 2190 Train loss: 0.178478 Train acc: 0.950000\n",
      "Epoch: 243/500 Iteration: 2195 Train loss: 0.179375 Train acc: 0.930000\n",
      "Epoch: 244/500 Iteration: 2200 Train loss: 0.136682 Train acc: 0.948333\n",
      "Epoch: 244/500 Iteration: 2200 Validation loss: 0.299092 Validation acc: 0.904167\n",
      "Epoch: 244/500 Iteration: 2205 Train loss: 0.206137 Train acc: 0.935000\n",
      "Epoch: 245/500 Iteration: 2210 Train loss: 0.145537 Train acc: 0.951667\n",
      "Epoch: 246/500 Iteration: 2215 Train loss: 0.155796 Train acc: 0.953333\n",
      "Epoch: 246/500 Iteration: 2220 Train loss: 0.171803 Train acc: 0.941667\n",
      "Epoch: 247/500 Iteration: 2225 Train loss: 0.176916 Train acc: 0.940000\n",
      "Epoch: 247/500 Iteration: 2225 Validation loss: 0.315749 Validation acc: 0.903333\n",
      "Epoch: 247/500 Iteration: 2230 Train loss: 0.198385 Train acc: 0.926667\n",
      "Epoch: 248/500 Iteration: 2235 Train loss: 0.158105 Train acc: 0.951667\n",
      "Epoch: 248/500 Iteration: 2240 Train loss: 0.196335 Train acc: 0.928333\n",
      "Epoch: 249/500 Iteration: 2245 Train loss: 0.124580 Train acc: 0.956667\n",
      "Epoch: 249/500 Iteration: 2250 Train loss: 0.201772 Train acc: 0.943333\n",
      "Epoch: 249/500 Iteration: 2250 Validation loss: 0.374386 Validation acc: 0.867500\n",
      "Epoch: 250/500 Iteration: 2255 Train loss: 0.175808 Train acc: 0.938333\n",
      "Epoch: 251/500 Iteration: 2260 Train loss: 0.174438 Train acc: 0.948333\n",
      "Epoch: 251/500 Iteration: 2265 Train loss: 0.170779 Train acc: 0.943333\n",
      "Epoch: 252/500 Iteration: 2270 Train loss: 0.166265 Train acc: 0.946667\n",
      "Epoch: 252/500 Iteration: 2275 Train loss: 0.207457 Train acc: 0.926667\n",
      "Epoch: 252/500 Iteration: 2275 Validation loss: 0.323933 Validation acc: 0.893333\n",
      "Epoch: 253/500 Iteration: 2280 Train loss: 0.168549 Train acc: 0.941667\n",
      "Epoch: 253/500 Iteration: 2285 Train loss: 0.219341 Train acc: 0.923333\n",
      "Epoch: 254/500 Iteration: 2290 Train loss: 0.181901 Train acc: 0.935000\n",
      "Epoch: 254/500 Iteration: 2295 Train loss: 0.264577 Train acc: 0.921667\n",
      "Epoch: 255/500 Iteration: 2300 Train loss: 0.191442 Train acc: 0.943333\n",
      "Epoch: 255/500 Iteration: 2300 Validation loss: 0.306234 Validation acc: 0.890000\n",
      "Epoch: 256/500 Iteration: 2305 Train loss: 0.187063 Train acc: 0.931667\n",
      "Epoch: 256/500 Iteration: 2310 Train loss: 0.189374 Train acc: 0.931667\n",
      "Epoch: 257/500 Iteration: 2315 Train loss: 0.193744 Train acc: 0.935000\n",
      "Epoch: 257/500 Iteration: 2320 Train loss: 0.186751 Train acc: 0.925000\n",
      "Epoch: 258/500 Iteration: 2325 Train loss: 0.174438 Train acc: 0.940000\n",
      "Epoch: 258/500 Iteration: 2325 Validation loss: 0.302517 Validation acc: 0.902500\n",
      "Epoch: 258/500 Iteration: 2330 Train loss: 0.194688 Train acc: 0.926667\n",
      "Epoch: 259/500 Iteration: 2335 Train loss: 0.142073 Train acc: 0.946667\n",
      "Epoch: 259/500 Iteration: 2340 Train loss: 0.206281 Train acc: 0.928333\n",
      "Epoch: 260/500 Iteration: 2345 Train loss: 0.142140 Train acc: 0.953333\n",
      "Epoch: 261/500 Iteration: 2350 Train loss: 0.169796 Train acc: 0.945000\n",
      "Epoch: 261/500 Iteration: 2350 Validation loss: 0.311127 Validation acc: 0.889167\n",
      "Epoch: 261/500 Iteration: 2355 Train loss: 0.178374 Train acc: 0.940000\n",
      "Epoch: 262/500 Iteration: 2360 Train loss: 0.169714 Train acc: 0.945000\n",
      "Epoch: 262/500 Iteration: 2365 Train loss: 0.172183 Train acc: 0.943333\n",
      "Epoch: 263/500 Iteration: 2370 Train loss: 0.161014 Train acc: 0.940000\n",
      "Epoch: 263/500 Iteration: 2375 Train loss: 0.175127 Train acc: 0.935000\n",
      "Epoch: 263/500 Iteration: 2375 Validation loss: 0.310069 Validation acc: 0.903333\n",
      "Epoch: 264/500 Iteration: 2380 Train loss: 0.130143 Train acc: 0.960000\n",
      "Epoch: 264/500 Iteration: 2385 Train loss: 0.200765 Train acc: 0.931667\n",
      "Epoch: 265/500 Iteration: 2390 Train loss: 0.149126 Train acc: 0.961667\n",
      "Epoch: 266/500 Iteration: 2395 Train loss: 0.160613 Train acc: 0.953333\n",
      "Epoch: 266/500 Iteration: 2400 Train loss: 0.152787 Train acc: 0.951667\n",
      "Epoch: 266/500 Iteration: 2400 Validation loss: 0.315068 Validation acc: 0.906667\n",
      "Epoch: 267/500 Iteration: 2405 Train loss: 0.166555 Train acc: 0.943333\n",
      "Epoch: 267/500 Iteration: 2410 Train loss: 0.165069 Train acc: 0.933333\n",
      "Epoch: 268/500 Iteration: 2415 Train loss: 0.144049 Train acc: 0.956667\n",
      "Epoch: 268/500 Iteration: 2420 Train loss: 0.163771 Train acc: 0.950000\n",
      "Epoch: 269/500 Iteration: 2425 Train loss: 0.112604 Train acc: 0.961667\n",
      "Epoch: 269/500 Iteration: 2425 Validation loss: 0.310600 Validation acc: 0.894167\n",
      "Epoch: 269/500 Iteration: 2430 Train loss: 0.193297 Train acc: 0.945000\n",
      "Epoch: 270/500 Iteration: 2435 Train loss: 0.124794 Train acc: 0.963333\n",
      "Epoch: 271/500 Iteration: 2440 Train loss: 0.131458 Train acc: 0.968333\n",
      "Epoch: 271/500 Iteration: 2445 Train loss: 0.121154 Train acc: 0.955000\n",
      "Epoch: 272/500 Iteration: 2450 Train loss: 0.124298 Train acc: 0.955000\n",
      "Epoch: 272/500 Iteration: 2450 Validation loss: 0.327308 Validation acc: 0.900833\n",
      "Epoch: 272/500 Iteration: 2455 Train loss: 0.151994 Train acc: 0.951667\n",
      "Epoch: 273/500 Iteration: 2460 Train loss: 0.132091 Train acc: 0.961667\n",
      "Epoch: 273/500 Iteration: 2465 Train loss: 0.149635 Train acc: 0.951667\n",
      "Epoch: 274/500 Iteration: 2470 Train loss: 0.098601 Train acc: 0.971667\n",
      "Epoch: 274/500 Iteration: 2475 Train loss: 0.187234 Train acc: 0.951667\n",
      "Epoch: 274/500 Iteration: 2475 Validation loss: 0.342523 Validation acc: 0.887500\n",
      "Epoch: 275/500 Iteration: 2480 Train loss: 0.134179 Train acc: 0.955000\n",
      "Epoch: 276/500 Iteration: 2485 Train loss: 0.133721 Train acc: 0.958333\n",
      "Epoch: 276/500 Iteration: 2490 Train loss: 0.130419 Train acc: 0.951667\n",
      "Epoch: 277/500 Iteration: 2495 Train loss: 0.147426 Train acc: 0.948333\n",
      "Epoch: 277/500 Iteration: 2500 Train loss: 0.150089 Train acc: 0.946667\n",
      "Epoch: 277/500 Iteration: 2500 Validation loss: 0.331565 Validation acc: 0.902500\n",
      "Epoch: 278/500 Iteration: 2505 Train loss: 0.148924 Train acc: 0.958333\n",
      "Epoch: 278/500 Iteration: 2510 Train loss: 0.163054 Train acc: 0.945000\n",
      "Epoch: 279/500 Iteration: 2515 Train loss: 0.106989 Train acc: 0.961667\n",
      "Epoch: 279/500 Iteration: 2520 Train loss: 0.180738 Train acc: 0.948333\n",
      "Epoch: 280/500 Iteration: 2525 Train loss: 0.135886 Train acc: 0.958333\n",
      "Epoch: 280/500 Iteration: 2525 Validation loss: 0.311676 Validation acc: 0.902500\n",
      "Epoch: 281/500 Iteration: 2530 Train loss: 0.151384 Train acc: 0.965000\n",
      "Epoch: 281/500 Iteration: 2535 Train loss: 0.131021 Train acc: 0.956667\n",
      "Epoch: 282/500 Iteration: 2540 Train loss: 0.125162 Train acc: 0.968333\n",
      "Epoch: 282/500 Iteration: 2545 Train loss: 0.137984 Train acc: 0.951667\n",
      "Epoch: 283/500 Iteration: 2550 Train loss: 0.121036 Train acc: 0.966667\n",
      "Epoch: 283/500 Iteration: 2550 Validation loss: 0.313280 Validation acc: 0.898333\n",
      "Epoch: 283/500 Iteration: 2555 Train loss: 0.133661 Train acc: 0.955000\n",
      "Epoch: 284/500 Iteration: 2560 Train loss: 0.104051 Train acc: 0.968333\n",
      "Epoch: 284/500 Iteration: 2565 Train loss: 0.164350 Train acc: 0.950000\n",
      "Epoch: 285/500 Iteration: 2570 Train loss: 0.106604 Train acc: 0.965000\n",
      "Epoch: 286/500 Iteration: 2575 Train loss: 0.105624 Train acc: 0.971667\n",
      "Epoch: 286/500 Iteration: 2575 Validation loss: 0.316358 Validation acc: 0.904167\n",
      "Epoch: 286/500 Iteration: 2580 Train loss: 0.131015 Train acc: 0.958333\n",
      "Epoch: 287/500 Iteration: 2585 Train loss: 0.133395 Train acc: 0.960000\n",
      "Epoch: 287/500 Iteration: 2590 Train loss: 0.145562 Train acc: 0.951667\n",
      "Epoch: 288/500 Iteration: 2595 Train loss: 0.133480 Train acc: 0.956667\n",
      "Epoch: 288/500 Iteration: 2600 Train loss: 0.144508 Train acc: 0.946667\n",
      "Epoch: 288/500 Iteration: 2600 Validation loss: 0.365752 Validation acc: 0.884167\n",
      "Epoch: 289/500 Iteration: 2605 Train loss: 0.105185 Train acc: 0.971667\n",
      "Epoch: 289/500 Iteration: 2610 Train loss: 0.176618 Train acc: 0.951667\n",
      "Epoch: 290/500 Iteration: 2615 Train loss: 0.127228 Train acc: 0.963333\n",
      "Epoch: 291/500 Iteration: 2620 Train loss: 0.156035 Train acc: 0.950000\n",
      "Epoch: 291/500 Iteration: 2625 Train loss: 0.160292 Train acc: 0.941667\n",
      "Epoch: 291/500 Iteration: 2625 Validation loss: 0.367162 Validation acc: 0.892500\n",
      "Epoch: 292/500 Iteration: 2630 Train loss: 0.195323 Train acc: 0.945000\n",
      "Epoch: 292/500 Iteration: 2635 Train loss: 0.173743 Train acc: 0.945000\n",
      "Epoch: 293/500 Iteration: 2640 Train loss: 0.136524 Train acc: 0.955000\n",
      "Epoch: 293/500 Iteration: 2645 Train loss: 0.143702 Train acc: 0.956667\n",
      "Epoch: 294/500 Iteration: 2650 Train loss: 0.101204 Train acc: 0.968333\n",
      "Epoch: 294/500 Iteration: 2650 Validation loss: 0.321392 Validation acc: 0.901667\n",
      "Epoch: 294/500 Iteration: 2655 Train loss: 0.160291 Train acc: 0.943333\n",
      "Epoch: 295/500 Iteration: 2660 Train loss: 0.122276 Train acc: 0.960000\n",
      "Epoch: 296/500 Iteration: 2665 Train loss: 0.150483 Train acc: 0.955000\n",
      "Epoch: 296/500 Iteration: 2670 Train loss: 0.124106 Train acc: 0.963333\n",
      "Epoch: 297/500 Iteration: 2675 Train loss: 0.113323 Train acc: 0.958333\n",
      "Epoch: 297/500 Iteration: 2675 Validation loss: 0.329201 Validation acc: 0.900833\n",
      "Epoch: 297/500 Iteration: 2680 Train loss: 0.132808 Train acc: 0.950000\n",
      "Epoch: 298/500 Iteration: 2685 Train loss: 0.118970 Train acc: 0.958333\n",
      "Epoch: 298/500 Iteration: 2690 Train loss: 0.139512 Train acc: 0.950000\n",
      "Epoch: 299/500 Iteration: 2695 Train loss: 0.101563 Train acc: 0.966667\n",
      "Epoch: 299/500 Iteration: 2700 Train loss: 0.170411 Train acc: 0.958333\n",
      "Epoch: 299/500 Iteration: 2700 Validation loss: 0.319401 Validation acc: 0.896667\n",
      "Epoch: 300/500 Iteration: 2705 Train loss: 0.127217 Train acc: 0.951667\n",
      "Epoch: 301/500 Iteration: 2710 Train loss: 0.122650 Train acc: 0.966667\n",
      "Epoch: 301/500 Iteration: 2715 Train loss: 0.095566 Train acc: 0.980000\n",
      "Epoch: 302/500 Iteration: 2720 Train loss: 0.127265 Train acc: 0.961667\n",
      "Epoch: 302/500 Iteration: 2725 Train loss: 0.165134 Train acc: 0.943333\n",
      "Epoch: 302/500 Iteration: 2725 Validation loss: 0.358763 Validation acc: 0.888333\n",
      "Epoch: 303/500 Iteration: 2730 Train loss: 0.113932 Train acc: 0.973333\n",
      "Epoch: 303/500 Iteration: 2735 Train loss: 0.121379 Train acc: 0.968333\n",
      "Epoch: 304/500 Iteration: 2740 Train loss: 0.087522 Train acc: 0.978333\n",
      "Epoch: 304/500 Iteration: 2745 Train loss: 0.157711 Train acc: 0.958333\n",
      "Epoch: 305/500 Iteration: 2750 Train loss: 0.118479 Train acc: 0.961667\n",
      "Epoch: 305/500 Iteration: 2750 Validation loss: 0.329595 Validation acc: 0.899167\n",
      "Epoch: 306/500 Iteration: 2755 Train loss: 0.115933 Train acc: 0.970000\n",
      "Epoch: 306/500 Iteration: 2760 Train loss: 0.104793 Train acc: 0.970000\n",
      "Epoch: 307/500 Iteration: 2765 Train loss: 0.120345 Train acc: 0.961667\n",
      "Epoch: 307/500 Iteration: 2770 Train loss: 0.141562 Train acc: 0.951667\n",
      "Epoch: 308/500 Iteration: 2775 Train loss: 0.128308 Train acc: 0.951667\n",
      "Epoch: 308/500 Iteration: 2775 Validation loss: 0.316913 Validation acc: 0.898333\n",
      "Epoch: 308/500 Iteration: 2780 Train loss: 0.130227 Train acc: 0.956667\n",
      "Epoch: 309/500 Iteration: 2785 Train loss: 0.072237 Train acc: 0.976667\n",
      "Epoch: 309/500 Iteration: 2790 Train loss: 0.136928 Train acc: 0.958333\n",
      "Epoch: 310/500 Iteration: 2795 Train loss: 0.090711 Train acc: 0.975000\n",
      "Epoch: 311/500 Iteration: 2800 Train loss: 0.102171 Train acc: 0.976667\n",
      "Epoch: 311/500 Iteration: 2800 Validation loss: 0.325283 Validation acc: 0.900000\n",
      "Epoch: 311/500 Iteration: 2805 Train loss: 0.105833 Train acc: 0.970000\n",
      "Epoch: 312/500 Iteration: 2810 Train loss: 0.112138 Train acc: 0.963333\n",
      "Epoch: 312/500 Iteration: 2815 Train loss: 0.141219 Train acc: 0.948333\n",
      "Epoch: 313/500 Iteration: 2820 Train loss: 0.097164 Train acc: 0.971667\n",
      "Epoch: 313/500 Iteration: 2825 Train loss: 0.125814 Train acc: 0.955000\n",
      "Epoch: 313/500 Iteration: 2825 Validation loss: 0.346535 Validation acc: 0.896667\n",
      "Epoch: 314/500 Iteration: 2830 Train loss: 0.084269 Train acc: 0.971667\n",
      "Epoch: 314/500 Iteration: 2835 Train loss: 0.136121 Train acc: 0.951667\n",
      "Epoch: 315/500 Iteration: 2840 Train loss: 0.119765 Train acc: 0.970000\n",
      "Epoch: 316/500 Iteration: 2845 Train loss: 0.128424 Train acc: 0.960000\n",
      "Epoch: 316/500 Iteration: 2850 Train loss: 0.306835 Train acc: 0.923333\n",
      "Epoch: 316/500 Iteration: 2850 Validation loss: 0.479510 Validation acc: 0.858333\n",
      "Epoch: 317/500 Iteration: 2855 Train loss: 0.332620 Train acc: 0.911667\n",
      "Epoch: 317/500 Iteration: 2860 Train loss: 0.293520 Train acc: 0.900000\n",
      "Epoch: 318/500 Iteration: 2865 Train loss: 0.267772 Train acc: 0.916667\n",
      "Epoch: 318/500 Iteration: 2870 Train loss: 0.263319 Train acc: 0.911667\n",
      "Epoch: 319/500 Iteration: 2875 Train loss: 0.189194 Train acc: 0.943333\n",
      "Epoch: 319/500 Iteration: 2875 Validation loss: 0.383469 Validation acc: 0.880833\n",
      "Epoch: 319/500 Iteration: 2880 Train loss: 0.227883 Train acc: 0.931667\n",
      "Epoch: 320/500 Iteration: 2885 Train loss: 0.172525 Train acc: 0.946667\n",
      "Epoch: 321/500 Iteration: 2890 Train loss: 0.186015 Train acc: 0.946667\n",
      "Epoch: 321/500 Iteration: 2895 Train loss: 0.149087 Train acc: 0.951667\n",
      "Epoch: 322/500 Iteration: 2900 Train loss: 0.133760 Train acc: 0.956667\n",
      "Epoch: 322/500 Iteration: 2900 Validation loss: 0.318492 Validation acc: 0.895000\n",
      "Epoch: 322/500 Iteration: 2905 Train loss: 0.142963 Train acc: 0.945000\n",
      "Epoch: 323/500 Iteration: 2910 Train loss: 0.118554 Train acc: 0.960000\n",
      "Epoch: 323/500 Iteration: 2915 Train loss: 0.138160 Train acc: 0.955000\n",
      "Epoch: 324/500 Iteration: 2920 Train loss: 0.107826 Train acc: 0.966667\n",
      "Epoch: 324/500 Iteration: 2925 Train loss: 0.162153 Train acc: 0.955000\n",
      "Epoch: 324/500 Iteration: 2925 Validation loss: 0.312740 Validation acc: 0.903333\n",
      "Epoch: 325/500 Iteration: 2930 Train loss: 0.099923 Train acc: 0.968333\n",
      "Epoch: 326/500 Iteration: 2935 Train loss: 0.111084 Train acc: 0.966667\n",
      "Epoch: 326/500 Iteration: 2940 Train loss: 0.101231 Train acc: 0.973333\n",
      "Epoch: 327/500 Iteration: 2945 Train loss: 0.102819 Train acc: 0.965000\n",
      "Epoch: 327/500 Iteration: 2950 Train loss: 0.136992 Train acc: 0.951667\n",
      "Epoch: 327/500 Iteration: 2950 Validation loss: 0.339876 Validation acc: 0.894167\n",
      "Epoch: 328/500 Iteration: 2955 Train loss: 0.119762 Train acc: 0.961667\n",
      "Epoch: 328/500 Iteration: 2960 Train loss: 0.139114 Train acc: 0.958333\n",
      "Epoch: 329/500 Iteration: 2965 Train loss: 0.086665 Train acc: 0.975000\n",
      "Epoch: 329/500 Iteration: 2970 Train loss: 0.162354 Train acc: 0.956667\n",
      "Epoch: 330/500 Iteration: 2975 Train loss: 0.107142 Train acc: 0.971667\n",
      "Epoch: 330/500 Iteration: 2975 Validation loss: 0.344010 Validation acc: 0.899167\n",
      "Epoch: 331/500 Iteration: 2980 Train loss: 0.105162 Train acc: 0.971667\n",
      "Epoch: 331/500 Iteration: 2985 Train loss: 0.100452 Train acc: 0.970000\n",
      "Epoch: 332/500 Iteration: 2990 Train loss: 0.111105 Train acc: 0.966667\n",
      "Epoch: 332/500 Iteration: 2995 Train loss: 0.107845 Train acc: 0.970000\n",
      "Epoch: 333/500 Iteration: 3000 Train loss: 0.108441 Train acc: 0.968333\n",
      "Epoch: 333/500 Iteration: 3000 Validation loss: 0.315002 Validation acc: 0.903333\n",
      "Epoch: 333/500 Iteration: 3005 Train loss: 0.131148 Train acc: 0.960000\n",
      "Epoch: 334/500 Iteration: 3010 Train loss: 0.083966 Train acc: 0.976667\n",
      "Epoch: 334/500 Iteration: 3015 Train loss: 0.154957 Train acc: 0.956667\n",
      "Epoch: 335/500 Iteration: 3020 Train loss: 0.094970 Train acc: 0.968333\n",
      "Epoch: 336/500 Iteration: 3025 Train loss: 0.101200 Train acc: 0.973333\n",
      "Epoch: 336/500 Iteration: 3025 Validation loss: 0.341173 Validation acc: 0.899167\n",
      "Epoch: 336/500 Iteration: 3030 Train loss: 0.090796 Train acc: 0.971667\n",
      "Epoch: 337/500 Iteration: 3035 Train loss: 0.120649 Train acc: 0.958333\n",
      "Epoch: 337/500 Iteration: 3040 Train loss: 0.133565 Train acc: 0.955000\n",
      "Epoch: 338/500 Iteration: 3045 Train loss: 0.106117 Train acc: 0.965000\n",
      "Epoch: 338/500 Iteration: 3050 Train loss: 0.103745 Train acc: 0.963333\n",
      "Epoch: 338/500 Iteration: 3050 Validation loss: 0.337861 Validation acc: 0.897500\n",
      "Epoch: 339/500 Iteration: 3055 Train loss: 0.077887 Train acc: 0.978333\n",
      "Epoch: 339/500 Iteration: 3060 Train loss: 0.131401 Train acc: 0.963333\n",
      "Epoch: 340/500 Iteration: 3065 Train loss: 0.083178 Train acc: 0.981667\n",
      "Epoch: 341/500 Iteration: 3070 Train loss: 0.118864 Train acc: 0.973333\n",
      "Epoch: 341/500 Iteration: 3075 Train loss: 0.092313 Train acc: 0.980000\n",
      "Epoch: 341/500 Iteration: 3075 Validation loss: 0.344619 Validation acc: 0.900833\n",
      "Epoch: 342/500 Iteration: 3080 Train loss: 0.097744 Train acc: 0.968333\n",
      "Epoch: 342/500 Iteration: 3085 Train loss: 0.128363 Train acc: 0.961667\n",
      "Epoch: 343/500 Iteration: 3090 Train loss: 0.091681 Train acc: 0.968333\n",
      "Epoch: 343/500 Iteration: 3095 Train loss: 0.106097 Train acc: 0.963333\n",
      "Epoch: 344/500 Iteration: 3100 Train loss: 0.073783 Train acc: 0.980000\n",
      "Epoch: 344/500 Iteration: 3100 Validation loss: 0.336189 Validation acc: 0.906667\n",
      "Epoch: 344/500 Iteration: 3105 Train loss: 0.137160 Train acc: 0.961667\n",
      "Epoch: 345/500 Iteration: 3110 Train loss: 0.088670 Train acc: 0.970000\n",
      "Epoch: 346/500 Iteration: 3115 Train loss: 0.105833 Train acc: 0.975000\n",
      "Epoch: 346/500 Iteration: 3120 Train loss: 0.067633 Train acc: 0.980000\n",
      "Epoch: 347/500 Iteration: 3125 Train loss: 0.082539 Train acc: 0.975000\n",
      "Epoch: 347/500 Iteration: 3125 Validation loss: 0.356737 Validation acc: 0.895000\n",
      "Epoch: 347/500 Iteration: 3130 Train loss: 0.108513 Train acc: 0.970000\n",
      "Epoch: 348/500 Iteration: 3135 Train loss: 0.093679 Train acc: 0.971667\n",
      "Epoch: 348/500 Iteration: 3140 Train loss: 0.110527 Train acc: 0.961667\n",
      "Epoch: 349/500 Iteration: 3145 Train loss: 0.073954 Train acc: 0.978333\n",
      "Epoch: 349/500 Iteration: 3150 Train loss: 0.125137 Train acc: 0.970000\n",
      "Epoch: 349/500 Iteration: 3150 Validation loss: 0.344761 Validation acc: 0.905833\n",
      "Epoch: 350/500 Iteration: 3155 Train loss: 0.101650 Train acc: 0.970000\n",
      "Epoch: 351/500 Iteration: 3160 Train loss: 0.106431 Train acc: 0.973333\n",
      "Epoch: 351/500 Iteration: 3165 Train loss: 0.124665 Train acc: 0.966667\n",
      "Epoch: 352/500 Iteration: 3170 Train loss: 0.085652 Train acc: 0.975000\n",
      "Epoch: 352/500 Iteration: 3175 Train loss: 0.120264 Train acc: 0.965000\n",
      "Epoch: 352/500 Iteration: 3175 Validation loss: 0.365811 Validation acc: 0.899167\n",
      "Epoch: 353/500 Iteration: 3180 Train loss: 0.087224 Train acc: 0.975000\n",
      "Epoch: 353/500 Iteration: 3185 Train loss: 0.101118 Train acc: 0.965000\n",
      "Epoch: 354/500 Iteration: 3190 Train loss: 0.068805 Train acc: 0.985000\n",
      "Epoch: 354/500 Iteration: 3195 Train loss: 0.133596 Train acc: 0.960000\n",
      "Epoch: 355/500 Iteration: 3200 Train loss: 0.158860 Train acc: 0.945000\n",
      "Epoch: 355/500 Iteration: 3200 Validation loss: 0.431289 Validation acc: 0.877500\n",
      "Epoch: 356/500 Iteration: 3205 Train loss: 0.167087 Train acc: 0.946667\n",
      "Epoch: 356/500 Iteration: 3210 Train loss: 0.143598 Train acc: 0.951667\n",
      "Epoch: 357/500 Iteration: 3215 Train loss: 0.133160 Train acc: 0.956667\n",
      "Epoch: 357/500 Iteration: 3220 Train loss: 0.108582 Train acc: 0.968333\n",
      "Epoch: 358/500 Iteration: 3225 Train loss: 0.115705 Train acc: 0.958333\n",
      "Epoch: 358/500 Iteration: 3225 Validation loss: 0.322714 Validation acc: 0.909167\n",
      "Epoch: 358/500 Iteration: 3230 Train loss: 0.131967 Train acc: 0.960000\n",
      "Epoch: 359/500 Iteration: 3235 Train loss: 0.080330 Train acc: 0.973333\n",
      "Epoch: 359/500 Iteration: 3240 Train loss: 0.145690 Train acc: 0.958333\n",
      "Epoch: 360/500 Iteration: 3245 Train loss: 0.109692 Train acc: 0.968333\n",
      "Epoch: 361/500 Iteration: 3250 Train loss: 0.156638 Train acc: 0.953333\n",
      "Epoch: 361/500 Iteration: 3250 Validation loss: 0.307747 Validation acc: 0.905833\n",
      "Epoch: 361/500 Iteration: 3255 Train loss: 0.178109 Train acc: 0.940000\n",
      "Epoch: 362/500 Iteration: 3260 Train loss: 0.112428 Train acc: 0.963333\n",
      "Epoch: 362/500 Iteration: 3265 Train loss: 0.150074 Train acc: 0.958333\n",
      "Epoch: 363/500 Iteration: 3270 Train loss: 0.081396 Train acc: 0.978333\n",
      "Epoch: 363/500 Iteration: 3275 Train loss: 0.096011 Train acc: 0.976667\n",
      "Epoch: 363/500 Iteration: 3275 Validation loss: 0.338332 Validation acc: 0.906667\n",
      "Epoch: 364/500 Iteration: 3280 Train loss: 0.055430 Train acc: 0.983333\n",
      "Epoch: 364/500 Iteration: 3285 Train loss: 0.121746 Train acc: 0.963333\n",
      "Epoch: 365/500 Iteration: 3290 Train loss: 0.075768 Train acc: 0.978333\n",
      "Epoch: 366/500 Iteration: 3295 Train loss: 0.085103 Train acc: 0.980000\n",
      "Epoch: 366/500 Iteration: 3300 Train loss: 0.079537 Train acc: 0.981667\n",
      "Epoch: 366/500 Iteration: 3300 Validation loss: 0.341293 Validation acc: 0.910833\n",
      "Epoch: 367/500 Iteration: 3305 Train loss: 0.076972 Train acc: 0.980000\n",
      "Epoch: 367/500 Iteration: 3310 Train loss: 0.110905 Train acc: 0.966667\n",
      "Epoch: 368/500 Iteration: 3315 Train loss: 0.070419 Train acc: 0.980000\n",
      "Epoch: 368/500 Iteration: 3320 Train loss: 0.093104 Train acc: 0.973333\n",
      "Epoch: 369/500 Iteration: 3325 Train loss: 0.063028 Train acc: 0.983333\n",
      "Epoch: 369/500 Iteration: 3325 Validation loss: 0.343035 Validation acc: 0.910833\n",
      "Epoch: 369/500 Iteration: 3330 Train loss: 0.099450 Train acc: 0.973333\n",
      "Epoch: 370/500 Iteration: 3335 Train loss: 0.067181 Train acc: 0.980000\n",
      "Epoch: 371/500 Iteration: 3340 Train loss: 0.092178 Train acc: 0.978333\n",
      "Epoch: 371/500 Iteration: 3345 Train loss: 0.079864 Train acc: 0.975000\n",
      "Epoch: 372/500 Iteration: 3350 Train loss: 0.058210 Train acc: 0.988333\n",
      "Epoch: 372/500 Iteration: 3350 Validation loss: 0.366350 Validation acc: 0.900833\n",
      "Epoch: 372/500 Iteration: 3355 Train loss: 0.103007 Train acc: 0.970000\n",
      "Epoch: 373/500 Iteration: 3360 Train loss: 0.073270 Train acc: 0.981667\n",
      "Epoch: 373/500 Iteration: 3365 Train loss: 0.094497 Train acc: 0.968333\n",
      "Epoch: 374/500 Iteration: 3370 Train loss: 0.054762 Train acc: 0.981667\n",
      "Epoch: 374/500 Iteration: 3375 Train loss: 0.100917 Train acc: 0.975000\n",
      "Epoch: 374/500 Iteration: 3375 Validation loss: 0.350350 Validation acc: 0.908333\n",
      "Epoch: 375/500 Iteration: 3380 Train loss: 0.074012 Train acc: 0.978333\n",
      "Epoch: 376/500 Iteration: 3385 Train loss: 0.078589 Train acc: 0.981667\n",
      "Epoch: 376/500 Iteration: 3390 Train loss: 0.071159 Train acc: 0.980000\n",
      "Epoch: 377/500 Iteration: 3395 Train loss: 0.075723 Train acc: 0.985000\n",
      "Epoch: 377/500 Iteration: 3400 Train loss: 0.093700 Train acc: 0.966667\n",
      "Epoch: 377/500 Iteration: 3400 Validation loss: 0.392621 Validation acc: 0.900833\n",
      "Epoch: 378/500 Iteration: 3405 Train loss: 0.060421 Train acc: 0.980000\n",
      "Epoch: 378/500 Iteration: 3410 Train loss: 0.083471 Train acc: 0.980000\n",
      "Epoch: 379/500 Iteration: 3415 Train loss: 0.060530 Train acc: 0.980000\n",
      "Epoch: 379/500 Iteration: 3420 Train loss: 0.113354 Train acc: 0.971667\n",
      "Epoch: 380/500 Iteration: 3425 Train loss: 0.074240 Train acc: 0.976667\n",
      "Epoch: 380/500 Iteration: 3425 Validation loss: 0.374337 Validation acc: 0.905000\n",
      "Epoch: 381/500 Iteration: 3430 Train loss: 0.088793 Train acc: 0.980000\n",
      "Epoch: 381/500 Iteration: 3435 Train loss: 0.072900 Train acc: 0.981667\n",
      "Epoch: 382/500 Iteration: 3440 Train loss: 0.106787 Train acc: 0.968333\n",
      "Epoch: 382/500 Iteration: 3445 Train loss: 0.112413 Train acc: 0.963333\n",
      "Epoch: 383/500 Iteration: 3450 Train loss: 0.075166 Train acc: 0.973333\n",
      "Epoch: 383/500 Iteration: 3450 Validation loss: 0.410797 Validation acc: 0.892500\n",
      "Epoch: 383/500 Iteration: 3455 Train loss: 0.090341 Train acc: 0.973333\n",
      "Epoch: 384/500 Iteration: 3460 Train loss: 0.042164 Train acc: 0.991667\n",
      "Epoch: 384/500 Iteration: 3465 Train loss: 0.106073 Train acc: 0.971667\n",
      "Epoch: 385/500 Iteration: 3470 Train loss: 0.065889 Train acc: 0.978333\n",
      "Epoch: 386/500 Iteration: 3475 Train loss: 0.093574 Train acc: 0.985000\n",
      "Epoch: 386/500 Iteration: 3475 Validation loss: 0.346973 Validation acc: 0.905000\n",
      "Epoch: 386/500 Iteration: 3480 Train loss: 0.073826 Train acc: 0.975000\n",
      "Epoch: 387/500 Iteration: 3485 Train loss: 0.084604 Train acc: 0.971667\n",
      "Epoch: 387/500 Iteration: 3490 Train loss: 0.107679 Train acc: 0.965000\n",
      "Epoch: 388/500 Iteration: 3495 Train loss: 0.080357 Train acc: 0.980000\n",
      "Epoch: 388/500 Iteration: 3500 Train loss: 0.116054 Train acc: 0.968333\n",
      "Epoch: 388/500 Iteration: 3500 Validation loss: 0.356281 Validation acc: 0.910833\n",
      "Epoch: 389/500 Iteration: 3505 Train loss: 0.077793 Train acc: 0.970000\n",
      "Epoch: 389/500 Iteration: 3510 Train loss: 0.134756 Train acc: 0.961667\n",
      "Epoch: 390/500 Iteration: 3515 Train loss: 0.128059 Train acc: 0.958333\n",
      "Epoch: 391/500 Iteration: 3520 Train loss: 0.101331 Train acc: 0.973333\n",
      "Epoch: 391/500 Iteration: 3525 Train loss: 0.071297 Train acc: 0.978333\n",
      "Epoch: 391/500 Iteration: 3525 Validation loss: 0.403680 Validation acc: 0.897500\n",
      "Epoch: 392/500 Iteration: 3530 Train loss: 0.074435 Train acc: 0.985000\n",
      "Epoch: 392/500 Iteration: 3535 Train loss: 0.085265 Train acc: 0.976667\n",
      "Epoch: 393/500 Iteration: 3540 Train loss: 0.063234 Train acc: 0.983333\n",
      "Epoch: 393/500 Iteration: 3545 Train loss: 0.079595 Train acc: 0.976667\n",
      "Epoch: 394/500 Iteration: 3550 Train loss: 0.047080 Train acc: 0.983333\n",
      "Epoch: 394/500 Iteration: 3550 Validation loss: 0.389891 Validation acc: 0.905000\n",
      "Epoch: 394/500 Iteration: 3555 Train loss: 0.085229 Train acc: 0.981667\n",
      "Epoch: 395/500 Iteration: 3560 Train loss: 0.056441 Train acc: 0.988333\n",
      "Epoch: 396/500 Iteration: 3565 Train loss: 0.063777 Train acc: 0.985000\n",
      "Epoch: 396/500 Iteration: 3570 Train loss: 0.049280 Train acc: 0.990000\n",
      "Epoch: 397/500 Iteration: 3575 Train loss: 0.052004 Train acc: 0.988333\n",
      "Epoch: 397/500 Iteration: 3575 Validation loss: 0.383232 Validation acc: 0.906667\n",
      "Epoch: 397/500 Iteration: 3580 Train loss: 0.076071 Train acc: 0.980000\n",
      "Epoch: 398/500 Iteration: 3585 Train loss: 0.062342 Train acc: 0.985000\n",
      "Epoch: 398/500 Iteration: 3590 Train loss: 0.096457 Train acc: 0.976667\n",
      "Epoch: 399/500 Iteration: 3595 Train loss: 0.067999 Train acc: 0.981667\n",
      "Epoch: 399/500 Iteration: 3600 Train loss: 0.110957 Train acc: 0.966667\n",
      "Epoch: 399/500 Iteration: 3600 Validation loss: 0.384624 Validation acc: 0.907500\n",
      "Epoch: 400/500 Iteration: 3605 Train loss: 0.096790 Train acc: 0.966667\n",
      "Epoch: 401/500 Iteration: 3610 Train loss: 0.081788 Train acc: 0.975000\n",
      "Epoch: 401/500 Iteration: 3615 Train loss: 0.075240 Train acc: 0.975000\n",
      "Epoch: 402/500 Iteration: 3620 Train loss: 0.112002 Train acc: 0.971667\n",
      "Epoch: 402/500 Iteration: 3625 Train loss: 0.102055 Train acc: 0.966667\n",
      "Epoch: 402/500 Iteration: 3625 Validation loss: 0.361044 Validation acc: 0.912500\n",
      "Epoch: 403/500 Iteration: 3630 Train loss: 0.073429 Train acc: 0.983333\n",
      "Epoch: 403/500 Iteration: 3635 Train loss: 0.076819 Train acc: 0.978333\n",
      "Epoch: 404/500 Iteration: 3640 Train loss: 0.051737 Train acc: 0.980000\n",
      "Epoch: 404/500 Iteration: 3645 Train loss: 0.106947 Train acc: 0.976667\n",
      "Epoch: 405/500 Iteration: 3650 Train loss: 0.068723 Train acc: 0.981667\n",
      "Epoch: 405/500 Iteration: 3650 Validation loss: 0.416554 Validation acc: 0.895833\n",
      "Epoch: 406/500 Iteration: 3655 Train loss: 0.106389 Train acc: 0.973333\n",
      "Epoch: 406/500 Iteration: 3660 Train loss: 0.079185 Train acc: 0.973333\n",
      "Epoch: 407/500 Iteration: 3665 Train loss: 0.087298 Train acc: 0.975000\n",
      "Epoch: 407/500 Iteration: 3670 Train loss: 0.101531 Train acc: 0.968333\n",
      "Epoch: 408/500 Iteration: 3675 Train loss: 0.076625 Train acc: 0.980000\n",
      "Epoch: 408/500 Iteration: 3675 Validation loss: 0.372226 Validation acc: 0.910833\n",
      "Epoch: 408/500 Iteration: 3680 Train loss: 0.088131 Train acc: 0.973333\n",
      "Epoch: 409/500 Iteration: 3685 Train loss: 0.039112 Train acc: 0.985000\n",
      "Epoch: 409/500 Iteration: 3690 Train loss: 0.099114 Train acc: 0.966667\n",
      "Epoch: 410/500 Iteration: 3695 Train loss: 0.068694 Train acc: 0.975000\n",
      "Epoch: 411/500 Iteration: 3700 Train loss: 0.081596 Train acc: 0.980000\n",
      "Epoch: 411/500 Iteration: 3700 Validation loss: 0.374095 Validation acc: 0.907500\n",
      "Epoch: 411/500 Iteration: 3705 Train loss: 0.074025 Train acc: 0.983333\n",
      "Epoch: 412/500 Iteration: 3710 Train loss: 0.076312 Train acc: 0.973333\n",
      "Epoch: 412/500 Iteration: 3715 Train loss: 0.075892 Train acc: 0.971667\n",
      "Epoch: 413/500 Iteration: 3720 Train loss: 0.056745 Train acc: 0.985000\n",
      "Epoch: 413/500 Iteration: 3725 Train loss: 0.095057 Train acc: 0.973333\n",
      "Epoch: 413/500 Iteration: 3725 Validation loss: 0.398000 Validation acc: 0.898333\n",
      "Epoch: 414/500 Iteration: 3730 Train loss: 0.177410 Train acc: 0.956667\n",
      "Epoch: 414/500 Iteration: 3735 Train loss: 0.174433 Train acc: 0.956667\n",
      "Epoch: 415/500 Iteration: 3740 Train loss: 0.081245 Train acc: 0.966667\n",
      "Epoch: 416/500 Iteration: 3745 Train loss: 0.102546 Train acc: 0.966667\n",
      "Epoch: 416/500 Iteration: 3750 Train loss: 0.106552 Train acc: 0.970000\n",
      "Epoch: 416/500 Iteration: 3750 Validation loss: 0.425286 Validation acc: 0.893333\n",
      "Epoch: 417/500 Iteration: 3755 Train loss: 0.071063 Train acc: 0.981667\n",
      "Epoch: 417/500 Iteration: 3760 Train loss: 0.085407 Train acc: 0.980000\n",
      "Epoch: 418/500 Iteration: 3765 Train loss: 0.074250 Train acc: 0.971667\n",
      "Epoch: 418/500 Iteration: 3770 Train loss: 0.100891 Train acc: 0.971667\n",
      "Epoch: 419/500 Iteration: 3775 Train loss: 0.053048 Train acc: 0.985000\n",
      "Epoch: 419/500 Iteration: 3775 Validation loss: 0.388997 Validation acc: 0.902500\n",
      "Epoch: 419/500 Iteration: 3780 Train loss: 0.095027 Train acc: 0.973333\n",
      "Epoch: 420/500 Iteration: 3785 Train loss: 0.083392 Train acc: 0.971667\n",
      "Epoch: 421/500 Iteration: 3790 Train loss: 0.078313 Train acc: 0.985000\n",
      "Epoch: 421/500 Iteration: 3795 Train loss: 0.058758 Train acc: 0.985000\n",
      "Epoch: 422/500 Iteration: 3800 Train loss: 0.072134 Train acc: 0.980000\n",
      "Epoch: 422/500 Iteration: 3800 Validation loss: 0.398323 Validation acc: 0.903333\n",
      "Epoch: 422/500 Iteration: 3805 Train loss: 0.067495 Train acc: 0.980000\n",
      "Epoch: 423/500 Iteration: 3810 Train loss: 0.050322 Train acc: 0.985000\n",
      "Epoch: 423/500 Iteration: 3815 Train loss: 0.073085 Train acc: 0.983333\n",
      "Epoch: 424/500 Iteration: 3820 Train loss: 0.056223 Train acc: 0.981667\n",
      "Epoch: 424/500 Iteration: 3825 Train loss: 0.100442 Train acc: 0.975000\n",
      "Epoch: 424/500 Iteration: 3825 Validation loss: 0.390686 Validation acc: 0.903333\n",
      "Epoch: 425/500 Iteration: 3830 Train loss: 0.044988 Train acc: 0.991667\n",
      "Epoch: 426/500 Iteration: 3835 Train loss: 0.066911 Train acc: 0.985000\n",
      "Epoch: 426/500 Iteration: 3840 Train loss: 0.046200 Train acc: 0.988333\n",
      "Epoch: 427/500 Iteration: 3845 Train loss: 0.051220 Train acc: 0.985000\n",
      "Epoch: 427/500 Iteration: 3850 Train loss: 0.076902 Train acc: 0.981667\n",
      "Epoch: 427/500 Iteration: 3850 Validation loss: 0.398684 Validation acc: 0.905833\n",
      "Epoch: 428/500 Iteration: 3855 Train loss: 0.043183 Train acc: 0.988333\n",
      "Epoch: 428/500 Iteration: 3860 Train loss: 0.062111 Train acc: 0.985000\n",
      "Epoch: 429/500 Iteration: 3865 Train loss: 0.046496 Train acc: 0.986667\n",
      "Epoch: 429/500 Iteration: 3870 Train loss: 0.084921 Train acc: 0.981667\n",
      "Epoch: 430/500 Iteration: 3875 Train loss: 0.043011 Train acc: 0.981667\n",
      "Epoch: 430/500 Iteration: 3875 Validation loss: 0.420122 Validation acc: 0.900000\n",
      "Epoch: 431/500 Iteration: 3880 Train loss: 0.123550 Train acc: 0.966667\n",
      "Epoch: 431/500 Iteration: 3885 Train loss: 0.068265 Train acc: 0.975000\n",
      "Epoch: 432/500 Iteration: 3890 Train loss: 0.120640 Train acc: 0.963333\n",
      "Epoch: 432/500 Iteration: 3895 Train loss: 0.112189 Train acc: 0.960000\n",
      "Epoch: 433/500 Iteration: 3900 Train loss: 0.053305 Train acc: 0.986667\n",
      "Epoch: 433/500 Iteration: 3900 Validation loss: 0.397487 Validation acc: 0.905000\n",
      "Epoch: 433/500 Iteration: 3905 Train loss: 0.076557 Train acc: 0.980000\n",
      "Epoch: 434/500 Iteration: 3910 Train loss: 0.057725 Train acc: 0.980000\n",
      "Epoch: 434/500 Iteration: 3915 Train loss: 0.091734 Train acc: 0.966667\n",
      "Epoch: 435/500 Iteration: 3920 Train loss: 0.040891 Train acc: 0.993333\n",
      "Epoch: 436/500 Iteration: 3925 Train loss: 0.060523 Train acc: 0.983333\n",
      "Epoch: 436/500 Iteration: 3925 Validation loss: 0.435556 Validation acc: 0.891667\n",
      "Epoch: 436/500 Iteration: 3930 Train loss: 0.060901 Train acc: 0.985000\n",
      "Epoch: 437/500 Iteration: 3935 Train loss: 0.074596 Train acc: 0.975000\n",
      "Epoch: 437/500 Iteration: 3940 Train loss: 0.079445 Train acc: 0.976667\n",
      "Epoch: 438/500 Iteration: 3945 Train loss: 0.064808 Train acc: 0.990000\n",
      "Epoch: 438/500 Iteration: 3950 Train loss: 0.079841 Train acc: 0.980000\n",
      "Epoch: 438/500 Iteration: 3950 Validation loss: 0.434172 Validation acc: 0.893333\n",
      "Epoch: 439/500 Iteration: 3955 Train loss: 0.157541 Train acc: 0.951667\n",
      "Epoch: 439/500 Iteration: 3960 Train loss: 0.207910 Train acc: 0.938333\n",
      "Epoch: 440/500 Iteration: 3965 Train loss: 0.216764 Train acc: 0.935000\n",
      "Epoch: 441/500 Iteration: 3970 Train loss: 0.190505 Train acc: 0.931667\n",
      "Epoch: 441/500 Iteration: 3975 Train loss: 0.192148 Train acc: 0.940000\n",
      "Epoch: 441/500 Iteration: 3975 Validation loss: 0.417154 Validation acc: 0.890833\n",
      "Epoch: 442/500 Iteration: 3980 Train loss: 0.133835 Train acc: 0.960000\n",
      "Epoch: 442/500 Iteration: 3985 Train loss: 0.075167 Train acc: 0.975000\n",
      "Epoch: 443/500 Iteration: 3990 Train loss: 0.088393 Train acc: 0.966667\n",
      "Epoch: 443/500 Iteration: 3995 Train loss: 0.096853 Train acc: 0.976667\n",
      "Epoch: 444/500 Iteration: 4000 Train loss: 0.074801 Train acc: 0.980000\n",
      "Epoch: 444/500 Iteration: 4000 Validation loss: 0.385817 Validation acc: 0.900000\n",
      "Epoch: 444/500 Iteration: 4005 Train loss: 0.109476 Train acc: 0.968333\n",
      "Epoch: 445/500 Iteration: 4010 Train loss: 0.042356 Train acc: 0.990000\n",
      "Epoch: 446/500 Iteration: 4015 Train loss: 0.093147 Train acc: 0.978333\n",
      "Epoch: 446/500 Iteration: 4020 Train loss: 0.064653 Train acc: 0.981667\n",
      "Epoch: 447/500 Iteration: 4025 Train loss: 0.050486 Train acc: 0.985000\n",
      "Epoch: 447/500 Iteration: 4025 Validation loss: 0.374778 Validation acc: 0.904167\n",
      "Epoch: 447/500 Iteration: 4030 Train loss: 0.076845 Train acc: 0.975000\n",
      "Epoch: 448/500 Iteration: 4035 Train loss: 0.052332 Train acc: 0.985000\n",
      "Epoch: 448/500 Iteration: 4040 Train loss: 0.064891 Train acc: 0.983333\n",
      "Epoch: 449/500 Iteration: 4045 Train loss: 0.042960 Train acc: 0.991667\n",
      "Epoch: 449/500 Iteration: 4050 Train loss: 0.070066 Train acc: 0.980000\n",
      "Epoch: 449/500 Iteration: 4050 Validation loss: 0.390030 Validation acc: 0.901667\n",
      "Epoch: 450/500 Iteration: 4055 Train loss: 0.040712 Train acc: 0.988333\n",
      "Epoch: 451/500 Iteration: 4060 Train loss: 0.073137 Train acc: 0.983333\n",
      "Epoch: 451/500 Iteration: 4065 Train loss: 0.047138 Train acc: 0.990000\n",
      "Epoch: 452/500 Iteration: 4070 Train loss: 0.049401 Train acc: 0.986667\n",
      "Epoch: 452/500 Iteration: 4075 Train loss: 0.065026 Train acc: 0.980000\n",
      "Epoch: 452/500 Iteration: 4075 Validation loss: 0.403069 Validation acc: 0.907500\n",
      "Epoch: 453/500 Iteration: 4080 Train loss: 0.058465 Train acc: 0.985000\n",
      "Epoch: 453/500 Iteration: 4085 Train loss: 0.060103 Train acc: 0.985000\n",
      "Epoch: 454/500 Iteration: 4090 Train loss: 0.043964 Train acc: 0.988333\n",
      "Epoch: 454/500 Iteration: 4095 Train loss: 0.072292 Train acc: 0.985000\n",
      "Epoch: 455/500 Iteration: 4100 Train loss: 0.031297 Train acc: 0.995000\n",
      "Epoch: 455/500 Iteration: 4100 Validation loss: 0.431794 Validation acc: 0.895833\n",
      "Epoch: 456/500 Iteration: 4105 Train loss: 0.070276 Train acc: 0.985000\n",
      "Epoch: 456/500 Iteration: 4110 Train loss: 0.040427 Train acc: 0.985000\n",
      "Epoch: 457/500 Iteration: 4115 Train loss: 0.053405 Train acc: 0.981667\n",
      "Epoch: 457/500 Iteration: 4120 Train loss: 0.077040 Train acc: 0.975000\n",
      "Epoch: 458/500 Iteration: 4125 Train loss: 0.049747 Train acc: 0.990000\n",
      "Epoch: 458/500 Iteration: 4125 Validation loss: 0.399420 Validation acc: 0.901667\n",
      "Epoch: 458/500 Iteration: 4130 Train loss: 0.051676 Train acc: 0.986667\n",
      "Epoch: 459/500 Iteration: 4135 Train loss: 0.041560 Train acc: 0.993333\n",
      "Epoch: 459/500 Iteration: 4140 Train loss: 0.087936 Train acc: 0.978333\n",
      "Epoch: 460/500 Iteration: 4145 Train loss: 0.033399 Train acc: 0.993333\n",
      "Epoch: 461/500 Iteration: 4150 Train loss: 0.054740 Train acc: 0.988333\n",
      "Epoch: 461/500 Iteration: 4150 Validation loss: 0.433568 Validation acc: 0.900000\n",
      "Epoch: 461/500 Iteration: 4155 Train loss: 0.044527 Train acc: 0.990000\n",
      "Epoch: 462/500 Iteration: 4160 Train loss: 0.050855 Train acc: 0.988333\n",
      "Epoch: 462/500 Iteration: 4165 Train loss: 0.096682 Train acc: 0.976667\n",
      "Epoch: 463/500 Iteration: 4170 Train loss: 0.056290 Train acc: 0.985000\n",
      "Epoch: 463/500 Iteration: 4175 Train loss: 0.076048 Train acc: 0.978333\n",
      "Epoch: 463/500 Iteration: 4175 Validation loss: 0.453944 Validation acc: 0.895833\n",
      "Epoch: 464/500 Iteration: 4180 Train loss: 0.052288 Train acc: 0.988333\n",
      "Epoch: 464/500 Iteration: 4185 Train loss: 0.096934 Train acc: 0.980000\n",
      "Epoch: 465/500 Iteration: 4190 Train loss: 0.034812 Train acc: 0.988333\n",
      "Epoch: 466/500 Iteration: 4195 Train loss: 0.076319 Train acc: 0.983333\n",
      "Epoch: 466/500 Iteration: 4200 Train loss: 0.055247 Train acc: 0.990000\n",
      "Epoch: 466/500 Iteration: 4200 Validation loss: 0.405030 Validation acc: 0.905833\n",
      "Epoch: 467/500 Iteration: 4205 Train loss: 0.050808 Train acc: 0.985000\n",
      "Epoch: 467/500 Iteration: 4210 Train loss: 0.061581 Train acc: 0.981667\n",
      "Epoch: 468/500 Iteration: 4215 Train loss: 0.041524 Train acc: 0.993333\n",
      "Epoch: 468/500 Iteration: 4220 Train loss: 0.053842 Train acc: 0.983333\n",
      "Epoch: 469/500 Iteration: 4225 Train loss: 0.039086 Train acc: 0.990000\n",
      "Epoch: 469/500 Iteration: 4225 Validation loss: 0.436825 Validation acc: 0.901667\n",
      "Epoch: 469/500 Iteration: 4230 Train loss: 0.075511 Train acc: 0.983333\n",
      "Epoch: 470/500 Iteration: 4235 Train loss: 0.030749 Train acc: 0.993333\n",
      "Epoch: 471/500 Iteration: 4240 Train loss: 0.057656 Train acc: 0.985000\n",
      "Epoch: 471/500 Iteration: 4245 Train loss: 0.044236 Train acc: 0.993333\n",
      "Epoch: 472/500 Iteration: 4250 Train loss: 0.064461 Train acc: 0.983333\n",
      "Epoch: 472/500 Iteration: 4250 Validation loss: 0.447552 Validation acc: 0.903333\n",
      "Epoch: 472/500 Iteration: 4255 Train loss: 0.072893 Train acc: 0.976667\n",
      "Epoch: 473/500 Iteration: 4260 Train loss: 0.051045 Train acc: 0.986667\n",
      "Epoch: 473/500 Iteration: 4265 Train loss: 0.054596 Train acc: 0.983333\n",
      "Epoch: 474/500 Iteration: 4270 Train loss: 0.031018 Train acc: 0.995000\n",
      "Epoch: 474/500 Iteration: 4275 Train loss: 0.078968 Train acc: 0.986667\n",
      "Epoch: 474/500 Iteration: 4275 Validation loss: 0.435703 Validation acc: 0.900833\n",
      "Epoch: 475/500 Iteration: 4280 Train loss: 0.055413 Train acc: 0.981667\n",
      "Epoch: 476/500 Iteration: 4285 Train loss: 0.056424 Train acc: 0.988333\n",
      "Epoch: 476/500 Iteration: 4290 Train loss: 0.053067 Train acc: 0.985000\n",
      "Epoch: 477/500 Iteration: 4295 Train loss: 0.048256 Train acc: 0.986667\n",
      "Epoch: 477/500 Iteration: 4300 Train loss: 0.075454 Train acc: 0.978333\n",
      "Epoch: 477/500 Iteration: 4300 Validation loss: 0.423908 Validation acc: 0.905833\n",
      "Epoch: 478/500 Iteration: 4305 Train loss: 0.053633 Train acc: 0.988333\n",
      "Epoch: 478/500 Iteration: 4310 Train loss: 0.049146 Train acc: 0.988333\n",
      "Epoch: 479/500 Iteration: 4315 Train loss: 0.043533 Train acc: 0.985000\n",
      "Epoch: 479/500 Iteration: 4320 Train loss: 0.074851 Train acc: 0.983333\n",
      "Epoch: 480/500 Iteration: 4325 Train loss: 0.022587 Train acc: 0.996667\n",
      "Epoch: 480/500 Iteration: 4325 Validation loss: 0.447452 Validation acc: 0.904167\n",
      "Epoch: 481/500 Iteration: 4330 Train loss: 0.058947 Train acc: 0.990000\n",
      "Epoch: 481/500 Iteration: 4335 Train loss: 0.061532 Train acc: 0.985000\n",
      "Epoch: 482/500 Iteration: 4340 Train loss: 0.055399 Train acc: 0.983333\n",
      "Epoch: 482/500 Iteration: 4345 Train loss: 0.065171 Train acc: 0.983333\n",
      "Epoch: 483/500 Iteration: 4350 Train loss: 0.070637 Train acc: 0.978333\n",
      "Epoch: 483/500 Iteration: 4350 Validation loss: 0.433113 Validation acc: 0.905833\n",
      "Epoch: 483/500 Iteration: 4355 Train loss: 0.088321 Train acc: 0.968333\n",
      "Epoch: 484/500 Iteration: 4360 Train loss: 0.035193 Train acc: 0.986667\n",
      "Epoch: 484/500 Iteration: 4365 Train loss: 0.141810 Train acc: 0.965000\n",
      "Epoch: 485/500 Iteration: 4370 Train loss: 0.050040 Train acc: 0.988333\n",
      "Epoch: 486/500 Iteration: 4375 Train loss: 0.092609 Train acc: 0.975000\n",
      "Epoch: 486/500 Iteration: 4375 Validation loss: 0.436769 Validation acc: 0.898333\n",
      "Epoch: 486/500 Iteration: 4380 Train loss: 0.094926 Train acc: 0.973333\n",
      "Epoch: 487/500 Iteration: 4385 Train loss: 0.105503 Train acc: 0.965000\n",
      "Epoch: 487/500 Iteration: 4390 Train loss: 0.096680 Train acc: 0.966667\n",
      "Epoch: 488/500 Iteration: 4395 Train loss: 0.088524 Train acc: 0.980000\n",
      "Epoch: 488/500 Iteration: 4400 Train loss: 0.113267 Train acc: 0.970000\n",
      "Epoch: 488/500 Iteration: 4400 Validation loss: 0.411302 Validation acc: 0.902500\n",
      "Epoch: 489/500 Iteration: 4405 Train loss: 0.059273 Train acc: 0.981667\n",
      "Epoch: 489/500 Iteration: 4410 Train loss: 0.084921 Train acc: 0.978333\n",
      "Epoch: 490/500 Iteration: 4415 Train loss: 0.059818 Train acc: 0.973333\n",
      "Epoch: 491/500 Iteration: 4420 Train loss: 0.158065 Train acc: 0.960000\n",
      "Epoch: 491/500 Iteration: 4425 Train loss: 0.123826 Train acc: 0.960000\n",
      "Epoch: 491/500 Iteration: 4425 Validation loss: 0.371543 Validation acc: 0.905000\n",
      "Epoch: 492/500 Iteration: 4430 Train loss: 0.210305 Train acc: 0.935000\n",
      "Epoch: 492/500 Iteration: 4435 Train loss: 0.138667 Train acc: 0.958333\n",
      "Epoch: 493/500 Iteration: 4440 Train loss: 0.106561 Train acc: 0.966667\n",
      "Epoch: 493/500 Iteration: 4445 Train loss: 0.073704 Train acc: 0.976667\n",
      "Epoch: 494/500 Iteration: 4450 Train loss: 0.067402 Train acc: 0.976667\n",
      "Epoch: 494/500 Iteration: 4450 Validation loss: 0.399561 Validation acc: 0.905833\n",
      "Epoch: 494/500 Iteration: 4455 Train loss: 0.126443 Train acc: 0.968333\n",
      "Epoch: 495/500 Iteration: 4460 Train loss: 0.047177 Train acc: 0.990000\n",
      "Epoch: 496/500 Iteration: 4465 Train loss: 0.077524 Train acc: 0.976667\n",
      "Epoch: 496/500 Iteration: 4470 Train loss: 0.047790 Train acc: 0.985000\n",
      "Epoch: 497/500 Iteration: 4475 Train loss: 0.041322 Train acc: 0.981667\n",
      "Epoch: 497/500 Iteration: 4475 Validation loss: 0.423988 Validation acc: 0.901667\n",
      "Epoch: 497/500 Iteration: 4480 Train loss: 0.044257 Train acc: 0.985000\n",
      "Epoch: 498/500 Iteration: 4485 Train loss: 0.056656 Train acc: 0.981667\n",
      "Epoch: 498/500 Iteration: 4490 Train loss: 0.058404 Train acc: 0.981667\n",
      "Epoch: 499/500 Iteration: 4495 Train loss: 0.035924 Train acc: 0.993333\n",
      "Epoch: 499/500 Iteration: 4500 Train loss: 0.075206 Train acc: 0.980000\n",
      "Epoch: 499/500 Iteration: 4500 Validation loss: 0.416520 Validation acc: 0.907500\n"
     ]
    }
   ],
   "source": [
    "validation_acc = []\n",
    "validation_loss = []\n",
    "\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        # Initialize \n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        # Loop over batches\n",
    "        for x,y in get_batches(X_tr, y_tr, batch_size):\n",
    "            \n",
    "            # Feed dictionary\n",
    "            feed = {inputs_ : x, labels_ : y, keep_prob_ : 0.5, \n",
    "                    initial_state : state, learning_rate_ : learning_rate}\n",
    "            \n",
    "            loss, _ , state, acc = sess.run([cost, optimizer, final_state, accuracy], \n",
    "                                             feed_dict = feed)\n",
    "            train_acc.append(acc)\n",
    "            train_loss.append(loss)\n",
    "            \n",
    "            # Print at each 5 iters\n",
    "            if (iteration % 5 == 0):\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {:d}\".format(iteration),\n",
    "                      \"Train loss: {:6f}\".format(loss),\n",
    "                      \"Train acc: {:.6f}\".format(acc))\n",
    "            \n",
    "            # Compute validation loss at every 25 iterations\n",
    "            if (iteration%25 == 0):\n",
    "                \n",
    "                # Initiate for validation set\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                \n",
    "                val_acc_ = []\n",
    "                val_loss_ = []\n",
    "                for x_v, y_v in get_batches(X_vld, y_vld, batch_size):\n",
    "                    # Feed\n",
    "                    feed = {inputs_ : x_v, labels_ : y_v, keep_prob_ : 1.0, initial_state : val_state}\n",
    "                    \n",
    "                    # Loss\n",
    "                    loss_v, state_v, acc_v = sess.run([cost, final_state, accuracy], feed_dict = feed)\n",
    "                    \n",
    "                    val_acc_.append(acc_v)\n",
    "                    val_loss_.append(loss_v)\n",
    "                \n",
    "                # Print info\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {:d}\".format(iteration),\n",
    "                      \"Validation loss: {:6f}\".format(np.mean(val_loss_)),\n",
    "                      \"Validation acc: {:.6f}\".format(np.mean(val_acc_)))\n",
    "                \n",
    "                # Store\n",
    "                validation_acc.append(np.mean(val_acc_))\n",
    "                validation_loss.append(np.mean(val_loss_))\n",
    "            \n",
    "            # Iterate \n",
    "            iteration += 1\n",
    "    \n",
    "    saver.save(sess,\"checkpoints-crnn/har.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAF3CAYAAAC2bHyQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6d3cd6e438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training and test loss\n",
    "t = np.arange(iteration-1)\n",
    "\n",
    "plt.figure(figsize = (6,6))\n",
    "plt.plot(t, np.array(train_loss), 'r-', t[t % 25 == 0], np.array(validation_loss), 'b*')\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAF3CAYAAABKeVdaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEYn7/49vH7+Hk6x+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6cbf9a6fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Accuracies\n",
    "plt.figure(figsize = (6,6))\n",
    "\n",
    "plt.plot(t, np.array(train_acc), 'r-', t[t % 25 == 0], validation_acc, 'b*')\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"Accuray\")\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.954305\n"
     ]
    }
   ],
   "source": [
    "test_acc = []\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # Restore\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints-crnn'))\n",
    "    \n",
    "    for x_t, y_t in get_batches(X_test, y_test, batch_size):\n",
    "        feed = {inputs_: x_t,\n",
    "                labels_: y_t,\n",
    "                keep_prob_: 1}\n",
    "        \n",
    "        batch_acc = sess.run(accuracy, feed_dict=feed)\n",
    "        test_acc.append(batch_acc)\n",
    "    print(\"Test accuracy: {:.6f}\".format(np.mean(test_acc)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
